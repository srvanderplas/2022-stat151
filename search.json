[
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Introduction to Statistical Computing",
    "section": "How to Use This Book",
    "text": "I’ve made an effort to use some specific formatting and enable certain features that make this book a useful tool for this class.\n\nSpecial Sections\n\nWatch Out\nWatch out sections contain things you may want to look out for - common errors, etc.\n\n \n\nExamples\nExample sections contain code and other information. Don’t skip them!\n\n \n\nMy Opinion\nThese sections contain things you should definitely not consider as fact and should just take with a grain of salt.\n\n \n\nGo Read\nSometimes, there are better resources out there than something I could write myself. When you see this section, go read the enclosed link as if it were part of the book.\n\n \n\nTry It Out\nTry it out sections contain activities you should do to reinforce the things you’ve just read.\n\n \n\nLearn More\nLearn More sections contain other references that may be useful on a specific topic. Suggestions are welcome (email me to suggest a new reference that I should add), as there’s no way for one person to catalog all of the helpful programming resources on the internet!\n\n \n\nNote\nNote sections contain clarification points (anywhere I would normally say “note that ….)\n\n\nExpandable Sections\n\n\nThese are expandable sections, with additional information when you click on the line\n\nThis additional information may be information that is helpful but not essential, or it may be that an example just takes a LOT of space and I want to make sure you can skim the book without having to scroll through a ton of output.\n\n \n\n\nMany times, examples will be in expandable sections\n\nThis keeps the code and output from obscuring the actual information in the textbook that I want you to retain. You can always look up the syntax, but you do need to absorb the details I’ve written out."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Introduction to Statistical Computing",
    "section": "About This Book",
    "text": "This is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.\nI have written this entire book using reproducible techniques, with R and python code and results included within the book’s text.\nStat 151 will be offered for the first time in Spring 2022, as I’m writing this in Fall 2021. Initially, my goal is to write the book in R and include python as an additional option/example. Eventually, I hope to teach Stat 151 in R and Python at the same time."
  },
  {
    "objectID": "01-getting-started.html#getting-started-objectives",
    "href": "01-getting-started.html#getting-started-objectives",
    "title": "1  Getting Started",
    "section": "Objectives",
    "text": "Understand the basics of how computers work\nUnderstand the file system mental model for computers\nSet up RStudio, R, Quarto, and python\nBe able to run demo code in R and python"
  },
  {
    "objectID": "01-getting-started.html#computer-basics",
    "href": "01-getting-started.html#computer-basics",
    "title": "1  Getting Started",
    "section": "1.1 Computer Basics",
    "text": "It is helpful when teaching a topic as technical as programming to ensure that everyone starts from the same basic foundational understanding and mental model of how things work. When teaching geology, for instance, the instructor should probably make sure that everyone understands that the earth is a round ball and not a flat plate – it will save everyone some time later.\nWe all use computers daily - we carry them around with us on our wrists, in our pockets, and in our backpacks. This is no guarantee, however, that we understand how they work or what makes them go.\n\n1.1.1 Hardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). Most of the other details aren’t necessary (for now).\n\n\nChapter 1 of Python for Everybody - Computer hardware architecture\n\n\n\n\n1.1.2 Operating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time.\n\n\n\n\n\n\n1.1.3 File Systems\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore. This article is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\n\nThat’s not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders).\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your “base folder” is known as your working directory - the place your program thinks of as home."
  },
  {
    "objectID": "02-markdown.html#scripts",
    "href": "02-markdown.html#scripts",
    "title": "2  Scripts and Notebooks",
    "section": "2.1 Scripts",
    "text": "Before I show you how to use literate programming, let’s look at what it replaces: scripts. Scripts are files of code that are meant to be run on their own. They may produce results, or format data and save it somewhere, or scrape data from the web – scripts can do just about anything.\nScripts can even have documentation within the file, using # characters (at least, in R and python) at the beginning of a line. # indicates a comment – that is, that the line does not contain code and should be ignored by the computer when the program is run. Comments are incredibly useful to help humans understand what the code does and why it does it.\n\n2.1.1 Plotting a logarithmic spiral\nThis code will use concepts we have not yet introduced - feel free to tinker with it if you want, but know that you’re not responsible for being able to write this code yet. You just need to read it and get a sense for what it does. I have heavily commented it to help with this process.\n\nRPython\n\n\n\n# Define the angle of the spiral (polar coords)\n# go around two full times (2*pi = one revolution)\ntheta <- seq(0, 4*pi, .01) \n# Define the distance from the origin of the spiral\n# Needs to have the same length as theta\nr <- seq(0, 5, length.out = length(theta))\n\n# Now define x and y in cartesian coordinates\nx <- r * cos(theta)\ny <- r * sin(theta)\n\nplot(x, y, type = \"l\")\n\n\n\n\nFigure 2.1: A Cartesian Spiral in R\n\n\n\n\nI have saved this script here. You can download it and open it up in RStudio (File -> Open -> Navigate to file location).\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the angle of the spiral (polar coords)\n# go around two full times (2*pi = one revolution)\ntheta = np.arange(0, 4 * np.pi, 0.01)\n# Define the distance from the origin of the spiral\n# Needs to have the same length as theta \n# (get length of theta with theta.size, \n#  and then divide 5 by that to get the increment)\nr = np.arange(0, 5, 5/theta.size)\n\n# Now define x and y in cartesian coordinates\nx = r * np.cos(theta)\ny = r * np.sin(theta)\n\n# Define the axes\nfig, ax = plt.subplots()\n# Plot the line\nax.plot(x, y)\nplt.show()\n\n\n\n\nFigure 2.2: A Cartesian Spiral in python\n\n\n\n\nI have saved this script here. You can download it and open it up in RStudio (File -> Open -> Navigate to file location).\n\n\n\n\nScripts can be run in Rstudio by clicking the Run button  at the top of the editor window when the script is open.\n\n2.1.2 Try it out!\n\nDownload the R and python scripts in the above example, open them in RStudio, and run each script using the Run button. What do you see?\n(Advanced) Open a terminal in RStudio (Tools -> Terminal -> New Terminal) and see if you can run the R script from the terminal using R CMD BATCH path/to/file/markdown-spiral-script.R (You will have to modify this command to point to the file on your machine)\nNotice that two new files appear in your working directory: Rplots.pdf and markdown-spiral-script.Rout\n(Advanced) Open a terminal in RStudio (Tools -> Terminal -> New Terminal) and see if you can run the R script from the terminal using python3 path/to/file/markdown-spiral-script.py (You will have to modify this command to point to the file on your machine)\nThis will require you to have python3 accessible to you on the command line, which may be a challenge if it is not set up in the way that I’m assuming it is. Feel free to make an appointment to see if we can figure it out, if this does not work the first time.\n\n\nMost of the time, you will run scripts interactively - that is, you’ll be sitting there watching the script run and seeing what the results are as you are modifying the script. However, one advantage to scripts over notebooks is that it is easy to write a script and schedule it to run without supervision to complete tasks which may be repetitive. I have a script that runs daily at midnight, 6am, noon, and 6pm to pull information off of the internet for a dataset I’m maintaining. I’ve set it up so that this all happens automatically and I only have to check the results when I am interested in working with that data."
  },
  {
    "objectID": "02-markdown.html#notebooks",
    "href": "02-markdown.html#notebooks",
    "title": "2  Scripts and Notebooks",
    "section": "2.2 Notebooks",
    "text": "Notebooks are an implementation of literate programming. Both R and python have native notebooks that are cross-platform and allow you to code in R or python. This book is written using Quarto markdown, which is an extension of Rmarkdown, but it is also possible to use jupyter notebooks to write R code.\nIn this class, we’re going to use Quarto/R markdown, because it is a much better tool for creating polished reports than Jupyter (in my opinion). This matters because the goal is that you learn something useful for your own coding and then you can easily apply it when you go to work as an analyst somewhere to produce impressive documents. Jupyter notebooks are great for interactive coding, but aren’t so wonderful for producing polished results. They also don’t allow you to switch between languages mid-notebook, and since I’m trying to teach this class in both R and python, I want you to have both languages available.\n\nThere are some excellent opinions surrounding the use of notebooks in data analysis:\n\nWhy I Don’t Like Notebooks” by Joel Grus at JupyterCon 2018\nThe First Notebook War by Yihui Xie (response to Joel’s talk).\nYihui Xie is the person responsible for knitr and Rmarkdown.\n\n\n \n\n2.2.1 Try it out\n\nR markdownJupyterQuarto markdown\n\n\nTake a look at the R markdown sample file I’ve created to go with the R script above. You can see the HTML file it generates here.\n\nDownload the Rmd file and open it with RStudio.\nChange the output to output: word_document and hit the Render button . Can you find the markdown-demo.docx file that was generated? What does it look like?\nChange the output to output: pdf_document and hit the Render button . Can you find the markdown-demo.pdf file that was generated? What does it look like?\n\nRmarkdown tries very hard to preserve your formatted text appropriately regardless of the output document type. While things may not look exactly the same, the goal is to allow you to focus on the content and the formatting will “just work”.\n\n\nTake a look at the jupyter notebook sample file I’ve created to go with the R script above. You can see the HTML file it generates here.\n\nDownload the ipynb file and open it with jupyter.\nExport the notebook as a pdf file (File -> Save as -> PDF via HTML). Can you find the jupyter-demo.pdf file that was generated? What does it look like?\nExport the notebook as an html file (File -> Save as -> HTML). Can you find the jupyter-demo.html file that was generated? What does it look like?\n\n\n\nThe nice thing about quarto is that it will work with python and R seamlessly, and you can compile the document using python or R. R markdown will also allow you to use python chunks, but you must compile the document in R.\nTake a look at the Qmd notebook sample file I’ve created to go with the scripts above. You’ll notice that it is basically the script portion of this textbook – that’s because I’m writing the textbook in Quarto.\n\nDownload the qmd file and open it with RStudio\nTry to compile the file by hitting the Render button \n(Advanced) In the terminal, type in quarto render path/to/file/quarto-demo.qmd. Does that render the HTML file? One advantage of this is that using quarto to render the file doesn’t require R at the command line. As the document contains R chunks, R is still required to compile the document, but the biggest difference between qmd and rmd is that qmd files are workflow agnostic - you can generate them in e.g. MS Visual Studio Code, compile them in that workflow, and never have to use RStudio."
  },
  {
    "objectID": "03-finding-your-way.html#programming",
    "href": "03-finding-your-way.html#programming",
    "title": "3  Finding your way in R and Python",
    "section": "3.1 Programming",
    "text": "Programming today is a race between software engineers striving to build bigger and better idiot-proof programs, and the universe trying to produce bigger and better idiots. So far, the universe is winning. - Rick Cook\n\nProgramming is the art of solving a problem by developing a sequence of steps that make up a solution, and then very carefully communicating those steps to the computer. To program, you need to know how to\n\nbreak a problem down into smaller, easily solvable problems\nsolve small problems\ncommunicate the solution to a computer using a programming language\n\nIn this class, we’ll be using both R and Python, and we’ll be using these languages to solve problems that are related to working with data. At first, we’ll start with smaller, simpler problems that don’t involve data, but by the end of the semester, you will hopefully be able to solve some statistical problems using one or both languages.\nIt will be hard at first - you have to learn the vocabulary in both languages in order to be able to put commands into logical “sentences”. The problem solving skills are the same for all programming languages, though, and while those are harder to learn, they’ll last you a lifetime."
  },
  {
    "objectID": "03-finding-your-way.html#hello-world",
    "href": "03-finding-your-way.html#hello-world",
    "title": "3  Finding your way in R and Python",
    "section": "3.2 Hello world",
    "text": "I particularly like the way that Python for Everybody (Severance 2016) explains vocabulary:\n\nUnlike human languages, the Python vocabulary is actually pretty small. We call this “vocabulary” the “reserved words”. These are words that have very special meaning to Python. When Python sees these words in a Python program, they have one and only one meaning to Python. Later as you write programs you will make up your own words that have meaning to you called variables. You will have great latitude in choosing your names for your variables, but you cannot use any of Python’s reserved words as a name for a variable.\n\n\nWhen we train a dog, we use special words like “sit”, “stay”, and “fetch”. When you talk to a dog and don’t use any of the reserved words, they just look at you with a quizzical look on their face until you say a reserved word. For example, if you say, “I wish more people would walk to improve their overall health”, what most dogs likely hear is, “blah blah blah walk blah blah blah blah.” That is because “walk” is a reserved word in dog language. Many might suggest that the language between humans and cats has no reserved words.\n\n\nThe reserved words in the language where humans talk to Python include the following:\n\nand       del       global      not       with\nas        elif      if          or        yield\nassert    else      import      pass\nbreak     except    in          raise\nclass     finally   is          return\ncontinue  for       lambda      try\ndef       from      nonlocal    while    \n\nThat is it, and unlike a dog, Python is already completely trained. When you say ‘try’, Python will try every time you say it without fail.\n\n\nWe will learn these reserved words and how they are used in good time, but for now we will focus on the Python equivalent of “speak” (in human-to-dog language). The nice thing about telling Python to speak is that we can even tell it what to say by giving it a message in quotes:\n\n\nprint('Hello world!')\n\nHello world!\n\n\n\nAnd we have even written our first syntactically correct Python sentence. Our sentence starts with the function print followed by a string of text of our choosing enclosed in single quotes. The strings in the print statements are enclosed in quotes. Single quotes and double quotes do the same thing; most people use single quotes except in cases like this where a single quote (which is also an apostrophe) appears in the string.\n\nR has a slightly smaller set of reserved words:\nif          else     repeat      while        \nfor         in       next        break  \nTRUE        FALSE    NULL        Inf         \nNA_integer_ NA_real_ NA_complex_ NA_character_  \nNaN         NA       function    ...\nIn R, the “Hello World” program looks exactly the same as it does in python.\n\nprint('Hello world!')\n\n[1] \"Hello world!\"\n\n\nIn many situations, R and python will be similar because both languages are based on C. R has a more complicated history, because it is also similar to Lisp, but both languages are still very similar to C and run C or C++ code in the background."
  },
  {
    "objectID": "03-finding-your-way.html#talking-to-python-and-r---interactive-mode",
    "href": "03-finding-your-way.html#talking-to-python-and-r---interactive-mode",
    "title": "3  Finding your way in R and Python",
    "section": "3.3 Talking to Python and R - Interactive mode",
    "text": "R and python both have an “interactive mode” that you will use most often. In the previous chapter, we talked about scripts and markdown documents, both of which are non-interactive methods for writing R and python code. But for the moment, let’s work with the interactive console in both languages in order to get familiar with how we talk to R and python.\nLet’s start by creating a Qmd file (File -> New File -> Quarto Document) - this will let us work with R and python at the same time.\nAdd an R chunk to your file by typing ```{r} into the first line of the file, and then hit return. RStudio should add a blank line followed by ```.\nAdd a python chunk to your file by typing ```{python} on a blank line below the R chunk you just created, and then hit return. RStudio should add a blank line followed by ```.\nYour file should look like this:\n\n\n\nScreenshot of qmd file after adding an empty r and python chunk\n\n\nIf instead your file looks like this:\n\n\n\nScreenshot of qmd file with visual markdown editing on\n\n\nyou have visual markdown mode on. To turn it off, click on the A icon at the top right of your editor window:\n\n\n\nScreenshot of editor window toolbar, with A icon highlighted in green\n\n\nIf we are working in interactive mode, why did I have you start out by creating a markdown document? Good Question! RStudio allows you to switch back and forth between R and python seamlessly, which is good and bad - it’s hard to get a python terminal without telling R which language you’re working in! You can create a python script if you’d prefer to work in a script instead of a markdown document, but that would involve working in 2 separate files, which I personally find rather tedious.\n\nThe R ConsoleThe Python Console\n\n\nIn your R chunk or script, type in 2+2 and hit Ctrl+Enter (or Cmd+Enter on a mac). Look down to the Console (which is usually below the editor window) and see if 4 appears. If you’re like me, output shows up in two places at once:\n\n\n\nLocation\nPicture\n\n\n\n\nChunk\n\n\n\nScript\n\n\n\nConsole\n\n\n\n\nR will indicate that it is waiting for your command with a > character in the console. If you don’t see that > character, chances are you’ve forgotten to finish a statement - check for parentheses and brackets.\nWhen you are working in an R script, any output is shown only in the console. When you are working in an R code chunk, output is shown both below the chunk and in the console.\nIf you want, you can also just work within the R console. This can be useful for quick, interactive work, or if, like me, you’re too lazy to pull up a calculator on your machine and you just want to use R to calculate something quickly. You just type your R command into the console:\n\n\n\nR console with commands “Hello”, print(“Hello”), and (unquoted) “I love R”, which causes an error\n\n\nThe first two statements in the above example work - “Hello” is a string, and is thus a valid statement equivalent to typing “2” into the console and getting “2” back out. The second command, print(\"Hello\"), does the same thing - “Hello” is returned as the result. The third command, I love R, however, results in an error - there is an unexpected symbol (the space) in the statement. R thinks we are telling it to do something with variables I and love (which are not defined), and it doesn’t know what we want it to do to the two objects.\nSuppose we define I and love as variables by putting a value into each object using <-, which is the assignment operator. Then, typing “I love” into the console generates the same error, and R tells us “hey, there’s an unexpected symbol here” - in this case, maybe we meant to add the two variables together.\n\n\n\nR console with commands “Hello”, print(“Hello”), and (unquoted) “I love R”, which causes an error. Defining variables I and love provides us a context in which R’s error message about unexpected symbols makes sense - R is reminding us that we need a numerical operator in between the two variable names.\n\n\n\n\nIn your python chunk or script, type in 2+2 and hit Ctrl+Enter (or Cmd+Enter on a mac). Look down to the Console (which is usually below the editor window) and see if 4 appears. If you’re like me, output shows up in two places at once:\n\n\n\nLocation\nPicture\n\n\n\n\nChunk\n\n\n\nScript\n\n\n\nConsole\n\n\n\n\nNotice that in the console, you get a bit of insight into how RStudio is running the python code: we see reticulate::repl_python(), which is R code telling R to run the line in Python. The python console has >>> instead of > to indicate that python is waiting for instructions.\nNotice also that the only difference between the R and python script file screenshots is that there is a different logo on the documents: . Personally, I think it’s easier to work in a markdown document and keep my notes with specific chunks labeled by language when I’m learning the two languages together, but when you are writing code for a specific project in a single language, it is probably better to use a script file specific to that language.\nIf you want to start the python console by itself (without a script or working in a markdown document), you can simply type reticulate::repl_python() into the R console. \nR is nice enough to remind you that to end the conversation with python, you just need to type “exit” or “quit”.\nIf you want to start a python console outside of RStudio, bring up your command prompt (Darwin on mac, Konsole on Linux, CMD on Windows) and type python3 into that window and you should see the familiar >>> waiting for a command."
  },
  {
    "objectID": "03-finding-your-way.html#talking-to-python-and-r---script-mode",
    "href": "03-finding-your-way.html#talking-to-python-and-r---script-mode",
    "title": "3  Finding your way in R and Python",
    "section": "3.4 Talking to Python and R - Script Mode",
    "text": "In the last chapter, we played around with scripts and markdown documents for python and R. In the last section, we played with interactive mode by typing R and python commands into a console or running code chunks interactively using the Run button or Ctrl/Cmd + Enter (which is the keyboard shortcut).\nYou may be learning to program in R and python because it’s a required part of the curriculum, but hopefully, you also have some broader ideas of what you might do with either language - process data, make pretty pictures, write a program to trigger the computer uprising…\nScripts are best used when you have a thing you want to do, and you will need to do that thing many times, perhaps with different input data. Suppose that I have a text file and I want to pull out the most common word in that file. In the next few examples, I will show you how to do this in R and python, and at the same time, demonstrate the difference between interactive mode and script mode in both languages. In each example, try to compare to the previous example to identify whether something is running as a full script or in interactive mode, and how it is launched (in R? at the command line?).\nSeverance (2016) provides a handy python program to count words. This program is meant to be run on the command line, and it will run for any specified text file.\n::: ex\n\n3.4.1 Example: Counting Words\n\nPython, on the Command LinePython within RStudioPython in Interactive Mode (RStudio)R within RStudioR in Interactive ModeR on the Command Line\n\n\nDownload words.py to your computer and open up a command line terminal in the location where you saved the file.\nBefore you run the script, save Oliver Twist to the same folder as dickens-oliver-627.txt (you can use another file name, but you will have to adjust your response to the program)\n\nname = input('Enter file:')\nhandle = open(name, 'r')\ncounts = dict()\n\nfor line in handle:\n    words = line.split()\n    for word in words:\n        counts[word] = counts.get(word, 0) + 1\n\nbigcount = None\nbigword = None\nfor word, count in list(counts.items()):\n    if bigcount is None or count > bigcount:\n        bigword = word\n        bigcount = count\n\nprint(bigword, bigcount)\n\n# Code: http://www.py4e.com/code3/words.py\n\nIn your terminal, type in python words.py. If all goes well, you should get a prompt that tells you to enter a file name. Type in dickens-oliver-627.txt, and the program will read in the file and execute the program according to the instructions shown above. You don’t need to understand what is happening in this program (just like you don’t need to understand what is happening in the R code above either) – you get the answer anyways: the most common word, according to the output from the program, is\nthe 8854\nThat is, the word the occurs 8854 times in the text.\n\n\n\nScreenshot of folder and python script evaluation, showing how to run the python script in the terminal and get the count of the most common word, ‘the’, in the file dickens-oliver-627.txt\n\n\n\n\nWe can run this script in interactive mode in RStudio if we want to: Open the words.py file you downloaded in RStudio.\n\n\n\nRstudio screenshot showing the words.py file opened, with a green highlighted rectangle around the button “Source Script” which allows you to run the file in RStudio.\n\n\nClick the “Source Script” button highlighted in green above, and then look at the console below the script window:\n Once you enter the path to the text file – this time, from the project working directory – you get the same answer. It can be a bit tricky to figure out what your current working directory is in RStudio, but in the R console you can get that information with the getwd() command.\n\n\n\nRStudio screenshot of console window with getwd() command and result\n\n\nSince I know that I have stored the text file in the data subdirectory of the stat151book folder, I can type in data/dickens-oliver-627.txt and the python program can find my file.\nIn the above example, RStudio is functioning essentially like a terminal window - it runs the script as a single file, and once it has your input, all commands are executed one after the other automatically. This is convenient if you want to test the whole block of code at once, but it can be more useful to test each line individually and “play” with the output a bit (or modify code line-by-line).\n\n\nSuppose we want to modify this python script to be more like the R script, where we tell python what the file name is in the file itself, instead of waiting for user input at the terminal.\nInstead of using the input command, I just provide python with a string that contains the path to the file. If you have downloaded the text file to a different folder and RStudio’s working directory is set to that folder, you would change the first line to name = \"dickens-oliver-627.txt\" - I have set things up to live in a data folder because if I had all of the files in the same directory where this book lives, I would never be able to find anything.\nCreate a new python script file in RStudio (File -> New File -> Python Script) and paste in the following lines of code, adjusting the path to the text file appropriately.\n\nname = \"data/dickens-oliver-627.txt\"\nhandle = open(name, 'r')\ncounts = dict()\n\nfor line in handle:\n    words = line.split()\n    for word in words:\n        counts[word] = counts.get(word, 0) + 1\n\nbigcount = None\nbigword = None\nfor word, count in list(counts.items()):\n    if bigcount is None or count > bigcount:\n        bigword = word\n        bigcount = count\n\nprint(bigword, bigcount)\n\n# Code: http://www.py4e.com/code3/words.py\n\nthe 8854\n\n\n\n\n\nAbove script in the RStudio text editor window, with the first 3 lines of code highlighted\n\n\nWith the first 3 lines highlighted, click the Run button.\n We can examine the objects that we have defined this far in the program by typing their names into the console directly.\n\n\n\nRStudio python console allows us to examine the objects we have defined after the first 3 lines of code have been run. We can see that counts is an empty object, handle is a pointer to a text file, and name is a string with the path to the text file – so far, so good.\n\n\nIf we want to continue walking through the program chunk by chunk, we can run the next four lines of code. Lines 5-8 are a for loop, so we should run them all at once unless we want to fiddle with how the for loop works.\n\n\n\nRStudio editor window with the next four lines of the code chunk highlighted. If we click the Run button, we can tell python to evaluate these few lines of code, and then we can see what the objects we’ve defined look like once that has been done\n\n\nSelect lines 5-8 as shown above, and click the Run button. Your console window should update with additional lines of code. You can type in counts after that has been evaluated to see what the counts object looks like now.\n\n\n\nRStudio python console with lines 5-8 run and the counts object displayed. Counts is now filled with words and corresponding integer counts of the frequency of that word’s appearance in the text\n\n\nThe next few lines of code determine which word has the highest count. We won’t get into the details here, but to finish out the running of the program, select lines 10-17 and run them in RStudio.\n\n\n\nRStudio editor window and console showing the results when lines 10-17 are evaluated. It is clear that line 17 results in the console output of the 8854\n\n\nRunning scripts in interactive mode or within RStudio is much more convenient if you are still working on the script - it allows you to debug the script line-by-line if necessary. Running a script at the terminal (like we did above) is sometimes more convenient if you have a pre-written script that you know already works. Both modes are useful, but for the time being you will probably be running scripts within your development environment (RStudio or VSCode or any other IDE you prefer) more often than at the command line.\n\n\nJust for fun, let’s work with Oliver Twist, by Charles Dickens, which I have saved here.\n\n# Read in the file\ntext <- readLines(\"dickens-oliver-627.txt\")\n\n# Split the lines of text into separate words\ntext <- strsplit(text, \" \")\n\n# Simplify the list\ntext <- unlist(text)\n\n# Count up the number of occurrences of each word\nword_freq <- table(text)\n\n# Sort the table by decreasing frequency\nword_freq <- sort(word_freq, decreasing = T)\n\n# Show the counts for the most common 10 words\nword_freq[1:10]\n\nMake a new R script (File -> New File -> R script) and copy the above code into R, or download the file to your computer directly and open the downloaded file in RStudio.\nIn the R console, run the command getwd() to see where R is running from. This is your “working directory”.\n\n\n\nR editor window with relevant script, with R console shown below. My working directory is /home/susan/Projects/Class/unl-stat151/stat151book/demo; yours will be different.\n\n\nSave the copy of Oliver Twist to the file dickens-oliver-627.txt in the folder that getwd() spit out. You can test that you have done this correctly by typing list.files() into the R console window and hitting enter. It is very important that you know where on your computer R is looking for files - otherwise, you will constantly get “file not found” errors, and that will be very annoying.\n\n\n\nR editor window with relevant script, with R console shown below. dickens-oliver-627.txt is in the working directory, so we can proceed.\n\n\nUse the “Run” button to run the script and see what the output is. How many times does ‘the’ appear in the file?\n\n\nUsing the file you created above, let’s examine what each line does in interactive mode.\n\n# Read in the file\ntext <- readLines(\"dickens-oliver-627.txt\")\n\nSelect the above line and click the “Run” button in RStudio. Once you’ve done that, type in text[1:5] in the R console to see the first 5 lines of the file.\n\n\n\nRStudio editor window with the first 2 lines of the words-noinput.R file selected. The screenshot also shows the console window after running the first 2 lines of the R file, with the text[1:5] command run interactively afterwards showing the first 5 lines of the text file we read in.\n\n\nRun the next line of code using the run button (or click on the line of code and hit Ctrl/Cmd + Enter).\n\n# Split the lines of text into separate words\ntext <- strsplit(text, \" \")\n\nType in text[[1]] to see what the text object looks like now.\n\ntext[[1]]\n\n [1] \"The\"       \"Project\"   \"Gutenberg\" \"Etext\"     \"of\"        \"Oliver\"   \n [7] \"Twist\"     \"by\"        \"Charles\"   \"Dickens\"  \n\n\n\n\n\nScreenshot of RStudio editor window with lines of code highlighted, plus RStudio console with the code as run and text[[1]] showing the first entry in the text object - a list of the separate words in the first line of the text file.\n\n\n\n# Simplify the list\ntext <- unlist(text)\ntext[[1]]\n\n[1] \"The\"\n\ntext[1:20]\n\n [1] \"The\"       \"Project\"   \"Gutenberg\" \"Etext\"     \"of\"        \"Oliver\"   \n [7] \"Twist\"     \"by\"        \"Charles\"   \"Dickens\"   \"#13\"       \"in\"       \n[13] \"our\"       \"series\"    \"by\"        \"Charles\"   \"Dickens\"   \"Copyright\"\n[19] \"laws\"      \"are\"      \n\n\nRunning unlist on text simplifies the object so that it is now a single vector of every word in the file, without regard for which line it appears on.\n\n# Count up the number of occurrences of each word\nword_freq <- table(text)\nword_freq[1:5]\n\ntext\n            _I_     --'    --by --kneel \n   4558       4       1       1       1 \n\n\nThe next line assembles a table of frequency counts in text. There are 4558 spaces, 4 occurrences of the string _I_, and so on.\n\n# Sort the table by decreasing frequency\nword_freq <- sort(word_freq, decreasing = T)\nword_freq[1:5]\n\ntext\n the  and        to   of \n8854 4902 4558 3767 3763 \n\n\nWe can then sort word_freq so that the most frequent words are listed first. The final line just prints out the first 10 words instead of the first 5.\n\n\nDownload the following file to your working directory: words.R, or paste the following code into a new R script and save it as words.R\n\n# Take arguments from the command line\nargs <- commandArgs(TRUE)\n\n# Read in the file\ntext <- readLines(args[1])\n\n# Split the lines of text into separate words\ntext <- strsplit(text, \" \")\n\n# Simplify the list\ntext <- unlist(text)\n\n# Count up the number of occurrences of each word\nword_freq <- table(text)\n\n# Sort the table by decreasing frequency\nword_freq <- sort(word_freq, decreasing = T)\n\n# Show the counts for the most common 10 words\nword_freq[1:10]\n\nIn a terminal window opened at the location you saved the file (and the corresponding text file), enter the following: Rscript words.R dickens-oliver-627.txt.\nHere, Rscript is the command that tells R to evaluate the file, words.R is the R code to run, and dickens-oliver-627.txt is an argument to your R script that tells R where to find the text file. This is similar to the python code, but instead, the user passes the file name in at the same time as the script instead of having to wait around a little bit.\n\n\n\n\n\n3.4.2 Comparing Python and R\nThis is one good example of the difference in culture between python and R: python is a general-purpose programming language, where R is a domain specific programming language. In both languages, I’ve shown you how I would run the script by default first - in python, I would use a pre-built script to run things, and in R I would open things up in RStudio and source the script rather than running R from the command line.\nThis is a bit of a cultural difference – because python is a general purpose programming language, it is easy to use for a wide variety of tasks, and is a common choice for creating scripts that are used on the command line. R is a domain-specific language, so it is extremely easy to use R for data analysis, but that tends to take place (in my experience) in an interactive or script-development setting using RStudio. It is less natural to me to write an R script that takes input from the user on the command line, even though obviously R is completely capable of doing that task. More commonly, I will write an R script for my own use, and thus there is no need to make it easy to use on the command line, because I can just change it in interactive mode. Python scripts, on the other hand, may be written for a novice to use at the command line with no idea of how to write or modify python code. This is a subtle difference, and may not make a huge impression on you now, but it is something to keep in mind as you learn to write code in each language – the culture around python and the culture around R are slightly different, and this affects how each language is used in practice."
  },
  {
    "objectID": "03-finding-your-way.html#getting-help",
    "href": "03-finding-your-way.html#getting-help",
    "title": "3  Finding your way in R and Python",
    "section": "3.5 Getting help",
    "text": "In both R and python, you can access help with a ? - the order is just slightly different.\nSuppose we want to get help on a for loop in either language.\nIn R, we can run this line of code to get help on for loops.\n\n?`for`\n\nBecause for is a reserved word in R, we have to use backticks (the key above the TAB key) to surround the word for so that R knows we’re talking about the function itself. Most other function help can be accessed using ?function_name.\nIn python, we use for? to access the same information.\n\nfor?\n\n(You will have to run this in interactive mode for it to work in either language)\nw3schools has an excellent python help page that may be useful as well - usually, these pages will have examples. A similar set of pages exists for R help on basic functions\n\n\nA nice explanation of the difference between an interpreter and a compiler. Both Python and R are interpreted languages that are compiled from lower-level languages like C.\n\n\n\n\n\n\nSeverance, Dr Charles Russell. 2016. Python for Everybody: Exploring Data in Python 3. Edited by Sue Blumenberg and Elliott Hauser. Ann Arbor, MI: CreateSpace Independent Publishing Platform. https://www.py4e.com/html3/."
  },
  {
    "objectID": "04-data-types.html#values-and-types",
    "href": "04-data-types.html#values-and-types",
    "title": "4  Basic Data Types",
    "section": "4.1 Values and Types",
    "text": "Let’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn both R and python, there are some very basic data types:\n\nlogical or boolean - FALSE/TRUE or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\ndouble or float or numeric- decimal numbers.\n\nfloat is short for floating-point value.\ndouble is a floating-point value with more precision (“double precision”).1\nR uses the name numeric to indicate a decimal value, regardless of precision.\n\ncharacter or string - holds text, usually enclosed in quotes.\n\nIf you don’t know what type a value is, both R and python have functions to help you with that.\n\nRPython\n\n\n\nclass(FALSE)\nclass(2L) # by default, R treats all numbers as numeric/decimal values. \n          # The L indicates that we're talking about an integer. \nclass(2)\nclass(\"Hello, programmer!\")\n\n[1] \"logical\"\n[1] \"integer\"\n[1] \"numeric\"\n[1] \"character\"\n\n\n\n\n\ntype(False)\ntype(2)\ntype(3.1415)\ntype(\"This is python code\")\n\n<class 'bool'>\n<class 'int'>\n<class 'float'>\n<class 'str'>\n\n\n\n\n\n\nIn R, boolean values are TRUE and FALSE, but in Python they are True and False. Capitalization matters a LOT.\nOther things matter too: if we try to write a million, we would write it 1000000 instead of 1,000,000 (in both languages). Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data."
  },
  {
    "objectID": "04-data-types.html#variables",
    "href": "04-data-types.html#variables",
    "title": "4  Basic Data Types",
    "section": "4.2 Variables",
    "text": "Programming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\nWe assign variables values using the syntax object_name <- value (R) or object_name = value (python). You can read this as “object name gets value” in your head.\n\nRPython\n\n\n\nmessage <- \"So long and thanks for all the fish\"\nyear <- 2025\nthe_answer <- 42L\nearth_demolished <- FALSE\n\n\n\n\nmessage = \"So long and thanks for all the fish\"\nyear = 2025\nthe_answer = 42\nearth_demolished = False\n\n\n\n\n\nNote that in R, we assign variables values using the <- operator, where in Python, we assign variables values using the = operator. Technically, = will work for assignment in both languages, but <- is more common than = in R by convention.\n\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\n\n4.2.1 Valid Names\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n– Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, _, and . in R. In Python, object names must start with a letter and can consist of letters, numbers, and _ (that is, . is not a valid character in a Python variable name). While it is technically fine to use uppercase variable names in Python, it’s recommended that you use lowercase names for variables (you’ll see why later).\nWhat happens if we try to create a variable name that isn’t valid?\nIn both languages, starting a variable name with a number will get you an error message that lets you know that something isn’t right - “unexpected symbol” in R and “invalid syntax” in python.\n\nRPython\n\n\n\n1st_thing <- \"check your variable names!\"\n\nError: <text>:1:2: unexpected symbol\n1: 1st_thing\n     ^\n\n\n\n\n\n1st_thing <- \"check your variable names!\"\n\nNote: Run the above chunk in your python window - the book won’t compile if I set it to evaluate 😿. It generates an error of SyntaxError: invalid syntax (<string>, line 1)\n\nsecond.thing <- \"this isn't valid\"\n\nError in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'second' is not defined\n\nDetailed traceback:\n  File \"<string>\", line 1, in <module>\n\n\nIn python, trying to have a . in a variable name gets a more interesting error: “ is not defined”. This is because in python, some objects have components and methods that can be accessed with .. We’ll get into this more later, but there is a good reason for python’s restriction about not using . in variable names.\n\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\nWhy is naming things hard? - Blog post by Neil Kakkar\n\nThere are a few different conventions for naming things that may be useful:\n\nsome_people_use_snake_case, where words are separated by underscores\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\nsome.people.use.periods (in R, obviously this doesn’t work in python)\nA few people mix conventions with variables_thatLookLike.this and they are almost universally hated 👿\n\nAs long as you pick ONE naming convention and don’t mix-and-match, you’ll be fine. It will be easier to remember what you named your variables (or at least guess) and you’ll have fewer moments where you have to go scrolling through your script file looking for a variable you named."
  },
  {
    "objectID": "04-data-types.html#type-conversions",
    "href": "04-data-types.html#type-conversions",
    "title": "4  Basic Data Types",
    "section": "4.3 Type Conversions",
    "text": "We talked about values and types above, but skipped over a few details because we didn’t know enough about variables. It’s now time to come back to those details.\nWhat happens when we have an integer and a numeric type and we add them together? Hopefully, you don’t have to think too hard about what the result of 2 + 3.5 is, but this is a bit more complicated for a computer for two reasons: storage, and arithmetic.\nIn days of yore, programmers had to deal with memory allocation - when declaring a variable, the programmer had to explicitly define what type the variable was. This tended to look something like the code chunk below:\nint a = 1\ndouble b = 3.14159\nTypically, an integer would take up 32 bits of memory, and a double would take up 64 bits, so doubles used 2x the memory that integers did. Both R and python are dynamically typed, which means you don’t have to deal with any of the trouble of declaring what your variables will hold - the computer automatically figures out how much memory to use when you run the code. So we can avoid the discussion of memory allocation and types because we’re using higher-level languages that handle that stuff for us2.\nBut the discussion of types isn’t something we can completely avoid, because we still have to figure out what to do when we do operations on things of two different types - even if memory isn’t a concern, we still have to figure out the arithmetic question.\nSo let’s see what happens with a couple of examples, just to get a feel for type conversion (aka type casting or type coercion), which is the process of changing an expression from one data type to another.\n\nRPython\n\n\n\nmode(2L + 3.14159) # add 2 and pi\n\n[1] \"numeric\"\n\nmode(2L + TRUE) # add integer 2 and TRUE\n\n[1] \"numeric\"\n\nmode(TRUE + FALSE) # add TRUE and FALSE\n\n[1] \"numeric\"\n\n\nIn R, all of the examples above are ‘numeric’ - basically, a catch-all class for things that are in some way, shape, or form numbers. Integers and decimal numbers are both numeric, but so are logicals (because they can be represented as 0 or 1).\n\n\n\ntype(2 + 3.14159)\n\n<class 'float'>\n\ntype(2 + True)\n\n<class 'int'>\n\ntype(True + False)\n\n<class 'int'>\n\n\nIn python, by contrast, anything without a decimal point is converted into an integer - essentially, python tries to minimize memory used without losing data. So it will never convert a float into an integer implicitly - if you want Python to do that, you’ll have to tell it to do so directly.\n\n\n\nYou may be asking yourself at this point why this matters, and that’s a decent question. We will eventually be reading in data from spreadsheets and other similar tabular data, and types become very important at that point, because we’ll have to know how R and python both handle type conversions.\n\nIn class activity\nDo a bit of experimentation - what happens when you try to add a string and a number? Which types are automatically converted to other types? Fill in the following table in your notes:\nAdding a ___ and a ___ produces a ___:\n\n\n\nLogical\nInteger\nDecimal\nString\n\n\n\n\n\nLogical\n\n\n\n\n\n\nInteger\n\n\n\n\n\n\nDecimal\n\n\n\n\n\n\nString\n\n\n\n\n\n\n\n\nAbove, we looked at automatic type conversions, but in many cases, we also may want to convert variables manually, specifying exactly what type we’d like them to be. A common application for this in data analysis is when there are “*” or “.” or other indicators in an otherwise numeric column of a spreadsheet that indicate missing data: when this data is read in, the whole column is usually read in as character data. So we need to know how to tell R and python that we want our string to be treated as a number, or vice-versa.\n\nRPython\n\n\nIn R, we can explicitly convert a variable’s type using as.XXX() functions, where XXX is the type you want to convert to (as.numeric, as.integer, as.logical, as.character, etc.).\n\nx <- 3\ny <- \"3.14159\"\n\nx + y\n\nError in x + y: non-numeric argument to binary operator\n\nx + as.numeric(y)\n\n[1] 6.14159\n\n\n\n\nIn python, the same basic idea holds true, but in python, we just use the variable type as a function: int(), float(), str(), and bool().\n\nx = 3\ny = \"3.14159\"\n\nx + y\n\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: unsupported operand type(s) for +: 'int' and 'str'\n\nDetailed traceback:\n  File \"<string>\", line 1, in <module>\n\nx + float(y)\n\n6.14159"
  },
  {
    "objectID": "04-data-types.html#operators-and-functions",
    "href": "04-data-types.html#operators-and-functions",
    "title": "4  Basic Data Types",
    "section": "4.4 Operators and Functions",
    "text": "In addition to variables, functions are extremely important in programming.\nLet’s first start with a special class of functions called operators. You’re probably familiar with operators as in arithmetic expressions: +, -, /, *, and so on.\nHere are a few of the most important ones:\n\n\n\nOperation\nR symbol\nPython symbol\n\n\n\n\nAddition\n+\n+\n\n\nSubtraction\n-\n-\n\n\nMultiplication\n*\n*\n\n\nDivision\n/\n/\n\n\nInteger Division\n%/%\n//\n\n\nModular Division\n%%\n%\n\n\nExponentiation\n^\n**\n\n\n\nNote that integer division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\n\nRPython\n\n\nSo 14 %/% 3 in R would be 4, and 14 %% 3 in R would be 2.\n\n14 %/% 3\n\n[1] 4\n\n14 %% 3\n\n[1] 2\n\n\n\n\n\n14 // 3\n\n4\n\n14 % 3\n\n2\n\n\n\n\n\nNote that these operands are all intended for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated (and different between R and python).\n\n4.4.1 Order of Operations\nBoth R and Python operate under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\nRPython\n\n\n\n(1+1)^(5-2) # 2 ^ 3 = 8\n\n[1] 8\n\n1 + 2^3 * 4 # 1 + (8 * 4)\n\n[1] 33\n\n3*1^3 # 3 * 1\n\n[1] 3\n\n\n\n\n\n(1+1)**(5-2)\n\n8\n\n1 + 2**3*4\n\n33\n\n3*1**3\n\n3\n\n\n\n\n\n\n\n4.4.2 String Operations\nPython has some additional operators that work on strings. In R, you will have to use functions to perform these operations, as R does not have string operators.\n\nPythonR\n\n\nIn Python, + will concatenate (stick together) two strings, and multiplying a string by an integer will repeat the string the specified number of times\n\n\"first \" + \"second\"\n\n'first second'\n\n\"hello \" * 3\n\n'hello hello hello '\n\n\n\n\nIn R, to concatenate things, we need to use functions: paste or paste0:\n\npaste(\"first\", \"second\", sep = \" \")\n\n[1] \"first second\"\n\npaste(\"first\", \"second\", collapse = \" \")\n\n[1] \"first second\"\n\npaste(c(\"first\", \"second\"), sep = \" \") # sep only works on separate parameters\n\n[1] \"first\"  \"second\"\n\npaste(c(\"first\", \"second\"), collapse = \" \") # collapse works on vectors\n\n[1] \"first second\"\n\npaste(c(\"a\", \"b\", \"c\", \"d\"), \n      c(\"first\", \"second\", \"third\", \"fourth\"), \n      sep = \"-\", collapse = \" \")\n\n[1] \"a-first b-second c-third d-fourth\"\n\n# sep is used to collapse parameters, then collapse is used to collapse vectors\n\npaste0(c(\"a\", \"b\", \"c\"))\n\n[1] \"a\" \"b\" \"c\"\n\npaste0(\"a\", \"b\", \"c\") # equivalent to paste(..., sep = \"\")\n\n[1] \"abc\"\n\n\nYou don’t need to understand the details of this at this point in the class, but it is useful to know how to combine strings in both languages.\n\n\n\n\n\n4.4.3 Functions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, operators (like those above) are a special type of functions – but we aren’t going to get into that now.\nWe’re also not going to talk about how to create our own functions just yet. Instead, I’m going to show you how to use functions.\nIt may be helpful at this point to print out the R reference card3 and the Python reference card4. These cheat sheets contain useful functions for a variety of tasks in each language.\nMethods are a special type of function that operate on a specific variable type. In Python, methods are applied using the syntax variable.method_name(). So, you can get the length of a string variable my_string using my_string.length().\nR has methods too, but they are invoked differently. In R, you would get the length of a string variable using length(my_string).\nRight now, it is not really necessary to know too much more about functions than this: you can invoke a function by passing in arguments, and the function will do a task and return the value.\n\n\nIn class activity\nTry out some of the functions mentioned on the R and Python cheatsheets.\nCan you figure out how to define a list or vector of numbers? If so, can you use a function to calculate the maximum value?\nCan you find the R functions that will allow you to repeat a string variable multiple times or concatenate two strings? Can you do this task in Python?"
  },
  {
    "objectID": "05-vectors.html#mathematical-logic",
    "href": "05-vectors.html#mathematical-logic",
    "title": "5  Vectors, Matrices, Arrays, and Control Structures",
    "section": "5.1 Mathematical Logic",
    "text": "Before we start talking about data structures and control structures, though, we’re going to take a minute to review some concepts from mathematical logic. This will be useful for both data structures and control structures, so stick with me for a few minutes.\n\n5.1.1 And, Or, and Not\nWe can combine logical statements using and, or, and not.\n\n(X AND Y) requires that both X and Y are true.\n(X OR Y) requires that one of X or Y is true.\n(NOT X) is true if X is false, and false if X is true. Sometimes called negation.\n\nIn R, we use ! to symbolize NOT, in Python, we use ~ for vector-wise negation (NOT).\nOrder of operations dictates that NOT is applied before other operations. So NOT X AND Y is read as (NOT X) AND (Y). You must use parentheses to change the way this is interpreted.\n\nRPython\n\n\n\nx <- c(TRUE, FALSE, TRUE, FALSE)\ny <- c(TRUE, TRUE, FALSE, FALSE)\n\nx & y # AND\n\n[1]  TRUE FALSE FALSE FALSE\n\nx | y # OR\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n!x & y # NOT X AND Y\n\n[1] FALSE  TRUE FALSE FALSE\n\nx & !y # X AND NOT Y\n\n[1] FALSE FALSE  TRUE FALSE\n\n\n\n\n\nimport numpy as np\nx = np.array([True, False, True, False])\ny = np.array([True, True, False, False])\n\nx & y\n\narray([ True, False, False, False])\n\nx | y\n\narray([ True,  True,  True, False])\n\n~x & y\n\narray([False,  True, False, False])\n\nx & ~y\n\narray([False, False,  True, False])\n\n\n\n\n\n\n\n5.1.2 De Morgan’s Laws\nDe Morgan’s Laws are a set of rules for how to combine logical statements. You can represent them in a number of ways:\n\nNOT(A or B) is equivalent to NOT(A) and NOT(B)\nNOT(A and B) is equivalent to NOT(A) or NOT(B)\n\n\nDefinitionsDeMorgan’s First LawDeMorgan’s Second Law\n\n\n Suppose that we set the convention that .\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A OR B (aka NOT (A OR B)) is the same as the region that is outside of (NOT A) and (NOT B)\n\n\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A AND B (aka NOT (A AND B)) is the same as the region that is outside of (NOT A) OR (NOT B)"
  },
  {
    "objectID": "05-vectors.html#data-structures",
    "href": "05-vectors.html#data-structures",
    "title": "5  Vectors, Matrices, Arrays, and Control Structures",
    "section": "5.2 Data Structures",
    "text": "In the previous chapter, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complicated arrangements of information.\n\n\n\nHomogeneous\nHeterogeneous\n\n\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\n\n5.2.1 Lists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\n\nA lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\n\nRPython\n\n\n\nx <- list(\"a\", 3, FALSE)\nx\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] FALSE\n\n\n\n\n\nx = [\"a\", 3, False]\nx\n\n['a', 3, False]\n\n\n\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. We call that process indexing.\n\n5.2.1.1 Indexing\nEvery element in a list has an index (a location, indicated by an integer position)1.\n\nR conceptR codePython conceptPython code\n\n\nIn R, we count from 1.\n\n\n\nAn R-indexed lego list, counting from 1 to 5\n\n\n\n\n\nx <- list(\"a\", 3, FALSE)\n\nx[1] # This returns a list\n\n[[1]]\n[1] \"a\"\n\nx[1:2] # This returns multiple elements in the list\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 3\n\nx[[1]] # This returns the item\n\n[1] \"a\"\n\nx[[1:2]] # This doesn't work - you can only use [[]] with a single index\n\nError in x[[1:2]]: subscript out of bounds\n\n\nIn R, list indexing with [] will return a list with the specified elements.\nTo actually retrieve the item in the list, use [[]]. The only downside to [[]] is that you can only access one thing at a time.\n\n\nIn Python, we count from 0.\n\n\n\nA python-indexed lego list, counting from 0 to 4\n\n\n\n\n\nx = [\"a\", 3, False]\n\nx[0]\n\n'a'\n\nx[1]\n\n3\n\nx[0:2]\n\n['a', 3]\n\n\nIn Python, we can use single brackets to get an object or a list back out, but we have to know how slices work. Essentially, in Python, 0:2 indicates that we want objects 0 and 1, but want to stop at 2 (not including 2). If you use a slice, Python will return a list; if you use a single index, python just returns the value in that location in the list.\n\n\n\nWe’ll talk more about indexing as it relates to vectors, but indexing is a general concept that applies to just about any multi-value object.\n\n\n\n5.2.2 Vectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want: \n\n\n5.2.2.1 Indexing by Location\nEach element in a vector has an index - an integer telling you what the item’s position within the vector is. I’m going to demonstrate indices with the string vector\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\n1-indexed language\n0-indexed language\n\n\nCount elements as 1, 2, 3, 4, …, N\nCount elements as 0, 1, 2, 3, , …, N-1\n\n\n\n\n\n\n\n\nRPython VectorsPython Series (Pandas)\n\n\nIn R, we create vectors with the c() function, which stands for “concatenate” - basically, we stick a bunch of objects into a row.\n\ndigits_pi <- c(3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5)\n\n# Access individual entries\ndigits_pi[1]\n\n[1] 3\n\ndigits_pi[2]\n\n[1] 1\n\ndigits_pi[3]\n\n[1] 4\n\n# R is 1-indexed - a list of 11 things goes from 1 to 11\ndigits_pi[0]\n\nnumeric(0)\n\ndigits_pi[11]\n\n[1] 5\n\n# Print out the vector\ndigits_pi\n\n [1] 3 1 4 1 5 9 2 6 5 3 5\n\n\n\n\nIn python, we create vectors using the array function in the numpy module. To add a python module, we use the syntax import <name> as <nickname>. Many modules have conventional (and very short) nicknames - for numpy, we will use np as the nickname. Any functions we reference in the numpy module will then be called using np.fun_name() so that python knows where to find them.2\n\nimport numpy as np\ndigits_list = [3,1,4,1,5,9,2,6,5,3,5]\ndigits_pi = np.array(digits_list)\n\n# Access individual entries\ndigits_pi[0]\n\n3\n\ndigits_pi[1]\n\n1\n\ndigits_pi[2]\n\n# Python is 0 indexed - a list of 11 things goes from 0 to 10\n\n4\n\ndigits_pi[0]\n\n3\n\ndigits_pi[11] \n\n# multiplication works on the whole vector at once\n\nError in py_call_impl(callable, dots$args, dots$keywords): IndexError: index 11 is out of bounds for axis 0 with size 11\n\nDetailed traceback:\n  File \"<string>\", line 1, in <module>\n\ndigits_pi * 2\n\n# Print out the vector\n\narray([ 6,  2,  8,  2, 10, 18,  4, 12, 10,  6, 10])\n\nprint(digits_pi)\n\n[3 1 4 1 5 9 2 6 5 3 5]\n\n\n\n\nPython has multiple things that look like vectors, including the pandas library’s Series structure. A Series is a one-dimensional array-like object containing a sequence of values and an associated array of labels (called its index).\n\nimport pandas as pd\ndigits_pi = pd.Series([3,1,4,1,5,9,2,6,5,3,5])\n\n# Access individual entries\ndigits_pi[0]\n\n3\n\ndigits_pi[1]\n\n1\n\ndigits_pi[2]\n\n# Python is 0 indexed - a list of 11 things goes from 0 to 10\n\n4\n\ndigits_pi[0]\n\n3\n\ndigits_pi[11] \n\n# logical indexing works here too\n\nError in py_call_impl(callable, dots$args, dots$keywords): KeyError: 11\n\nDetailed traceback:\n  File \"<string>\", line 1, in <module>\n  File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/series.py\", line 958, in __getitem__\n    return self._get_value(key)\n  File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/series.py\", line 1069, in _get_value\n    loc = self.index.get_loc(label)\n  File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexes/range.py\", line 387, in get_loc\n    raise KeyError(key) from err\n\ndigits_pi[digits_pi > 3]\n# simple multiplication works in a vectorized manner\n# that is, the whole vector is multiplied at once\n\n2     4\n4     5\n5     9\n7     6\n8     5\n10    5\ndtype: int64\n\ndigits_pi * 2\n\n# Print out the series\n\n0      6\n1      2\n2      8\n3      2\n4     10\n5     18\n6      4\n7     12\n8     10\n9      6\n10    10\ndtype: int64\n\nprint(digits_pi)\n\n0     3\n1     1\n2     4\n3     1\n4     5\n5     9\n6     2\n7     6\n8     5\n9     3\n10    5\ndtype: int64\n\n\nThe Series object has a list of labels in the first printed column, and a list of values in the second. If we want, we can specify the labels manually to use as e.g. plot labels later:\n\nimport pandas as pd\nweekdays = pd.Series(['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'], index = ['S', 'M', 'T', 'W', 'R', 'F', 'Sat'])\n\n# access individual objs\nweekdays[0]\n\n'Sunday'\n\nweekdays[1]\n\n'Monday'\n\nweekdays['S']\n\n'Sunday'\n\nweekdays['Sat']\n\n# access the index\n\n'Saturday'\n\nweekdays.index\n\nIndex(['S', 'M', 'T', 'W', 'R', 'F', 'Sat'], dtype='object')\n\nweekdays.index[6] = 'Z' # you can't assign things to the index to change it\n\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: Index does not support mutable operations\n\nDetailed traceback:\n  File \"<string>\", line 1, in <module>\n  File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 5021, in __setitem__\n    raise TypeError(\"Index does not support mutable operations\")\n\nweekdays\n\nS         Sunday\nM         Monday\nT        Tuesday\nW      Wednesday\nR       Thursday\nF         Friday\nSat     Saturday\ndtype: object\n\n\n\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific things as well:\n\n\nRPython\n\n\n\nfavorite_cats <- c(\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\")\n\nfavorite_cats\n\n[1] \"Grumpy\"   \"Garfield\" \"Jorts\"    \"Jean\"    \n\nfavorite_cats[2] <- \"Nyan Cat\"\n\nfavorite_cats\n\n[1] \"Grumpy\"   \"Nyan Cat\" \"Jorts\"    \"Jean\"    \n\n\n\n\n\nfavorite_cats = [\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\"]\n\nfavorite_cats\n\n['Grumpy', 'Garfield', 'Jorts', 'Jean']\n\nfavorite_cats[1] = \"Nyan Cat\"\n\nfavorite_cats\n\n['Grumpy', 'Nyan Cat', 'Jorts', 'Jean']\n\n\n\n\n\nIf you’re curious about any of these cats, see the footnotes3.\n\n\n\n5.2.2.2 Indexing with Logical Vectors\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that goes along with a vector of another type to use as a logical index.\n\n\n\nlego vectors - a pink/purple hued set of 1x3 bricks representing the data and a corresponding set of 1x1 grey and black bricks representing the logical index vector of the same length\n\n\nIf we let the black lego represent “True” and the grey lego represent “False”, we can use the logical vector to pull out all values in the main vector.\n\n\n\n\n\n\n\nBlack = True, Grey = False\nGrey = True, Black = False\n\n\n\n\n\n\n\n\n\nNote that for logical indexing to work properly, the logical index must be the same length as the vector we’re indexing. This constraint will return when we talk about data frames, but for now just keep in mind that logical indexing doesn’t make sense when this constraint isn’t true.\n\nIndexing with logical vectors in RIndexing with logical vectors in python\n\n\n\n# Define a character vector\nweekdays <- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\nweekend <- c(\"Sunday\", \"Saturday\")\n\n# Create logical vectors\nrelax_days <- c(1, 0, 0, 0, 0, 0, 1) # doing this the manual way\nrelax_days <- weekdays %in% weekend # This creates a logical vector \n                                    # with less manual construction\nrelax_days\n\n[1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nschool_days <- !relax_days # FALSE if weekend, TRUE if not\nschool_days\n\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n\n# Using logical vectors to index the character vector\nweekdays[school_days] # print out all school days\n\n[1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n\n\n\n\n\nimport numpy as np;\n\nanimals = np.array([\"Cat\", \"Dog\", \"Snake\", \"Lizard\", \"Tarantula\", \"Hamster\", \"Gerbil\", \"Otter\"])\n\n# Define a logical vector\ngood_pets = np.array([True, True, False, False, False, True, True, False])\nbad_pets = np.invert(good_pets) # Invert the logical vector \n                                # so True -> False and False -> True\n\nanimals[good_pets]\n\narray(['Cat', 'Dog', 'Hamster', 'Gerbil'], dtype='<U9')\n\nanimals[bad_pets]\n\narray(['Snake', 'Lizard', 'Tarantula', 'Otter'], dtype='<U9')\n\nanimals[~good_pets] # equivalent to using bad_pets\n\narray(['Snake', 'Lizard', 'Tarantula', 'Otter'], dtype='<U9')\n\n\n\n\n\n\n\n5.2.2.3 Reviewing Types\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently-typed things?\n\nRPython\n\n\n\nc(2L, FALSE, 3.1415, \"animal\") # all converted to strings\n\n[1] \"2\"      \"FALSE\"  \"3.1415\" \"animal\"\n\nc(2L, FALSE, 3.1415) # converted to numerics\n\n[1] 2.0000 0.0000 3.1415\n\nc(2L, FALSE) # converted to integers\n\n[1] 2 0\n\n\n\n\n\nimport numpy as np\n\nnp.array([2, False, 3.1415, \"animal\"]) # all converted to strings\n\narray(['2', 'False', '3.1415', 'animal'], dtype='<U32')\n\nnp.array([2, False, 3.1415]) # converted to floats\n\narray([2.    , 0.    , 3.1415])\n\nnp.array([2, False]) # converted to integers\n\narray([2, 0])\n\n\n\n\n\nAs a reminder, this is an example of implicit type conversion - R and python decide what type to use for you, going with the type that doesn’t lose data but takes up as little space as possible.\n\n\nTry it Out!\n\nProblemR SolutionPython Solution\n\n\nCreate a vector of the integers from one to 30. Use logical indexing to pick out only the numbers which are multiples of 3.\n\n\n\nx <- 1:30\nx [ x %% 3 == 0]\n\n [1]  3  6  9 12 15 18 21 24 27 30\n\n\n\n\n\nimport numpy as np\nx = np.array(range(1, 31)) # because python is 0 indexed\nx[ x % 3 == 0]\n\narray([ 3,  6,  9, 12, 15, 18, 21, 24, 27, 30])\n\n\n\n\n\n\nChallengeGeneral SolutionR SolutionPython Solution\n\n\nExtra challenge: Pick out numbers which are multiples of 2 or 3, but not multiples of 6!\n\n\nThis operation is xor, a.k.a. exclusive or. That is, X or Y, but not X AND Y.\nWe can write xor as (X OR Y) & !(X AND Y) – or we can use a predefined function: xor() in R, ^ in python.\n\n\n\nx <- 1:30\n\nx2 <- x %% 2 == 0 # multiples of 2\nx3 <- x %% 3 == 0 # multiples of 3\nx2xor3 <- xor(x2, x3)\nx2xor3_2 <- (x2 | x3) & !(x2 & x3)\nx[x2xor3]\n\n [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\n\nx[x2xor3_2]\n\n [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\n\n\n\n\n\nimport numpy as np\nx = np.array(range(1, 31))\n\nx2 = x % 2 == 0 # multiples of 2\nx3 = x % 3 == 0 # multiples of 3\nx2xor3 = x2 ^ x3\n\nx[x2xor3]\n\narray([ 2,  3,  4,  8,  9, 10, 14, 15, 16, 20, 21, 22, 26, 27, 28])\n\n\n\n\n\n\n\n\n5.2.3 Matrices\nA matrix is the next step after a vector - it’s a set of values arranged in a two-dimensional, rectangular format.\n\nMatrix (Lego)RPython\n\n\n\n\n\nlego depiction of a 3-row, 4-column matrix of 2x2 red-colored blocks\n\n\n\n\n\n# Minimal matrix in R: take a vector, \n# tell R how many rows you want\nmatrix(1:12, nrow = 3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nmatrix(1:12, ncol = 3) # or columns\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n# by default, R will fill in column-by-column\n# the byrow parameter tells R to go row-by-row\nmatrix(1:12, nrow = 3, byrow = T)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n# We can also easily create square matrices \n# with a specific diagonal (this is useful for modeling)\ndiag(rep(1, times = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\n\n\n\nIn python, matrices are just a special case of a class called ndarray - n-dimensional arrays.\n\nimport numpy as np\n# Minimal ndarray in python by typing in the values in a structured format\nnp.array([[0,  1,  2],\n          [3,  4,  5],\n          [6,  7,  8],\n          [9, 10, 11]])\n# This syntax creates a list of the rows we want in our matrix\n\n# Matrix in python using a data vector and size parameters\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n\nnp.reshape(range(0,12), (3,4))\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\nnp.reshape(range(0,12), (4,3))\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n\nnp.reshape(range(0,12), (3,4), order = 'F')\n\narray([[ 0,  3,  6,  9],\n       [ 1,  4,  7, 10],\n       [ 2,  5,  8, 11]])\n\n\nIn python, we create 2-dimensional arrays (aka matrices) either by creating a list of rows to join together or by reshaping a 1-dimensional array. The trick with reshaping the 1-dimensional array is the order argument: ‘F’ stands for “Fortran-like” and ‘C’ stands for “C-like”… so to go by column, you use ‘F’ and to go by row, you use ‘C’. Totally intuitive, right?\n\n\n\nThis class comes before linear algebra in the required course sequence, so most of the problems we’re going to work on will not require much in the way of matrix or array operations. For now, you need the following:\n\nKnow that matrices exist and what they are (2-dimensional arrays of numbers)\nUnderstand how they are indexed (because it is extremely similar to data frames that we’ll work with in the next chapter)\nBe aware that there are lots of functions that depend on matrix operations at their core (including linear regression)\n\n\n5.2.3.1 Indexing in Matrices\nBoth R and python use [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix in R, we would use [3,1] to get to the third row and first column entry; in python, we would use [2,0] (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\nRPython\n\n\n\nmy_mat <- matrix(1:12, nrow = 3, byrow = T)\n\nmy_mat[3,1] <- 500\n\nmy_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]  500   10   11   12\n\n\n\n\nRemember that zero-indexing!\n\nimport numpy as np\n\nmy_mat = np.reshape(range(1, 13), (3,4))\n\nmy_mat[2,0] = 500\n\nmy_mat\n\narray([[  1,   2,   3,   4],\n       [  5,   6,   7,   8],\n       [500,  10,  11,  12]])\n\n\n\n\n\n\n\n5.2.3.2 Matrix Operations\nThere are a number of matrix operations that we need to know for basic programming purposes:\n\nscalar multiplication \\[c*\\textbf{X} = c * \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] = \\left[\\begin{array}{cc} c*x_{1,1} & c*x_{1, 2}\\\\c*x_{2,1} & c*x_{2,2}\\end{array}\\right]\\]\ntranspose - flip the matrix across the left top -> right bottom diagonal. \\[t(\\textbf{X}) = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right]^T = \\left[\\begin{array}{cc} x_{1,1} & x_{2,1}\\\\x_{1,2} & x_{2,2}\\end{array}\\right]\\]\nmatrix multiplication (dot product) - you will learn more about this in linear algebra, but here’s a preview. Here is a better explanation of the cross product \\[\\textbf{X}*\\textbf{Y} = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] * \\left[\\begin{array}{cc} y_{1,1} \\\\y_{2,1} \\end{array}\\right] = \\left[\\begin{array}{c}x_{1,1}*y_{1,1} + x_{1,2}*y_{2,1} \\\\x_{2, 1}*y_{1,1} + x_{2,2}*y_{2,1}\\end{array}\\right]\\] Note that matrix multiplication depends on having matrices of compatible dimensions. If you have two matrices of dimension \\((a \\times b)\\) and \\((c \\times d)\\), then \\(b\\) must be equal to \\(c\\) for the multiplication to work, and your result will be \\((a \\times d)\\).\n\n\nRPython\n\n\n\nx <- matrix(c(1, 2, 3, 4), nrow = 2, byrow = T)\ny <- matrix(c(5, 6), nrow = 2)\n\n# Scalar multiplication\nx * 3\n\n     [,1] [,2]\n[1,]    3    6\n[2,]    9   12\n\n3 * x\n\n     [,1] [,2]\n[1,]    3    6\n[2,]    9   12\n\n# Transpose\nt(x)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nt(y)\n\n     [,1] [,2]\n[1,]    5    6\n\n# matrix multiplication (dot product)\nx %*% y\n\n     [,1]\n[1,]   17\n[2,]   39\n\n\n\n\n\nimport numpy as np\nx = np.array([[1,2],[3,4]])\ny = np.array([[5],[6]])\n\n# scalar multiplication\nx*3\n\narray([[ 3,  6],\n       [ 9, 12]])\n\n3*x\n\n# transpose\n\narray([[ 3,  6],\n       [ 9, 12]])\n\nx.T # shorthand\n\narray([[1, 3],\n       [2, 4]])\n\nx.transpose() # Long form\n\n# Matrix multiplication (dot product)\n\narray([[1, 3],\n       [2, 4]])\n\nnp.dot(x, y)\n\narray([[17],\n       [39]])\n\n\n\n\n\n\n\n\n5.2.4 Arrays\nArrays are a generalized n-dimensional version of a vector: all elements have the same type, and they are indexed using square brackets in both R and python: [dim1, dim2, dim3, ...]\nI don’t think you will need to create 3+ dimensional arrays in this class, but if you want to try it out, here is some code.\n\nRPython\n\n\n\narray(1:8, dim = c(2,2,2))\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\nNote that displaying this requires 2 slices, since it’s hard to display 3D information in a 2D terminal arrangement.\n\n\n\nimport numpy as np\n\nnp.array([[[1,2],[3,4]],[[5,6], [7,8]]])\n\narray([[[1, 2],\n        [3, 4]],\n\n       [[5, 6],\n        [7, 8]]])"
  },
  {
    "objectID": "05-vectors.html#control-structures",
    "href": "05-vectors.html#control-structures",
    "title": "5  Vectors, Matrices, Arrays, and Control Structures",
    "section": "5.3 Control Structures",
    "text": "Control structures are statements in a program that determine when code is evaluated (and how many times it might be evaluated). There are two main types of control structures: if-statements and loops.\nBefore we start on the types of control structures, let’s get in the right mindset. We’re all used to “if-then” logic, and use it in everyday conversation, but computers require another level of specificity when you’re trying to provide instructions.\nCheck out this video of the classic “make a peanut butter sandwich instructions challenge”:\n\n\nHere’s another example:\n\n\n\n‘If you’re done being pedantic, we should get dinner.’ ‘You did it again!’ ‘No, I didn’t.’\n\n\nThe key takeaways from these bits of media are that you should read this section with a focus on exact precision - state exactly what you mean, and the computer will do what you say. If you instead expect the computer to get what you mean, you’re going to have a bad time.\n\n5.3.1 Conditional Statements\nConditional statements determine if code is evaluated.\nThey look like this:\nif (condition)\n  then\n    (thing to do)\n  else\n    (other thing to do)\nThe else (other thing to do) part may be omitted.\nWhen this statement is read by the computer, the computer checks to see if condition is true or false. If the condition is true, then (thing to do) is also run. If the condition is false, then (other thing to do) is run instead.\nLet’s try this out:\n\nRPython\n\n\n\nx <- 3\ny <- 1\n\nif (x > 2) { \n  y <- 8\n} else {\n  y <- 4\n}\n\nprint(paste(\"x =\", x, \"; y =\", y))\n\n[1] \"x = 3 ; y = 8\"\n\n\nIn R, the logical condition after if must be in parentheses. It is common to then enclose the statement to be run if the condition is true in {} so that it is clear what code matches the if statement. You can technically put the condition on the line after the if (x > 2) line, and everything will still work, but then it gets hard to figure out what to do with the else statement - it technically would also go on the same line, and that gets hard to read.\n\nx <- 3\ny <- 1\n\nif (x > 2) y <- 8 else y <- 4\n\nprint(paste(\"x =\", x, \"; y =\", y))\n\n[1] \"x = 3 ; y = 8\"\n\n\nSo while the 2nd version of the code technically works, the first version with the brackets is much easier to read and understand. Please try to emulate the first version!\n\n\n\nx = 3\ny = 1\n\nif x > 2:\n  y = 8\nelse:\n  y = 4\n\nprint(\"x =\", x, \"; y =\", y)\n\nx = 3 ; y = 8\n\n\nIn python, all code grouping is accomplished with spaces instead of with brackets. So in python, we write our if statement as if x > 2: with the colon indicating that what follows is the code to evaluate. The next line is indented with 2 spaces to show that the code on those lines belongs to that if statement. Then, we use the else: statement to provide an alternative set of code to run if the logical condition in the if statement is false. Again, we indent the code under the else statement to show where it “belongs”.\nPython will throw errors if you mess up the spacing. This is one thing that is very annoying about Python… but it’s a consequence of trying to make the code more readable.\n\n\n\n\n\n5.3.1.1 Representing Conditional Statements as Diagrams\nA common way to represent conditional logic is to draw a flow chart diagram.\nIn a flow chart, conditional statements are represented as diamonds, and other code is represented as a rectangle. Yes/no or True/False branches are labeled. Typically, after a conditional statement, the program flow returns to a single point.\n\n\n\nProgram flow diagram outline of a simple if/else statement\n\n\n\n\nTry it out!\n\nProblemR SolutionPython SolutionProgram Flow Chart\n\n\nThe US Tax code has brackets, such that the first $10,275 of your income is taxed at 10%, anything between $10,275 and $41,775 is taxed at 12%, and so on.\nHere is the table of tax brackets for single filers in 2022:\n\n\n\nrate\nIncome\n\n\n\n\n10%\n$0 to $10,275\n\n\n12%\n$10,275 to $41,775\n\n\n22%\n$41,775 to $89,075\n\n\n24%\n$89,075 to $170,050\n\n\n32%\n$170,050 to $215,950\n\n\n35%\n$215,950 to $539,900\n\n\n37%\n$539,900 or more\n\n\n\nNote: For the purposes of this problem, we’re ignoring the personal exemption and the standard deduction, so we’re already simplifying the tax code.\nWrite a set of if statements that assess someone’s income and determine what their overall tax rate is.\nHint: You may want to keep track of how much of the income has already been taxed in a variable and what the total tax accumulation is in another variable.\n\n\n\n# Start with total income\nincome <- 200000\n\n# x will hold income that hasn't been taxed yet\nx <- income\n# y will hold taxes paid\ny <- 0\n\nif (x <= 10275) {\n  y <- x*.1 # tax paid\n  x <- 0 # All money has been taxed\n} else {\n  y <- y + 10275 * .1\n  x <- x - 10275 # Money remaining that hasn't been taxed\n}\n\nif (x <= (41775 - 10275)) {\n  y <- y + x * .12\n  x <- 0\n} else {\n  y <- y + (41775 - 10275) * .12\n  x <- x - (41775 - 10275) \n}\n\nif (x <= (89075 - 41775)) {\n  y <- y + x * .22\n  x <- 0\n} else {\n  y <- y + (89075 - 41775) * .22\n  x <- x - (89075 - 41775)\n}\n\nif (x <= (170050 - 89075)) {\n  y <- y + x * .24\n  x <- 0\n} else {\n  y <- y + (170050 - 89075) * .24\n  x <- x - (170050 - 89075)\n}\n\nif (x <= (215950 - 170050)) {\n  y <- y + x * .32\n  x <- 0\n} else {\n  y <- y + (215950 - 170050) * .32\n  x <- x - (215950 - 170050)\n}\n\nif (x <= (539900 - 215950)) {\n  y <- y + x * .35\n  x <- 0\n} else {\n  y <- y + (539900 - 215950) * .35\n  x <- x - (539900 - 215950)\n}\n\nif (x > 0) {\n  y <- y + x * .37\n}\n\n\nprint(paste(\"Total Tax Rate on $\", income, \" in income = \", round(y/income, 4)*100, \"%\"))\n\n[1] \"Total Tax Rate on $ 2e+05  in income =  22.12 %\"\n\n\n\n\n\n# Start with total income\nincome = 200000\n\n# untaxed will hold income that hasn't been taxed yet\nuntaxed = income\n# taxed will hold taxes paid\ntaxes = 0\n\nif untaxed <= 10275:\n  taxes = untaxed*.1 # tax paid\n  untaxed = 0 # All money has been taxed\nelse:\n  taxes = taxes + 10275 * .1\n  untaxed = untaxed - 10275 # money remaining that hasn't been taxed\n\nif untaxed <= (41775 - 10275):\n  taxes = taxes + untaxed * .12\n  untaxed = 0\nelse:\n  taxes = taxes + (41775 - 10275) * .12\n  untaxed = untaxed - (41775 - 10275) \n\n\nif untaxed <= (89075 - 41775):\n  taxes = taxes + untaxed * .22\n  untaxed = 0\nelse: \n  taxes = taxes + (89075 - 41775) * .22\n  untaxed = untaxed - (89075 - 41775)\n\nif untaxed <= (170050 - 89075):\n  taxes = taxes + untaxed * .24\n  untaxed = 0\nelse: \n  taxes = taxes + (170050 - 89075) * .24\n  untaxed = untaxed - (170050 - 89075)\n\nif untaxed <= (215950 - 170050):\n  taxes = taxes + untaxed * .32\n  untaxed = 0\nelse:\n  taxes = taxes + (215950 - 170050) * .32\n  untaxed = untaxed - (215950 - 170050)\n\nif untaxed <= (539900 - 215950):\n  taxes = taxes + untaxed * .35\n  untaxed = 0\nelse: \n  taxes = taxes + (539900 - 215950) * .35\n  untaxed = untaxed - (539900 - 215950)\n\n\nif untaxed > 0:\n  taxes = taxes + untaxed * .37\n\n\n\nprint(\"Total Tauntaxed Rate on $\", income, \" in income = \", round(taxes/income, 4)*100, \"%\")\n\nTotal Tauntaxed Rate on $ 200000  in income =  22.12 %\n\n\nWe will find a better way to represent this calculation once we discuss loops - we can store each bracket’s start and end point in a vector and loop through them. Any time you find yourself copy-pasting code and changing values, you should consider using a loop (or eventually a function) instead.\n\n\nLet’s explore using program flow maps for a slightly more complicated problem: The tax bracket example that we used to demonstrate if statement syntax.\n\n\n\n\n\n\n\nThe control flow diagram for the code in the previous example\n\n\nControl flow diagrams can be extremely helpful when figuring out how programs work (and where gaps in your logic are when you’re debugging). It can be very helpful to map out your program flow as you’re untangling a problem.\n\n\n\n\n\n5.3.1.2 Chaining Conditional Statements: Else-If\nIn many cases, it can be helpful to have a long chain of conditional statements describing a sequence of alternative statements.\n\nFor instance, suppose I want to determine what categorical age bracket someone falls into based on their numerical age. All of the bins are mutually exclusive - you can’t be in the 25-40 bracket and the 41-55 bracket.\n\nProgram Flow MapRPython\n\n\n\n\n\nProgram flow map for a series of mutually exclusive categories. If our goal is to take a numeric age variable and create a categorical set of age brackets, such as <18, 18-25, 26-40, 41-55, 56-65, and >65, we can do this with a series of if-else statements chained together. Only one of the bracket assignments is evaluated, so it is important to place the most restrictive condition first.\n\n\nThe important thing to realize when examining this program flow map is that if age <= 18 is true, then none of the other conditional statements even get evaluated. That is, once a statement is true, none of the other statements matter. Because of this, it is important to place the most restrictive statement first.\n\n\n\nProgram flow map for a series of mutually exclusive categories, emphasizing that only some statements are evaluated. When age = 40, only (age <= 18), (age <= 25), and (age <= 40) are evaluated conditionally. Of the assignment statements, only bracket = ‘26-40’ is evaluated when age = 40.\n\n\nIf for some reason you wrote your conditional statements in the wrong order, the wrong label would get assigned:\n\n\n\nProgram flow map for a series of mutually exclusive categories, with category labels in the wrong order - <40 is evaluated first, and so <= 25 and <= 18 will never be evaluated and the wrong label will be assigned for anything in those categories.\n\n\nIn code, we would write this statement using else-if (or elif) statements.\n\n\n\nage <- 40 # change this as you will to see how the code works\n\nif (age < 18) {\n  bracket <- \"<18\"\n} else if (age <= 25) {\n  bracket <- \"18-25\"\n} else if (age <= 40) {\n  bracket <- \"26-40\"\n} else if (age <= 55) {\n  bracket <- \"41-55\" \n} else if (age <= 65) {\n  bracket <- \"56-65\"\n} else {\n  bracket <- \">65\"\n}\n\nbracket\n\n[1] \"26-40\"\n\n\n\n\nPython uses elif as a shorthand for else if statements. As always, indentation/white space in python matters. If you put an extra blank line between two elif statements, then the interpreter will complain. If you don’t indent properly, the interpreter will complain.\n\nage = 40 # change this to see how the code works\n\nif age < 18:\n  bracket = \"<18\"\nelif age <= 25:\n  bracket = \"18-25\"\nelif age <= 40:\n  bracket = \"26-40\"\nelif age <= 55:\n  bracket = \"41-55\"\nelif age <= 65:\n  bracket = \"56-65\"\nelse:\n  bracket = \">65\"\n  \nbracket\n\n'26-40'\n\n\n\n\n\n\n\n\n\n5.3.2 Loops\n\nOften, we write programs which update a variable in a way that the new value of the variable depends on the old value:\nx = x + 1\nThis means that we add one to the current value of x.\nBefore we write a statement like this, we have to initialize the value of x because otherwise, we don’t know what value to add one to.\nx = 0\nx = x + 1\nWe sometimes use the word increment to talk about adding one to the value of x; decrement means subtracting one from the value of x.\nA particularly powerful tool for making these types of repetitive changes in programming is the loop, which executes statements a certain number of times. Loops can be written in several different ways, but all loops allow for executing a block of code a variable number of times.\n\n5.3.2.1 While Loops\nIn the previous section, we discussed conditional statements, where a block of code is only executed if a logical statement is true.\nThe simplest type of loop is the while loop, which executes a block of code until a statement is no longer true.\n\n\n\nFlow map showing while-loop pseudocode (while x <= N) { # code that changes x in some way} and the program flow map expansion where we check if x > N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then change x and start over.\n\n\n\nRPython\n\n\n\nx <- 0\n\nwhile (x < 10) { \n  # Everything in here is executed \n  # during each iteration of the loop\n  print(x)\n  x <- x + 1\n}\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\n\n\n\n\nx = 0\n\nwhile x < 10:\n  print(x)\n  x = x + 1\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n\n\n\nTry it Out!\n\nProblemMath NotationR SolutionPython solution\n\n\nWrite a while loop that verifies that \\[\\lim_{N \\rightarrow \\infty} \\prod_{k=1}^N \\left(1 + \\frac{1}{k^2}\\right) = \\frac{e^\\pi - e^{-\\pi}}{2\\pi}.\\]\nTerminate your loop when you get within 0.0001 of \\(\\frac{e^\\pi - e^{-\\pi}}{2\\pi}\\). At what value of \\(k\\) is this point reached?\n\n\nBreaking down math notation for code:\n\nIf you are unfamiliar with the notation \\(\\prod_{k=1}^N f(k)\\), this is the product of \\(f(k)\\) for \\(k = 1, 2, ..., N\\), \\[f(1)\\cdot f(2)\\cdot ... \\cdot f(N)\\]\nTo evaluate a limit, we just keep increasing \\(N\\) until we get arbitrarily close to the right hand side of the equation.\n\nIn this problem, we can just keep increasing \\(k\\) and keep track of the cumulative product. So we define k=1, prod = 1, and ans before the loop starts. Then, we loop over k, multiplying prod by \\((1 + 1/k^2)\\) and then incrementing \\(k\\) by one each time. At each iteration, we test whether prod is close enough to ans to stop the loop.\n\n\nIn R, you will use pi and exp() - these are available by default without any additional libraries or packages.\n\nk <- 1\nprod <- 1\nans <- (exp(pi) - exp(-pi))/(2*pi)\ndelta <- 0.0001\n\nwhile (abs(prod - ans) >= 0.0001) {\n  prod <- prod * (1 + 1/k^2)\n  k <- k + 1\n}\n\nk\n\n[1] 36761\n\nprod\n\n[1] 3.675978\n\nans\n\n[1] 3.676078\n\n\n\n\nNote that in python, you will have to import the math library to get the values of pi and the exp function. You can refer to these as math.pi and math.exp() respectively.\n\nimport math\n\nk = 1\nprod = 1\nans = (math.exp(math.pi) - math.exp(-math.pi))/(2*math.pi)\ndelta = 0.0001\n\nwhile abs(prod - ans) >= 0.0001:\n  prod = prod * (1 + k**-2)\n  k = k + 1\n  if k > 500000:\n    break\n\n\nprint(\"At \", k, \" iterations, the product is \", prod, \"compared to the limit \", ans,\".\")\n\nAt  36761  iterations, the product is  3.675977910975878 compared to the limit  3.676077910374978 .\n\n\n\n\n\n\n\n5.3.2.2 For Loops\nAnother common type of loop is a for loop. In a for loop, we run the block of code, iterating through a series of values (commonly, one to N, but not always). Generally speaking, for loops are known as definite loops because the code inside a for loop is executed a specific number of times. While loops are known as indefinite loops because the code within a while loop is evaluated until the condition is falsified, which is not always a known number of times.\n\nFlow MapRPython\n\n\n\n\n\nFlow map showing for-loop pseudocode (for j in 1 to N) { # code} and the program flow map expansion where j starts at 1 and we check if j > N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then increment j and start over.\n\n\n\n\n\nfor (i in 1:5 ) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nfor i in range(5):\n  print(i)\n\n0\n1\n2\n3\n4\n\n\nBy default range(5) goes from 0 to 5, the upper bound. When i = 5 the loop exits. This is because range(5) creates a vector [0, 1, 2, 3, 4].\n\n\n\nFor loops are often run from 1 to N (or 0 to N-1 in python) but in essence, a for loop is run for every value of a vector (which is why loops are included in the same chapter as vectors).\n\n\nRPython\n\n\nFor instance, in R, there is a built-in variable called month.name. Type month.name into your R console to see what it looks like. If we want to iterate along the values of month.name, we can:\n\nfor (i in month.name)\n  print(i)\n\n[1] \"January\"\n[1] \"February\"\n[1] \"March\"\n[1] \"April\"\n[1] \"May\"\n[1] \"June\"\n[1] \"July\"\n[1] \"August\"\n[1] \"September\"\n[1] \"October\"\n[1] \"November\"\n[1] \"December\"\n\n\n\n\nIn python, we have to define our vector or list to start out with, but that’s easy enough:\n\nfuturama_crew = ['Fry', 'Leela', 'Bender', 'Amy', 'the Professor', 'Hermes', 'Zoidberg', 'Nibbler']\nfor i in futurama_crew:\n  print(i)\n\nFry\nLeela\nBender\nAmy\nthe Professor\nHermes\nZoidberg\nNibbler\n\n\n\n\n\n\n\n\n5.3.2.3 Avoiding Infinite Loops\nIt is very easy to create an infinite loop when you are working with while loops. Infinite loops never exit, because the condition is always true. If in the while loop example we decrement x instead of incrementing x, the loop will run forever.\nYou want to try very hard to avoid ever creating an infinite loop - it can cause your session to crash.\nOne common way to avoid infinite loops is to create a second variable that just counts how many times the loop has run. If that variable gets over a certain threshold, you exit the loop.\n\nRPython\n\n\nThis while loop runs until either x < 10 or n > 50 - so it will run an indeterminate number of times and depends on the random values added to x. Since this process (a ‘random walk’) could theoretically continue forever, we add the n>50 check to the loop so that we don’t tie up the computer for eternity.\n\nx <- 0\nn <- 0 # count the number of times the loop runs\n\nwhile (x < 10) { \n  print(x)\n  x <- x + rnorm(1) # add a random normal (0, 1) draw each time\n  n <- n + 1\n  if (n > 50) \n    break # this stops the loop if n > 50\n}\n\n[1] 0\n[1] 0.5567862\n[1] -0.8349651\n[1] -0.5882737\n[1] -0.412133\n[1] -1.85785\n[1] -1.787028\n[1] -2.533486\n[1] -1.877735\n[1] -2.06326\n[1] -1.586586\n[1] -2.350196\n[1] -1.116612\n[1] -1.206025\n[1] -1.614238\n[1] -1.37894\n[1] -1.982327\n[1] -1.862768\n[1] -1.920062\n[1] -0.5365264\n[1] -1.082352\n[1] -0.2471183\n[1] 0.005367326\n[1] 1.307005\n[1] 0.850398\n[1] 2.144454\n[1] 2.234377\n[1] 2.091633\n[1] 1.636342\n[1] 3.296266\n[1] 2.438561\n[1] 1.682596\n[1] 1.028358\n[1] 2.080376\n[1] 2.469416\n[1] 2.633026\n[1] 2.021439\n[1] 1.035432\n[1] -0.5375777\n[1] 0.6162029\n[1] 1.925514\n[1] 2.927909\n[1] 4.246673\n[1] 5.7231\n[1] 6.307095\n[1] 7.011597\n[1] 6.348687\n[1] 6.450116\n[1] 6.127926\n[1] 6.922993\n[1] 7.109063\n\n\n\n\n\nimport numpy as np; # for the random normal draw\n\nx = 0\nn = 0 # count the number of times the loop runs\n\nwhile x < 10:\n  print(x)\n  x = x + np.random.normal(0, 1, 1) # add a random normal (0, 1) draw each time\n  n = n + 1\n  if n > 50:\n    break # this stops the loop if n > 50\n\n0\n[-1.51639558]\n[-0.47802657]\n[-0.80315344]\n[-0.03504616]\n[-0.43500147]\n[-0.12118908]\n[1.04237238]\n[1.35801108]\n[2.6402119]\n[1.99744932]\n[2.75118237]\n[2.52521137]\n[2.41247685]\n[3.22501329]\n[3.54698493]\n[3.74440447]\n[4.74173863]\n[6.20799042]\n[5.68335422]\n[3.76560843]\n[3.84245509]\n[3.09899607]\n[3.20161347]\n[3.09782107]\n[2.36751456]\n[3.08796014]\n[1.58902037]\n[2.60351524]\n[3.51995429]\n[4.77680214]\n[5.07876618]\n[7.022196]\n[5.76077131]\n[3.7819611]\n[4.34894078]\n[2.29904481]\n[1.96470926]\n[1.31814786]\n[-1.06351174]\n[-0.58781641]\n[0.13946981]\n[2.86732013]\n[5.06253999]\n[4.14216827]\n[2.34849717]\n[2.02014979]\n[2.19569514]\n[2.02653252]\n[-0.46471854]\n[0.19826098]\n\n\n\n\n\nIn both of the examples above, there are more efficient ways to write a random walk, but we will get to that later. The important thing here is that we want to make sure that our loops don’t run for all eternity.\n\n\n5.3.2.4 Controlling Loops\n\n\nSometimes it is useful to control the statements in a loop with a bit more precision. You may want to skip over code and proceed directly to the next iteration, or, as demonstrated in the previous section with the break statement, it may be useful to exit the loop prematurely.\n\nBreak StatementNext/Continue Statement\n\n\n\n\n\nA break statement is used to exit a loop prematurely\n\n\n\n\n\n\n\nA next (or continue) statement is used to skip the body of the loop and continue to the next iteration\n\n\n\n\n\n\nLet’s demonstrate the details of next/continue and break statements.\nWe can do different things based on whether i is evenly divisible by 3, 5, or both 3 and 5 (thus divisible by 15)\n\nRPython\n\n\n\nfor (i in 1:20) {\n  if (i %% 15 == 0) {\n    print(\"Exiting now\")\n    break\n  } else if (i %% 3 == 0) {    \n    print(\"Divisible by 3\")\n    next\n    print(\"After the next statement\") # this should never execute\n  } else if (i %% 5 == 0) {\n    print(\"Divisible by 5\")\n  } else {\n    print(i)\n  }\n}\n\n[1] 1\n[1] 2\n[1] \"Divisible by 3\"\n[1] 4\n[1] \"Divisible by 5\"\n[1] \"Divisible by 3\"\n[1] 7\n[1] 8\n[1] \"Divisible by 3\"\n[1] \"Divisible by 5\"\n[1] 11\n[1] \"Divisible by 3\"\n[1] 13\n[1] 14\n[1] \"Exiting now\"\n\n\n\n\n\nfor i in range(1, 20):\n  if i%15 == 0:\n    print(\"Exiting now\")\n    break\n  elif i%3 == 0:\n    print(\"Divisible by 3\")\n    continue\n    print(\"After the next statement\") # this should never execute\n  elif i%5 == 0:\n    print(\"Divisible by 5\")\n  else: \n    print(i)\n\n1\n2\nDivisible by 3\n4\nDivisible by 5\nDivisible by 3\n7\n8\nDivisible by 3\nDivisible by 5\n11\nDivisible by 3\n13\n14\nExiting now\n\n\n\n\n\n\nTo be quite honest, I haven’t really ever needed to use next/continue statements when I’m programming, and I rarely use break statements. However, it’s useful to know they exist just in case you come across a problem where you could put either one to use."
  },
  {
    "objectID": "06-data-structures.html#aside-data-pipes",
    "href": "06-data-structures.html#aside-data-pipes",
    "title": "6  Data Structures",
    "section": "Aside: Data Pipes",
    "text": "Pipes are useful items for moving things from one place to another. In data programming, pipes are operators that let us move data around. In R, we have two primary pipes that are similar (you may see both used if you google for code online). Any R version after 4.1 has a built-in pipe, |>; the tidyverse libraries use a pipe from the magrittr package, %>%.\nFor right now, it’s ok to think of the two pipes as essentially the same (but you can read about the differences here).\nFundamentally, a pipe allows you to take a function b() and apply it to x, like b(x), but write it as x |> b() or x %>% b(). This is particularly useful in cases where there are multiple sequential analysis steps, because where in regular notation you have to read the functions from the inside out to understand the sequential steps, with pipes, you have a clear step-by-step list of the order of operations.\nIn Python, there is a pipe function in the Pandas library that works using .pipe(function) notation. You can see this example for more information. From what I’ve seen reading code online, however, pipes are less commonly used in Python code than they are in R code. That’s ok - languages have different conventions, and it is usually best to adopt the convention of the language you’re working in so that your code can be read, run, and maintained by others more easily.\n\n\n\nUse the rnorm function in R to generate 100 draws from a standard normal distribution, then use the pipe to calculate the mean.\n\n\nlibrary(dplyr) # load the pipe %>%\n\nrnorm(100) %>%\n  mean()\n## [1] -0.1306156\n\nrnorm(100) |> mean()\n## [1] 0.1521778\n\n\n\n\nCalculate the mean of 100 random normal variables in python.\n\n\nimport numpy as np\nimport pandas as pd\n\nnums = pd.Series(np.random.normal(size = 100))\nnums.mean()\n## -0.08844537735507918\n\nThe conclusion here is that it’s far easier to not use the pipe in python because the .function notation that python uses mimics the step-by-step approach of pipes in R even without using the actual pipe function. When you use data frames instead of Series, you might start using the pipe, but only in some circumstances."
  },
  {
    "objectID": "06-data-structures.html#motivation-working-with-multiple-vectors",
    "href": "06-data-structures.html#motivation-working-with-multiple-vectors",
    "title": "6  Data Structures",
    "section": "6.1 Motivation: Working with Multiple Vectors",
    "text": "In the previous chapter, we talked about homogeneous structures: arrangements of data, like vectors and matrices, where every entry in the larger structure has the same type. In this chapter, we’ll be talking about the root of most data science analysis projects: the data frame.\nLike an excel spreadsheet, data frames are arrangements of data in columns and rows.\nThis format has two main restrictions:\n\nEvery entry in each column must have the same data type\nEvery column must have the same number of rows\n\n\n\n\nA lego data frame of 4 columns and 12 rows. Each column is a separate color hue (data type), with slight variations in the hue of each individual bricks.\n\n\nThe picture above shows a data frame of 4 columns, each with a different data type (brick size/hue). The data frame has 12 rows. This picture may look similar to one that we used to show logical indexing in the last chapter, and that is not a coincidence. You can get everything from a data frame that you would get from a collection of 4 separate vectors… but there are advantages to keeping things in a data frame instead.\nIn the previous chapter, we learned how to make different vectors in R, numpy, and pandas. Consider for a moment https://worldpopulationreview.com/states, which lists the population of each state. You can find this dataset in CSV form here.\n\n\n\nMultiple vectors in Python\n\n(I’m going to cheat and read this in using pandas functions we haven’t learned yet to demonstrate why this stuff matters.)\n\nimport pandas as pd\n\ndata = pd.read_html(\"https://worldpopulationreview.com/states\")[0]\nlist(data.columns) # get names\n\n# Create a few population series\n## ['Rank', 'State', '2022 Population', 'Growth 2022', '2021 Population', '2010 Census', 'Growth Since 2010', '% of US', 'Density (mi²)']\npopulation2022 = pd.Series(data['2022 Population'].values, index = data['State'].values)\npopulation2021 = pd.Series(data['2021 Population'].values, index = data['State'].values)\npopulation2010 = pd.Series(data['2010 Census'].values, index = data['State'].values)\n\nSuppose that we want to sort each population vector by the population in that year.\n\nimport pandas as pd\ndata = pd.read_html(\"https://worldpopulationreview.com/states\")[0]\n\npopulation2022 = pd.Series(data['2022 Population'].values, index = data['State'].values).sort_values()\npopulation2021 = pd.Series(data['2021 Population'].values, index = data['State'].values).sort_values()\npopulation2010 = pd.Series(data['2010 Census'].values, index = data['State'].values).sort_values()\n\npopulation2022.head()\n## Wyoming                 582233\n## Vermont                 622882\n## District of Columbia    718355\n## Alaska                  720763\n## North Dakota            774008\n## dtype: int64\npopulation2021.head()\n## Wyoming                 581075\n## Vermont                 623251\n## District of Columbia    714153\n## Alaska                  724357\n## North Dakota            770026\n## dtype: int64\npopulation2010.head()\n## Wyoming                 564487\n## District of Columbia    605226\n## Vermont                 625879\n## North Dakota            674715\n## Alaska                  713910\n## dtype: int64\n\nThe only problem is that by doing this, we’ve now lost the ordering that matched across all 3 vectors. Pandas Series are great for this, because they use labels that allow us to reconstitute which value corresponds to which label, but in R or even in numpy arrays, vectors don’t inherently come with labels. In these situations, sorting by one value can actually destroy the connection between two vectors!\n\n\n\n\n\nVector-based analysis in R\n\n\ndf <- read.csv(\"https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/population2022.csv\")\n\n# Use vectors instead of the data frame\nstate <- df$State\npop2022 <- df$Pop\npop2021 <- df$Pop2021\npop2010 <- df$Pop2010\n\n# Create a vector to index population in 2022 in order\norder2022 <- order(pop2022)\n\n# To keep variables together, we have to do things like this:\nhead(state[order2022])\n## [1] \"Wyoming\"              \"Vermont\"              \"District of Columbia\"\n## [4] \"Alaska\"               \"North Dakota\"         \"South Dakota\"\nhead(pop2022[order2022])\n## [1] 582233 622882 718355 720763 774008 902542\n\n# It makes more sense just to reorder the whole data frame:\nhead(df[order2022,])\n##    rank                State    Pop  Growth Pop2021 Pop2010 growthSince2010\n## 52   52              Wyoming 582233  0.0020  581075  564487          0.0314\n## 51   51              Vermont 622882 -0.0006  623251  625879         -0.0048\n## 50   50 District of Columbia 718355  0.0059  714153  605226          0.1869\n## 49   49               Alaska 720763 -0.0050  724357  713910          0.0096\n## 48   48         North Dakota 774008  0.0052  770026  674715          0.1472\n## 47   47         South Dakota 902542  0.0066  896581  816166          0.1058\n##    Percent    density\n## 52  0.0017     5.9967\n## 51  0.0019    67.5797\n## 50  0.0021 11776.3115\n## 49  0.0021     1.2631\n## 48  0.0023    11.2173\n## 47  0.0027    11.9052\n\n\n\nThe primary advantage to data frames is that rows of data are kept together. Since we often think of a row of data as a single observation in a sample, this is an extremely important feature that makes data frames a huge improvement on a collection of vectors of the same length: it’s much harder for observations in a single row to get shuffled around and mismatched!\nIn R, data frames are built in as type data.frame, though there are packages that provide other implementations of data frames that have additional features, such as the tibble package used in many other common packages. We will cover functions from both base R and the tibble package in this chapter.\nIn Python, we will use the pandas library, which is conventionally abbreviated pd. So before you use any data frames in python, you will need to add the following line to your code: import pandas as pd."
  },
  {
    "objectID": "06-data-structures.html#creating-data-frames",
    "href": "06-data-structures.html#creating-data-frames",
    "title": "6  Data Structures",
    "section": "6.2 Creating Data Frames",
    "text": "6.2.1 From Scratch\nIf you want to create a data frame “from scratch” in either R or python, the easiest way to do so is to construct a list of vectors.\nData sourced from Wikipedia’s List of Oldest dogs\n\n\n\nCreating Data frames from scratch in R\n\n\ndog_names <- c(\"Bluey\", \"Bramble\", \"Chanel\", \"Max\")\ndog_ages <- c(29.41, 25, 21, 29.77)\n\n# Using the data.frame function\ndata <- data.frame(Name = dog_names, Age = dog_ages)\ndata\n##      Name   Age\n## 1   Bluey 29.41\n## 2 Bramble 25.00\n## 3  Chanel 21.00\n## 4     Max 29.77\n\n# Using the tibble function\nlibrary(tibble)\ndata <- tibble(Name = dog_names, Age = dog_ages)\n# Notice the difference in how the object is printed...\ndata\n## # A tibble: 4 × 2\n##   Name      Age\n##   <chr>   <dbl>\n## 1 Bluey    29.4\n## 2 Bramble  25  \n## 3 Chanel   21  \n## 4 Max      29.8\n\n# Using the tribble function in the tibble package\ndata <- tribble(~Name, ~Age,\n                \"Bluey\", 29.41,\n                \"Bramble\", 25,\n                \"Chanel\", 21,\n                \"Max\", 29.77)\n# This allows you to write out the data yourself in table format\n# Column Names are indicated by putting ~ before the (bare) column name\ndata\n## # A tibble: 4 × 2\n##   Name      Age\n##   <chr>   <dbl>\n## 1 Bluey    29.4\n## 2 Bramble  25  \n## 3 Chanel   21  \n## 4 Max      29.8\n\n\n\n\n\n\nCreating Data frames from scratch in python\n\n\nimport pandas as pd\n\n# Create a list of lists\ndata = [['Bluey', 29.41],\n        ['Bramble', 25],\n        ['Chanel', 21],\n        ['Max', 29.77]]\n\ndata = pd.DataFrame(data, columns = ['Name', 'Age'])\n\n# Create a dict with lists\ndata = {'Name': ['Bluey', 'Bramble', 'Chanel', 'Max'],\n        'Age':  [29.41, 25, 21, 29.77]}\n\ndata = pd.DataFrame(data)\n\nI am intentionally not discussing dictionaries (dicts) in Python at this point - my goal is to get you up and running to do data analysis in Python with as little overhead as possible. If you are interested, you can read up on dicts in Python 4 Everybody. We will hopefully find time to come back and discuss the finer points of lists, dicts, tuples, and other constructs later in the semester or in a subsequent course.\n\n\n\n\n6.2.2 Reading in Data\nOne of the easier ways to create a data frame (rather than typing the whole thing in) is to read in data from somewhere else - a file, a table on a webpage, etc. We’re not going to go into the finer points of this (you’ll get into that in Stat 251, Data Wrangling), but it is important to at least know how to read in relatively nicely formatted data.\nOne nice source of (relatively neat) data is the TidyTuesday github repository1.\n\n\n\nIn Base R, we can read the data in using the read.csv function\n\n\nairmen <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv')\nhead(airmen)\n##                    name last_name    first_name      graduation_date\n## 1   Adams, John H., Jr.     Adams  John H., Jr. 1945-04-15T00:00:00Z\n## 2           Adams, Paul     Adams          Paul 1943-04-29T00:00:00Z\n## 3 Adkins, Rutherford H.    Adkins Rutherford H. 1944-10-16T00:00:00Z\n## 4    Adkins, Winston A.    Adkins    Winston A. 1944-02-08T00:00:00Z\n## 5 Alexander, Halbert L. Alexander    Halbert L. 1944-11-20T00:00:00Z\n## 6  Alexander, Harvey R. Alexander     Harvey R. 1944-04-15T00:00:00Z\n##   rank_at_graduation     class graduated_from    pilot_type\n## 1             2nd Lt   SE-45-B           TAAF Single engine\n## 2             2nd Lt   SE-43-D           TAAF Single engine\n## 3             2nd Lt SE-44-I-1           TAAF Single engine\n## 4             2nd Lt   TE-44-B           TAAF   Twin engine\n## 5             2nd Lt   SE-44-I           TAAF Single engine\n## 6             2nd Lt   TE-44-D           TAAF   Twin engine\n##   military_hometown_of_record state aerial_victory_credits\n## 1                 Kansas City    KS                   <NA>\n## 2                  Greenville    SC                   <NA>\n## 3                  Alexandria    VA                   <NA>\n## 4                     Chicago    IL                   <NA>\n## 5                  Georgetown    IL                   <NA>\n## 6                  Georgetown    IL                   <NA>\n##   number_of_aerial_victory_credits reported_lost reported_lost_date\n## 1                                0          <NA>               <NA>\n## 2                                0          <NA>               <NA>\n## 3                                0          <NA>               <NA>\n## 4                                0          <NA>               <NA>\n## 5                                0          <NA>               <NA>\n## 6                                0          <NA>               <NA>\n##   reported_lost_location                                   web_profile\n## 1                   <NA>     https://cafriseabove.org/john-h-adams-jr/\n## 2                   <NA>          https://cafriseabove.org/paul-adams/\n## 3                   <NA> https://cafriseabove.org/rutherford-h-adkins/\n## 4                   <NA>                                          <NA>\n## 5                   <NA> https://cafriseabove.org/halbert-l-alexander/\n## 6                   <NA>  https://cafriseabove.org/harvey-r-alexander/\n\n\n\n\nIf we want instead to create a tibble, we can use the readr package’s read_csv function, which is a bit more robust.\n\n\nlibrary(readr)\nairmen <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv')\nhead(airmen)\n## # A tibble: 6 × 16\n##   name           last_name first_name graduation_date     rank_at_graduat… class\n##   <chr>          <chr>     <chr>      <dttm>              <chr>            <chr>\n## 1 Adams, John H… Adams     John H., … 1945-04-15 00:00:00 2nd Lt           SE-4…\n## 2 Adams, Paul    Adams     Paul       1943-04-29 00:00:00 2nd Lt           SE-4…\n## 3 Adkins, Ruthe… Adkins    Rutherfor… 1944-10-16 00:00:00 2nd Lt           SE-4…\n## 4 Adkins, Winst… Adkins    Winston A. 1944-02-08 00:00:00 2nd Lt           TE-4…\n## 5 Alexander, Ha… Alexander Halbert L. 1944-11-20 00:00:00 2nd Lt           SE-4…\n## 6 Alexander, Ha… Alexander Harvey R.  1944-04-15 00:00:00 2nd Lt           TE-4…\n## # … with 10 more variables: graduated_from <chr>, pilot_type <chr>,\n## #   military_hometown_of_record <chr>, state <chr>,\n## #   aerial_victory_credits <chr>, number_of_aerial_victory_credits <dbl>,\n## #   reported_lost <chr>, reported_lost_date <dttm>,\n## #   reported_lost_location <chr>, web_profile <chr>\n\n\n\n\nIn pandas, we can read the csv using pd.read_csv\n\n\nimport pandas as pd\n\nairmen = pd.read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv\")\nairmen.head()\n##                     name  ...                                    web_profile\n## 0    Adams, John H., Jr.  ...      https://cafriseabove.org/john-h-adams-jr/\n## 1            Adams, Paul  ...           https://cafriseabove.org/paul-adams/\n## 2  Adkins, Rutherford H.  ...  https://cafriseabove.org/rutherford-h-adkins/\n## 3     Adkins, Winston A.  ...                                            NaN\n## 4  Alexander, Halbert L.  ...  https://cafriseabove.org/halbert-l-alexander/\n## \n## [5 rows x 16 columns]"
  },
  {
    "objectID": "06-data-structures.html#working-with-data-frames",
    "href": "06-data-structures.html#working-with-data-frames",
    "title": "6  Data Structures",
    "section": "6.3 Working with Data Frames",
    "text": "6.3.1 Summaries (part 1)\nOften, we want to know what a data frame contains. R and pandas both have easy summary methods for a data frame:\n\nsummary(airmen)\n##      name            last_name          first_name       \n##  Length:1006        Length:1006        Length:1006       \n##  Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character  \n##                                                          \n##                                                          \n##                                                          \n##                                                          \n##  graduation_date                   rank_at_graduation    class          \n##  Min.   :1942-03-06 00:00:00.000   Length:1006        Length:1006       \n##  1st Qu.:1943-10-22 00:00:00.000   Class :character   Class :character  \n##  Median :1944-05-23 00:00:00.000   Mode  :character   Mode  :character  \n##  Mean   :1944-07-02 13:18:52.462                                        \n##  3rd Qu.:1945-04-15 00:00:00.000                                        \n##  Max.   :1948-10-12 00:00:00.000                                        \n##  NA's   :11                                                             \n##  graduated_from      pilot_type        military_hometown_of_record\n##  Length:1006        Length:1006        Length:1006                \n##  Class :character   Class :character   Class :character           \n##  Mode  :character   Mode  :character   Mode  :character           \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##     state           aerial_victory_credits number_of_aerial_victory_credits\n##  Length:1006        Length:1006            Min.   :0.0000                  \n##  Class :character   Class :character       1st Qu.:0.0000                  \n##  Mode  :character   Mode  :character       Median :0.0000                  \n##                                            Mean   :0.1118                  \n##                                            3rd Qu.:0.0000                  \n##                                            Max.   :4.0000                  \n##                                                                            \n##  reported_lost      reported_lost_date   reported_lost_location\n##  Length:1006        Min.   :1943-07-02   Length:1006           \n##  Class :character   1st Qu.:1943-07-02   Class :character      \n##  Mode  :character   Median :1943-07-02   Mode  :character      \n##                     Mean   :1943-07-02                         \n##                     3rd Qu.:1943-07-02                         \n##                     Max.   :1943-07-02                         \n##                     NA's   :1004                               \n##  web_profile       \n##  Length:1006       \n##  Class :character  \n##  Mode  :character  \n##                    \n##                    \n##                    \n## \n\nlibrary(skimr) # Fancier summaries\nskim(airmen)\n\n\nData summary\n\n\nName\nairmen\n\n\nNumber of rows\n1006\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nnumeric\n1\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1.00\n9\n28\n0\n1003\n0\n\n\nlast_name\n0\n1.00\n3\n12\n0\n617\n0\n\n\nfirst_name\n0\n1.00\n3\n17\n0\n804\n0\n\n\nrank_at_graduation\n5\n1.00\n3\n14\n0\n7\n0\n\n\nclass\n20\n0.98\n3\n9\n0\n72\n0\n\n\ngraduated_from\n0\n1.00\n4\n23\n0\n4\n0\n\n\npilot_type\n0\n1.00\n11\n13\n0\n5\n0\n\n\nmilitary_hometown_of_record\n9\n0.99\n3\n19\n0\n366\n0\n\n\nstate\n11\n0.99\n2\n5\n0\n48\n0\n\n\naerial_victory_credits\n934\n0.07\n31\n137\n0\n50\n0\n\n\nreported_lost\n1004\n0.00\n1\n1\n0\n1\n0\n\n\nreported_lost_location\n1004\n0.00\n23\n23\n0\n1\n0\n\n\nweb_profile\n813\n0.19\n34\n95\n0\n190\n0\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nnumber_of_aerial_victory_credits\n0\n1\n0.11\n0.46\n0\n0\n0\n0\n4\n▇▁▁▁▁\n\n\n\nVariable type: POSIXct\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ngraduation_date\n11\n0.99\n1942-03-06\n1948-10-12\n1944-05-23\n52\n\n\nreported_lost_date\n1004\n0.00\n1943-07-02\n1943-07-02\n1943-07-02\n1\n\n\n\n\n\nNotice that the type of summary depends on the data type.\n\n# All variables - strings are summarized with NaNs\nairmen.describe(include = 'all')\n\n# Only summarize numeric variables\n##                       name  ...                                        web_profile\n## count                 1006  ...                                                193\n## unique                1003  ...                                                190\n## top     Brothers, James E.  ...  https://cafriseabove.org/captain-graham-smith-...\n## freq                     2  ...                                                  2\n## mean                   NaN  ...                                                NaN\n## std                    NaN  ...                                                NaN\n## min                    NaN  ...                                                NaN\n## 25%                    NaN  ...                                                NaN\n## 50%                    NaN  ...                                                NaN\n## 75%                    NaN  ...                                                NaN\n## max                    NaN  ...                                                NaN\n## \n## [11 rows x 16 columns]\nairmen.describe(include = [np.number])\n\n# Only summarize string variables (objects)\n##        number_of_aerial_victory_credits\n## count                       1006.000000\n## mean                           0.111829\n## std                            0.457844\n## min                            0.000000\n## 25%                            0.000000\n## 50%                            0.000000\n## 75%                            0.000000\n## max                            4.000000\nairmen.describe(include = ['O'])\n\n# Get counts of how many NAs in each column\n##                       name  ...                                        web_profile\n## count                 1006  ...                                                193\n## unique                1003  ...                                                190\n## top     Brothers, James E.  ...  https://cafriseabove.org/captain-graham-smith-...\n## freq                     2  ...                                                  2\n## \n## [4 rows x 15 columns]\nairmen.info(show_counts=True)\n## <class 'pandas.core.frame.DataFrame'>\n## RangeIndex: 1006 entries, 0 to 1005\n## Data columns (total 16 columns):\n##  #   Column                            Non-Null Count  Dtype  \n## ---  ------                            --------------  -----  \n##  0   name                              1006 non-null   object \n##  1   last_name                         1006 non-null   object \n##  2   first_name                        1006 non-null   object \n##  3   graduation_date                   995 non-null    object \n##  4   rank_at_graduation                999 non-null    object \n##  5   class                             986 non-null    object \n##  6   graduated_from                    1006 non-null   object \n##  7   pilot_type                        1006 non-null   object \n##  8   military_hometown_of_record       997 non-null    object \n##  9   state                             995 non-null    object \n##  10  aerial_victory_credits            72 non-null     object \n##  11  number_of_aerial_victory_credits  1006 non-null   float64\n##  12  reported_lost                     2 non-null      object \n##  13  reported_lost_date                2 non-null      object \n##  14  reported_lost_location            2 non-null      object \n##  15  web_profile                       193 non-null    object \n## dtypes: float64(1), object(15)\n## memory usage: 125.9+ KB\n\nIn pandas, you will typically want to separate out .describe() calls for numeric and non-numeric columns. Another handy function in pandas is .info(), which you can use to show the number of non-NA values. This is particularly useful in sparse datasets where there may be a LOT of missing values and you may want to find out which columns have useful information for more than just a few rows.\n\n\n6.3.2 Indexing\nTo access a subset of a data frame, we index by [row, column] in both languages (though in python we need a helper function tagged on the end of the object).\n\n\nIndexing in python (lots of output)\n\n\n# .iloc allows for integer location-based indexing\nairmen.iloc[0:4,] # leave the space for cols blank to get all columns\n##                     name  ...                                    web_profile\n## 0    Adams, John H., Jr.  ...      https://cafriseabove.org/john-h-adams-jr/\n## 1            Adams, Paul  ...           https://cafriseabove.org/paul-adams/\n## 2  Adkins, Rutherford H.  ...  https://cafriseabove.org/rutherford-h-adkins/\n## 3     Adkins, Winston A.  ...                                            NaN\n## \n## [4 rows x 16 columns]\nairmen.iloc[0:4,[0, 3, 5]] # include a vector of column indices\n\n# .loc allows for using the row and column indexes\n##                     name       graduation_date      class\n## 0    Adams, John H., Jr.  1945-04-15T00:00:00Z    SE-45-B\n## 1            Adams, Paul  1943-04-29T00:00:00Z    SE-43-D\n## 2  Adkins, Rutherford H.  1944-10-16T00:00:00Z  SE-44-I-1\n## 3     Adkins, Winston A.  1944-02-08T00:00:00Z    TE-44-B\nairmen.loc['0':'4',]\n##                           name  ...                                    web_profile\n## 0          Adams, John H., Jr.  ...      https://cafriseabove.org/john-h-adams-jr/\n## 1                  Adams, Paul  ...           https://cafriseabove.org/paul-adams/\n## 2        Adkins, Rutherford H.  ...  https://cafriseabove.org/rutherford-h-adkins/\n## 3           Adkins, Winston A.  ...                                            NaN\n## 4        Alexander, Halbert L.  ...  https://cafriseabove.org/halbert-l-alexander/\n## ..                         ...  ...                                            ...\n## 395        Hicks, Frederick P.  ...                                            NaN\n## 396  Higginbotham, Mitchell L.  ...                                            NaN\n## 397         Highbaugh, Earl B.  ...                                            NaN\n## 398      Highbaugh, Richard B.  ...                                            NaN\n## 399      Hill, Charles A., Jr.  ...                                            NaN\n## \n## [400 rows x 16 columns]\nairmen.loc[0:4,'name':'first_name'] # columns between name and first_name\n##                     name  last_name     first_name\n## 0    Adams, John H., Jr.      Adams   John H., Jr.\n## 1            Adams, Paul      Adams           Paul\n## 2  Adkins, Rutherford H.     Adkins  Rutherford H.\n## 3     Adkins, Winston A.     Adkins     Winston A.\n## 4  Alexander, Halbert L.  Alexander     Halbert L.\nairmen.loc[0:4,[0,3,5]] # can't use position indexes with .loc\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: \"None of [Int64Index([0, 3, 5], dtype='int64')] are in the [columns]\"\n## \n## Detailed traceback:\n##   File \"<string>\", line 1, in <module>\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 961, in __getitem__\n##     return self._getitem_tuple(key)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1149, in _getitem_tuple\n##     return self._getitem_tuple_same_dim(tup)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 827, in _getitem_tuple_same_dim\n##     retval = getattr(retval, self.name)._getitem_axis(key, axis=i)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1191, in _getitem_axis\n##     return self._getitem_iterable(key, axis=axis)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1132, in _getitem_iterable\n##     keyarr, indexer = self._get_listlike_indexer(key, axis)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1327, in _get_listlike_indexer\n##     keyarr, indexer = ax._get_indexer_strict(key, axis_name)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 5782, in _get_indexer_strict\n##     self._raise_if_missing(keyarr, indexer, axis_name)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 5842, in _raise_if_missing\n##     raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n\nThis uses a function of pandas we have not previously explored: slicing. Slicing in pandas acts very similar to R’s seq method for integers, in that you can set your start and end points and use : between them. However, in python, the last index is the (non-inclusive) endpoint, so 0:3 will give you 0, 1, 2. If you want all columns after a certain index, you can use x:, where x is the starting index.\n\n\n\nIndexing in R\n\n\nairmen[1:5, ]\n## # A tibble: 5 × 16\n##   name           last_name first_name graduation_date     rank_at_graduat… class\n##   <chr>          <chr>     <chr>      <dttm>              <chr>            <chr>\n## 1 Adams, John H… Adams     John H., … 1945-04-15 00:00:00 2nd Lt           SE-4…\n## 2 Adams, Paul    Adams     Paul       1943-04-29 00:00:00 2nd Lt           SE-4…\n## 3 Adkins, Ruthe… Adkins    Rutherfor… 1944-10-16 00:00:00 2nd Lt           SE-4…\n## 4 Adkins, Winst… Adkins    Winston A. 1944-02-08 00:00:00 2nd Lt           TE-4…\n## 5 Alexander, Ha… Alexander Halbert L. 1944-11-20 00:00:00 2nd Lt           SE-4…\n## # … with 10 more variables: graduated_from <chr>, pilot_type <chr>,\n## #   military_hometown_of_record <chr>, state <chr>,\n## #   aerial_victory_credits <chr>, number_of_aerial_victory_credits <dbl>,\n## #   reported_lost <chr>, reported_lost_date <dttm>,\n## #   reported_lost_location <chr>, web_profile <chr>\nairmen[1:5, c(1, 4, 6)]\n## # A tibble: 5 × 3\n##   name                  graduation_date     class    \n##   <chr>                 <dttm>              <chr>    \n## 1 Adams, John H., Jr.   1945-04-15 00:00:00 SE-45-B  \n## 2 Adams, Paul           1943-04-29 00:00:00 SE-43-D  \n## 3 Adkins, Rutherford H. 1944-10-16 00:00:00 SE-44-I-1\n## 4 Adkins, Winston A.    1944-02-08 00:00:00 TE-44-B  \n## 5 Alexander, Halbert L. 1944-11-20 00:00:00 SE-44-I\n\nairmen[1:5, c(\"name\", \"first_name\")]\n## # A tibble: 5 × 2\n##   name                  first_name   \n##   <chr>                 <chr>        \n## 1 Adams, John H., Jr.   John H., Jr. \n## 2 Adams, Paul           Paul         \n## 3 Adkins, Rutherford H. Rutherford H.\n## 4 Adkins, Winston A.    Winston A.   \n## 5 Alexander, Halbert L. Halbert L.\n\n\nIn R, we can also easily pull out a single column using the $ method. Note that this gives us a vector (that is, we’ve lost the connection to the row index).\n\nairmen$name[1:5]\n## [1] \"Adams, John H., Jr.\"   \"Adams, Paul\"           \"Adkins, Rutherford H.\"\n## [4] \"Adkins, Winston A.\"    \"Alexander, Halbert L.\"\n\nhead(airmen[\"name\"]) # head() just gives us the first few rows\n## # A tibble: 6 × 1\n##   name                 \n##   <chr>                \n## 1 Adams, John H., Jr.  \n## 2 Adams, Paul          \n## 3 Adkins, Rutherford H.\n## 4 Adkins, Winston A.   \n## 5 Alexander, Halbert L.\n## 6 Alexander, Harvey R.\n\nIn python, we can also easily pull out a single column:\n\nairmen.name\n## 0         Adams, John H., Jr.\n## 1                 Adams, Paul\n## 2       Adkins, Rutherford H.\n## 3          Adkins, Winston A.\n## 4       Alexander, Halbert L.\n##                 ...          \n## 1001         Young, Albert L.\n## 1002     Young, Benjamin, Jr.\n## 1003         Young, Eddie Lee\n## 1004            Young, Lee W.\n## 1005        Young, William W.\n## Name: name, Length: 1006, dtype: object\nairmen['name']\n## 0         Adams, John H., Jr.\n## 1                 Adams, Paul\n## 2       Adkins, Rutherford H.\n## 3          Adkins, Winston A.\n## 4       Alexander, Halbert L.\n##                 ...          \n## 1001         Young, Albert L.\n## 1002     Young, Benjamin, Jr.\n## 1003         Young, Eddie Lee\n## 1004            Young, Lee W.\n## 1005        Young, William W.\n## Name: name, Length: 1006, dtype: object\n\nThe df.column notation, called attribute access only works in some circumstances: where the column name is not the same as a method (e.g. min) and is a valid Python identifier (so df.1 does not work). When attribute access does not work, you can still access the column by name using df['colname'] (standard indexing).\n\n\nIndexing in python with pandas (Pandas documentation) This includes good information on which indexing operations are most efficient and recommended for production code.\nSlicing dataframes in R - The Pirate’s Guide to R\nIndexing, Slicing, and Subsetting DataFrames in Python - Visualization in Python for Ecologists\n\n\n\n\n6.3.3 Row and Column Names\nIn both R and python, data frames have two sets of names: a set of row names, and a set of column names. In my experience, it is much more common to use column names in R and less common to actually use row names2; in Python it seems that people tend to use both sets of names frequently.\n\n\nRow and Column Names in Python\n\nLet’s start with column names. Column names can be defined by creating a Series object (remember, that’s just a fancy name for an indexed vector) and assigning it to the df.columns object, where df is the name of the data frame.\n\n# Get index of column names\nairmen.columns\n\n# We can set the names using simple assignment\n## Index(['name', 'last_name', 'first_name', 'graduation_date',\n##        'rank_at_graduation', 'class', 'graduated_from', 'pilot_type',\n##        'military_hometown_of_record', 'state', 'aerial_victory_credits',\n##        'number_of_aerial_victory_credits', 'reported_lost',\n##        'reported_lost_date', 'reported_lost_location', 'web_profile'],\n##       dtype='object')\nairmen.columns = pd.Series(['Name', 'Last', 'First', 'Graduation_Date', 'Graduation_Rank', 'Class', 'Graduated_From', 'Pilot_Type', 'Hometown', 'State', 'Aerial_Victory_Credits', 'Num_Aerial_Victory_Credits', 'Reported_Lost', 'Reported_Lost_Date', 'Reported_Lost_Location', 'Web_Profile'])\n\n# Now the names are capitalized\nairmen.columns\n## Index(['Name', 'Last', 'First', 'Graduation_Date', 'Graduation_Rank', 'Class',\n##        'Graduated_From', 'Pilot_Type', 'Hometown', 'State',\n##        'Aerial_Victory_Credits', 'Num_Aerial_Victory_Credits', 'Reported_Lost',\n##        'Reported_Lost_Date', 'Reported_Lost_Location', 'Web_Profile'],\n##       dtype='object')\n\n\n# Get index of row names\nairmen.index # this structure has numeric row names\n# we can access individual rows using the numeric index (iloc)\n## RangeIndex(start=0, stop=1006, step=1)\nairmen.iloc[0]\n# we can also access individual rows using the regular index (loc)\n## Name                                                Adams, John H., Jr.\n## Last                                                              Adams\n## First                                                      John H., Jr.\n## Graduation_Date                                    1945-04-15T00:00:00Z\n## Graduation_Rank                                                  2nd Lt\n## Class                                                           SE-45-B\n## Graduated_From                                                     TAAF\n## Pilot_Type                                                Single engine\n## Hometown                                                    Kansas City\n## State                                                                KS\n## Aerial_Victory_Credits                                              NaN\n## Num_Aerial_Victory_Credits                                          0.0\n## Reported_Lost                                                       NaN\n## Reported_Lost_Date                                                  NaN\n## Reported_Lost_Location                                              NaN\n## Web_Profile                   https://cafriseabove.org/john-h-adams-jr/\n## Name: 0, dtype: object\nairmen.loc[0]\n# this doesn't work because the row names are integers\n## Name                                                Adams, John H., Jr.\n## Last                                                              Adams\n## First                                                      John H., Jr.\n## Graduation_Date                                    1945-04-15T00:00:00Z\n## Graduation_Rank                                                  2nd Lt\n## Class                                                           SE-45-B\n## Graduated_From                                                     TAAF\n## Pilot_Type                                                Single engine\n## Hometown                                                    Kansas City\n## State                                                                KS\n## Aerial_Victory_Credits                                              NaN\n## Num_Aerial_Victory_Credits                                          0.0\n## Reported_Lost                                                       NaN\n## Reported_Lost_Date                                                  NaN\n## Reported_Lost_Location                                              NaN\n## Web_Profile                   https://cafriseabove.org/john-h-adams-jr/\n## Name: 0, dtype: object\nairmen.loc['Adams, John H., Jr.']\n\n# We can set row names using simple assignment\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: 'Adams, John H., Jr.'\n## \n## Detailed traceback:\n##   File \"<string>\", line 1, in <module>\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 967, in __getitem__\n##     return self._getitem_axis(maybe_callable, axis=axis)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1202, in _getitem_axis\n##     return self._get_label(key, axis=axis)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1153, in _get_label\n##     return self.obj.xs(label, axis=axis)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/generic.py\", line 3876, in xs\n##     loc = index.get_loc(key)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexes/range.py\", line 389, in get_loc\n##     raise KeyError(key)\nairmen.index = airmen.name\n\n# Row names are changed\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'DataFrame' object has no attribute 'name'\n## \n## Detailed traceback:\n##   File \"<string>\", line 1, in <module>\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/generic.py\", line 5583, in __getattr__\n##     return object.__getattribute__(self, name)\nairmen.index # now the row name index is a string and we can look names up this way\n\n# we can still access individual rows using the numeric index (iloc)\n## RangeIndex(start=0, stop=1006, step=1)\nairmen.iloc[0]\n# we can mo longer access individual rows using the regular index (loc)\n# with a numeric value\n## Name                                                Adams, John H., Jr.\n## Last                                                              Adams\n## First                                                      John H., Jr.\n## Graduation_Date                                    1945-04-15T00:00:00Z\n## Graduation_Rank                                                  2nd Lt\n## Class                                                           SE-45-B\n## Graduated_From                                                     TAAF\n## Pilot_Type                                                Single engine\n## Hometown                                                    Kansas City\n## State                                                                KS\n## Aerial_Victory_Credits                                              NaN\n## Num_Aerial_Victory_Credits                                          0.0\n## Reported_Lost                                                       NaN\n## Reported_Lost_Date                                                  NaN\n## Reported_Lost_Location                                              NaN\n## Web_Profile                   https://cafriseabove.org/john-h-adams-jr/\n## Name: 0, dtype: object\nairmen.loc[0]\n# but because we set the row names to be the individuals actual names,\n# we can use those in the .loc statement\n## Name                                                Adams, John H., Jr.\n## Last                                                              Adams\n## First                                                      John H., Jr.\n## Graduation_Date                                    1945-04-15T00:00:00Z\n## Graduation_Rank                                                  2nd Lt\n## Class                                                           SE-45-B\n## Graduated_From                                                     TAAF\n## Pilot_Type                                                Single engine\n## Hometown                                                    Kansas City\n## State                                                                KS\n## Aerial_Victory_Credits                                              NaN\n## Num_Aerial_Victory_Credits                                          0.0\n## Reported_Lost                                                       NaN\n## Reported_Lost_Date                                                  NaN\n## Reported_Lost_Location                                              NaN\n## Web_Profile                   https://cafriseabove.org/john-h-adams-jr/\n## Name: 0, dtype: object\nairmen.loc['Adams, John H., Jr.']\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: 'Adams, John H., Jr.'\n## \n## Detailed traceback:\n##   File \"<string>\", line 1, in <module>\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 967, in __getitem__\n##     return self._getitem_axis(maybe_callable, axis=axis)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1202, in _getitem_axis\n##     return self._get_label(key, axis=axis)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1153, in _get_label\n##     return self.obj.xs(label, axis=axis)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/generic.py\", line 3876, in xs\n##     loc = index.get_loc(key)\n##   File \"/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/pandas/core/indexes/range.py\", line 389, in get_loc\n##     raise KeyError(key)\n\nWhen we select certain rows using the row names, we typically refer to the row names as the key.\n\n\nRead more about database keys here if you are interested. This is material that we will cover in Stat 351, but it may be useful for you now if you are interested in learning to program more efficiently from the start.\n\n\n\nRow and Column Names in R\n\nIn R, column and row names are just normal vectors - no special data types here!\n\nnames(airmen)\n##  [1] \"name\"                             \"last_name\"                       \n##  [3] \"first_name\"                       \"graduation_date\"                 \n##  [5] \"rank_at_graduation\"               \"class\"                           \n##  [7] \"graduated_from\"                   \"pilot_type\"                      \n##  [9] \"military_hometown_of_record\"      \"state\"                           \n## [11] \"aerial_victory_credits\"           \"number_of_aerial_victory_credits\"\n## [13] \"reported_lost\"                    \"reported_lost_date\"              \n## [15] \"reported_lost_location\"           \"web_profile\"\n\n# Set new column names\nnames(airmen) <- c('Name', 'Last', 'First', 'Graduation_Date', 'Graduation_Rank', 'Class', 'Graduated_From', 'Pilot_Type', 'Hometown', 'State', 'Aerial_Victory_Credits', 'Num_Aerial_Victory_Credits', 'Reported_Lost', 'Reported_Lost_Date', 'Reported_Lost_Location', 'Web_Profile')\n\n# Using new names\nairmen$Name[1:5]\n## [1] \"Adams, John H., Jr.\"   \"Adams, Paul\"           \"Adkins, Rutherford H.\"\n## [4] \"Adkins, Winston A.\"    \"Alexander, Halbert L.\"\n\nIf we want to set row names in R, we can try the obvious approach: ::: {.cell}\nrownames(airmen) <- airmen$Name\n## Warning: Setting row names on a tibble is deprecated.\n## Warning: non-unique values when setting 'row.names': 'Brothers, James E.',\n## 'Walker, William H.', 'Watkins, Edward W.'\n## Error in `.rowNamesDF<-`(x, value = value): duplicate 'row.names' are not allowed\n:::\nBut this runs into trouble, since we have some duplicate names. We can see which names are duplicates by using the table command combined with sort and head to truncate the output. I’m going to use the pipe command, |>, to separate these steps. This is equivalent to head(sort(table(airmen$Name), decreasing = T)) but is much easier to read since it can be read as a “recipe” of steps.\n\ntable(airmen$Name) |>\n  sort(decreasing = T) |>\n  head()\n## \n##    Brothers, James E.    Walker, William H.    Watkins, Edward W. \n##                     2                     2                     2 \n##   Adams, John H., Jr.           Adams, Paul Adkins, Rutherford H. \n##                     1                     1                     1\n\nR requires rownames to be unique, so we are better off with some other identifier that we don’t have here (SSN, military ID number, etc.) that is guaranteed to be unique. Since we don’t have that kind of information, we really don’t get any advantage by setting the rownames in R.\n\n\n\n6.3.4 Creating New Columns\nIn both Python and R, it is very easy to add new (derived) columns to a data frame, using methods similar to how you access data from a pre-existing column:\n\nairmen$initials <- paste0(substr(airmen$First, 1, 1), substr(airmen$Last, 1, 1))\nhead(airmen$initials)\n## [1] \"JA\" \"PA\" \"RA\" \"WA\" \"HA\" \"HA\"\n\n\nairmen['initials'] = airmen['First'].str.slice(0,1) + airmen['Last'].str.slice(0,1)\nairmen['initials'][0:7]\n## 0    JA\n## 1    PA\n## 2    RA\n## 3    WA\n## 4    HA\n## 5    HA\n## 6    RA\n## Name: initials, dtype: object\n\nAnother way to create new variables in R involves the use of the dplyr package. There are at least 2 advantages to using this package for these types of tasks:\n\nThere is a consistent way to call each function and engage with the data (this API - application programming interface - is designed around using the pipe, %>% discussed above)\nYou don’t have to reference the data frame name and the column name using df$colname; instead, you use the function on the data frame and work with “bare” column names inside that function.\n\n\nlibrary(dplyr)\n\nairmen <- airmen %>%\n  mutate(initials = paste0(substr(First, 1, 1), substr(Last, 1, 1)))\nselect(airmen, 1:3, initials) # Select lets us choose to only show some columns\n## # A tibble: 1,006 × 4\n##    Name                      Last      First          initials\n##    <chr>                     <chr>     <chr>          <chr>   \n##  1 Adams, John H., Jr.       Adams     John H., Jr.   JA      \n##  2 Adams, Paul               Adams     Paul           PA      \n##  3 Adkins, Rutherford H.     Adkins    Rutherford H.  RA      \n##  4 Adkins, Winston A.        Adkins    Winston A.     WA      \n##  5 Alexander, Halbert L.     Alexander Halbert L.     HA      \n##  6 Alexander, Harvey R.      Alexander Harvey R.      HA      \n##  7 Alexander, Robert R.      Alexander Robert R.      RA      \n##  8 Alexander, Walter G., III Alexander Walter G., III WA      \n##  9 Allen, Carl V.            Allen     Carl V.        CA      \n## 10 Allen, Clarence W.        Allen     Clarence W.    CA      \n## # … with 996 more rows\n\nNotice that by running mutate on the data frame, we automatically get a tibble back.\nYou can do a similar trick with the .assign function in pandas, but unlike in R, you still have to reference the dataframe object within the .assign function.\n\nairmen.assign(\n  initials = airmen['First'].str.slice(0,1) + airmen['Last'].str.slice(0,1)\n)\n##                        Name  ... initials\n## 0       Adams, John H., Jr.  ...       JA\n## 1               Adams, Paul  ...       PA\n## 2     Adkins, Rutherford H.  ...       RA\n## 3        Adkins, Winston A.  ...       WA\n## 4     Alexander, Halbert L.  ...       HA\n## ...                     ...  ...      ...\n## 1001       Young, Albert L.  ...       AY\n## 1002   Young, Benjamin, Jr.  ...       BY\n## 1003       Young, Eddie Lee  ...       EY\n## 1004          Young, Lee W.  ...       LY\n## 1005      Young, William W.  ...       WY\n## \n## [1006 rows x 17 columns]\n\n\n\n6.3.5 Subsets of Rows\nAnother major operation commonly performed on data frames is to choose only a certain set of rows, either randomly or using a logical condition.\n\n\nIn base R, we would use the subset() function:\n\n\nsubset(airmen, Last == \"Young\")\n## # A tibble: 5 × 17\n##   Name      Last  First Graduation_Date     Graduation_Rank Class Graduated_From\n##   <chr>     <chr> <chr> <dttm>              <chr>           <chr> <chr>         \n## 1 Young, A… Young Albe… 1944-03-12 00:00:00 2nd Lt          SE-4… TAAF          \n## 2 Young, B… Young Benj… 1945-05-23 00:00:00 2nd Lt          SE-4… TAAF          \n## 3 Young, E… Young Eddi… 1946-05-14 00:00:00 Flight Officer  SE-4… TAAF          \n## 4 Young, L… Young Lee … 1945-06-27 00:00:00 Flight Officer  SE-4… TAAF          \n## 5 Young, W… Young Will… 1945-09-08 00:00:00 2nd Lt          SE-4… TAAF          \n## # … with 10 more variables: Pilot_Type <chr>, Hometown <chr>, State <chr>,\n## #   Aerial_Victory_Credits <chr>, Num_Aerial_Victory_Credits <dbl>,\n## #   Reported_Lost <chr>, Reported_Lost_Date <dttm>,\n## #   Reported_Lost_Location <chr>, Web_Profile <chr>, initials <chr>\n\n\n\n\nIn python, we use the .query() function:\n\n\nairmen.query('Last == \"Young\"')\n##                       Name  ... initials\n## 1001      Young, Albert L.  ...       AY\n## 1002  Young, Benjamin, Jr.  ...       BY\n## 1003      Young, Eddie Lee  ...       EY\n## 1004         Young, Lee W.  ...       LY\n## 1005     Young, William W.  ...       WY\n## \n## [5 rows x 17 columns]\n\nNote that here, the column name (and logical condition) are in quotes, but you don’t have to reference the data frame name.\n\n\n\nIn dplyr, we use the filter function:\n\n\nairmen %>%\n  filter(Last == \"Young\")\n## # A tibble: 5 × 17\n##   Name      Last  First Graduation_Date     Graduation_Rank Class Graduated_From\n##   <chr>     <chr> <chr> <dttm>              <chr>           <chr> <chr>         \n## 1 Young, A… Young Albe… 1944-03-12 00:00:00 2nd Lt          SE-4… TAAF          \n## 2 Young, B… Young Benj… 1945-05-23 00:00:00 2nd Lt          SE-4… TAAF          \n## 3 Young, E… Young Eddi… 1946-05-14 00:00:00 Flight Officer  SE-4… TAAF          \n## 4 Young, L… Young Lee … 1945-06-27 00:00:00 Flight Officer  SE-4… TAAF          \n## 5 Young, W… Young Will… 1945-09-08 00:00:00 2nd Lt          SE-4… TAAF          \n## # … with 10 more variables: Pilot_Type <chr>, Hometown <chr>, State <chr>,\n## #   Aerial_Victory_Credits <chr>, Num_Aerial_Victory_Credits <dbl>,\n## #   Reported_Lost <chr>, Reported_Lost_Date <dttm>,\n## #   Reported_Lost_Location <chr>, Web_Profile <chr>, initials <chr>\n\n\nWe can also sample from a data frame. This can be useful when working with a large dataset, or when simulating to determine how a method performs on some data.\nFor simplicity, I’m going to primarily show you the dplyr and pandas equivalents - it is possible to do this in base R, but it’s much easier to do in dplyr.\n\n\nSampling in R\n\n\nairmen %>% sample_frac(.01)\n## # A tibble: 10 × 17\n##    Name     Last  First Graduation_Date     Graduation_Rank Class Graduated_From\n##    <chr>    <chr> <chr> <dttm>              <chr>           <chr> <chr>         \n##  1 Cheek, … Cheek Quen… 1945-09-08 00:00:00 2nd Lt          TE-4… TAAF          \n##  2 Bratche… Brat… Ever… 1943-08-30 00:00:00 2nd Lt          SE-4… TAAF          \n##  3 Hill, C… Hill  Char… 1944-02-08 00:00:00 2nd Lt          TE-4… TAAF          \n##  4 Goodwin… Good… Luth… 1944-09-08 00:00:00 1st Lt          TE-4… TAAF          \n##  5 Cross, … Cross Will… 1943-10-01 00:00:00 Flight Officer  SE-4… TAAF          \n##  6 Foreman… Fore… Walt… 1943-04-29 00:00:00 2nd Lt          SE-4… TAAF          \n##  7 Ramsey,… Rams… Pier… 1945-09-08 00:00:00 Flight Officer  TE-4… TAAF          \n##  8 Hockada… Hock… Wend… 1944-05-23 00:00:00 2nd Lt          SE-4… TAAF          \n##  9 DeBow, … DeBow Char… 1942-03-06 00:00:00 2nd Lt          SE-4… TAAF          \n## 10 Rhodes,… Rhod… Geor… 1943-10-01 00:00:00 2nd Lt          SE-4… TAAF          \n## # … with 10 more variables: Pilot_Type <chr>, Hometown <chr>, State <chr>,\n## #   Aerial_Victory_Credits <chr>, Num_Aerial_Victory_Credits <dbl>,\n## #   Reported_Lost <chr>, Reported_Lost_Date <dttm>,\n## #   Reported_Lost_Location <chr>, Web_Profile <chr>, initials <chr>\nairmen %>% sample_n(5)\n## # A tibble: 5 × 17\n##   Name      Last  First Graduation_Date     Graduation_Rank Class Graduated_From\n##   <chr>     <chr> <chr> <dttm>              <chr>           <chr> <chr>         \n## 1 Smith, E… Smith Edwa… 1943-07-28 00:00:00 2nd Lt          SE-4… TAAF          \n## 2 Terry, R… Terry Robe… NA                  Flight Officer  <NA>  TAAF          \n## 3 Johnson,… John… Lang… 1943-05-28 00:00:00 2nd Lt          SE-4… TAAF          \n## 4 Long, Cl… Long  Clyd… 1945-04-15 00:00:00 Flight Officer  SE-4… TAAF          \n## 5 Jones, M… Jones Majo… 1944-04-15 00:00:00 2nd Lt          SE-4… TAAF          \n## # … with 10 more variables: Pilot_Type <chr>, Hometown <chr>, State <chr>,\n## #   Aerial_Victory_Credits <chr>, Num_Aerial_Victory_Credits <dbl>,\n## #   Reported_Lost <chr>, Reported_Lost_Date <dttm>,\n## #   Reported_Lost_Location <chr>, Web_Profile <chr>, initials <chr>\n\n\n\n\nSampling rows is similarly easy in python:\n\n\nairmen.sample(frac=.01)\n##                          Name  ... initials\n## 750           Rogers, Amos A.  ...       AR\n## 299            Gay, Thomas L.  ...       TG\n## 324          Gray, Elliott H.  ...       EG\n## 59        Bohannon, Horace A.  ...       HB\n## 154          Cisco, Arnold W.  ...       AC\n## 20      Armstrong, William P.  ...       WA\n## 931      Whyte, James W., Jr.  ...       JW\n## 528       Lester, Clarence D.  ...       CL\n## 862          Toney, Mitchel N  ...       MT\n## 331  Greenlee, George B., Jr.  ...       GG\n## \n## [10 rows x 17 columns]\nairmen.sample(n = 5)\n##                       Name     Last  ... Web_Profile initials\n## 514          Lane, Earl R.     Lane  ...         NaN       EL\n## 322       Govan, Claude B.    Govan  ...         NaN       CG\n## 799    Smith, Frederick D.    Smith  ...         NaN       FS\n## 475  Johnson, Theopolis W.  Johnson  ...         NaN       TJ\n## 360        Harris, Cassius   Harris  ...         NaN       CH\n## \n## [5 rows x 17 columns]\n\n\n\n\n6.3.6 Summaries (part 2)\nUsing subsets of rows and columns, we can also create summaries of data frames that are more customized to what we want. This is particularly powerful when we combine it with material we learned in the previous chapter: for loops.\n\nThere are more efficient ways to generate summaries than what I will show you here, but it’s important to see how for loops might apply to this use case before we talk about ways to do this using e.g. the dplyr package in R. This will help you understand what those more efficient functions are doing “under the hood”.\n\nIf we want to create a data frame that has one row for each unique value of a variable in our source data, we can iterate through the data taking subsets and creating a set of summary variables that are customized to our problem.\nSuppose we want to take the airmen data and calculate the total number of aerial victory credits from each state.\n\nWe start out with the following game plan:\n\nCreate an empty summary dataset. We plan for the result to have one row for each state.\nIterate through the states in the dataset\nFor each state, subset the data within the loop to get only values from that state\nCalculate the total number of aerial victory credits\nAdd a row to the summary dataset with the values from the state\nExit the loop\n\nWe want to check and make sure that the reported aerial victory credit count is what we’re expecting:\n\ntable(airmen$Num_Aerial_Victory_Credits, useNA = 'ifany')\n## \n##   0   1 1.5   2   3   4 \n## 934  43   1  19   6   3\n\n\nairmen['Num_Aerial_Victory_Credits'].value_counts()\n## 0.0    934\n## 1.0     43\n## 2.0     19\n## 3.0      6\n## 4.0      3\n## 1.5      1\n## Name: Num_Aerial_Victory_Credits, dtype: int64\n\n\n\nCreating a summary dataset in R with a loop\n\n\n# Create an empty data frame to store the results\nstate_victories <- data.frame(State = NULL, total_victory_credits = NULL)\n\nfor (i in unique(airmen$State)) {\n  # Get a subset of the data with only airmen from the selected state\n  state_sub <- filter(airmen, State == i)\n  \n  victory_credits <- sum(state_sub$Num_Aerial_Victory_Credits)\n  \n  # Append our row for this state onto the summary data frame\n  state_victories <- rbind(\n    state_victories, \n    data.frame(State = i, total_victory_credits = victory_credits)\n  )\n  \n}\n\nstate_victories %>%\n  arrange(desc(total_victory_credits)) # Sort in descending order\n##    State total_victory_credits\n## 1     NY                  14.0\n## 2     CA                  12.0\n## 3     IL                  10.0\n## 4     MO                  10.0\n## 5     VA                   6.5\n## 6     OH                   6.0\n## 7     KS                   5.0\n## 8     PA                   4.0\n## 9     IA                   4.0\n## 10    GA                   4.0\n## 11    OK                   4.0\n## 12    NJ                   3.0\n## 13    AL                   3.0\n## 14    IN                   3.0\n## 15    TX                   3.0\n## 16    OR                   3.0\n## 17    WA                   3.0\n## 18    WV                   2.0\n## 19    FL                   2.0\n## 20    TN                   2.0\n## 21    WI                   2.0\n## 22    SC                   1.0\n## 23    MD                   1.0\n## 24    NC                   1.0\n## 25    MA                   1.0\n## 26    CT                   1.0\n## 27    DC                   1.0\n## 28    NE                   1.0\n## 29    In                   0.0\n## 30    RI                   0.0\n## 31 Haiti                   0.0\n## 32  <NA>                   0.0\n## 33    MI                   0.0\n## 34    LA                   0.0\n## 35    CO                   0.0\n## 36    MS                   0.0\n## 37    MN                   0.0\n## 38    AZ                   0.0\n## 39    AR                   0.0\n## 40    KY                   0.0\n## 41    HT                   0.0\n## 42    VT                   0.0\n## 43    CN                   0.0\n## 44    VI                   0.0\n## 45    DE                   0.0\n## 46    KN                   0.0\n## 47    TD                   0.0\n## 48   Unk                   0.0\n## 49    WY                   0.0\n\n\n\n\nCreating a summary dataset in python with a loop\n\n\n# Create an empty data frame to store the results\nstate_victories = pd.DataFrame(columns = ['state', 'total_victory_credits'])\n\nfor i in airmen.State.unique():\n  # Get a subset of the data with only airmen from the selected state\n  state_sub = airmen.loc[airmen.State == i]\n  \n  victory_credits = state_sub['Num_Aerial_Victory_Credits'].sum()\n  \n  state_victories.loc[i] = [i, victory_credits]\n\nstate_victories. \\\nsort_values([\"total_victory_credits\"], ascending = False). \\\niloc[1:10,]\n# Putting slashes at the end of the line here allows me to separate\n# the operations into steps, rather like using a pipe in R\n# It only works if nothing is after the \\ though, so \n# I have to put the comment below the code instead of inline\n##    state  total_victory_credits\n## CA    CA                   12.0\n## IL    IL                   10.0\n## MO    MO                   10.0\n## VA    VA                    6.5\n## OH    OH                    6.0\n## KS    KS                    5.0\n## GA    GA                    4.0\n## IA    IA                    4.0\n## OK    OK                    4.0\n\nWriting this code from scratch, not knowing Python that well, required the following google queries and pages:\n\npython summary table: https://pbpython.com/sidetable.html (I didn’t use sidetable, but I did use the pandas code here that shows how to use .value_counts())\ncreate an empty pandas dataframe: Query result. I used Method 3.\nunique values in python: Query result (not helpful). This worked for numpy but not for pandas series/dataframe columns.\npandas column unique values: Query result (helpful)\npandas sort decreasing: Query result\n\nBecause I outlined my steps above, though, I could figure out pretty easily how to google for exactly what I needed at each step. Writing out the steps ahead of time, and knowing how to work using data frames and row-by-row operations helps with googling solutions that work!\n\n\n\n\n6.3.7 Try it (all) Out!\nLet’s look at a dataset of dog breed rankings (1 = low, 5 = high) for different traits. We can read in this data using the following code:\n\nbreed_traits <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_traits.csv')\n\n\nbreed_traits = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_traits.csv')\n\nCan you complete the following tasks?\n\nPull out any dogs that have “Terrier” in the name. Hint: Testing with strings in python, Using grep and grepl in R\nCreate a data frame of all dogs who are 4 or 5 on “Affectionate with Family” and name it good_with_family. Make sure you keep all of the columns in the original data frame.\nCreate a data frame with all variables relating to coat (shedding level, coat type, coat length, grooming frequency) as well as the dog breed. Make sure you keep all rows, but only include the necessary columns.\nDraw a random sample of 10 dogs from your dataset.\nCreate a new variable, named mess, that is the product of the dog’s shedding level and their drooling level. (It’s probably a bad idea to multiply categorical variables, but in this case we’re going to go with it.) Summarize this numeric variable.\n\n\n\nSolutions (R)\n\n\nlibrary(dplyr) # data frame manipulations\nlibrary(stringr) # string comparison functions in tidyverse\n\n# 1: Terriers\nterrorists <- breed_traits[grepl(\"Terrier\", breed_traits$Breed),]\nterrorists2 <- breed_traits %>%\n  filter(str_detect(Breed, \"Terrier\"))\n\n# 2: Good with Family\ngood_with_family <- breed_traits[breed_traits$Affectionate.With.Family > 3,]\ngood_with_family2 <- breed_traits %>%\n  filter(Affectionate.With.Family > 3)\n\n# 3: Coat variables\ncoat <- breed_traits[,c(1, 4:5, 7:8)] # count column by column\ncoat2 <- breed_traits %>%\n  select(Breed, matches(c(\"Coat\", \"Grooming\", \"Shedding\"))) # tidy way\ncoat3 <- breed_traits[,grepl(\"Coat|Grooming|Shedding|Breed\", names(breed_traits))] # match string names\n\n# 4: random sample of 10 dogs\nsample_dogs <- breed_traits[sample(1:nrow(breed_traits), 10),] # base R way\nsample_dogs2 <- breed_traits %>% sample_n(10) # tidy way\n\n# 5: mess\nbreed_traits$mess <- breed_traits$Drooling.Level * breed_traits$Shedding.Level\nbreed_traits <- breed_traits %>%\n  mutate(mess2 = Drooling.Level*Shedding.Level)\n\nsummary(breed_traits$mess)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.000   2.000   3.000   4.841   6.000  20.000\nbreed_traits %>% select(mess2) %>% summary()\n##      mess2       \n##  Min.   : 0.000  \n##  1st Qu.: 2.000  \n##  Median : 3.000  \n##  Mean   : 4.841  \n##  3rd Qu.: 6.000  \n##  Max.   :20.000\n\n\n\n\nSolutions (python)\n\n\n# 1: Terriers\nterrorists = breed_traits.loc[breed_traits.Breed.str.match(r\".*Terrier.*\"),]\n\n# 2: Good with Family\ngood_with_family = breed_traits.query(\"`Affectionate With Family` > 3\")\ngood_with_family['Affectionate With Family'].min() # just checking\n\n# 3: Coat variables\n## 4\ncoat = breed_traits.loc[:,[\"Breed\", \"Shedding Level\", \"Coat Grooming Frequency\", \"Coat Type\", \"Coat Length\"]]\ncoat2 = breed_traits.iloc[:,breed_traits.columns.str.match(r\"Breed|Coat|Shedding\")]\n\n# 4: random sample of 10 dogs\nsample_dogs = breed_traits.sample(n = 10)\n\n# 5: mess\nbreed_traits['mess'] = breed_traits['Drooling Level'] * breed_traits['Shedding Level']\nbreed_traits.mess.describe\n## <bound method NDFrame.describe of 0      8\n## 1      9\n## 2      8\n## 3      8\n## 4      9\n##       ..\n## 190    2\n## 191    3\n## 192    2\n## 193    6\n## 194    3\n## Name: mess, Length: 195, dtype: int64>\nbreed_traits.mess.min()\n## 0\nbreed_traits.mess.max()\n## 20"
  },
  {
    "objectID": "06-data-structures.html#basic-plotting-examples",
    "href": "06-data-structures.html#basic-plotting-examples",
    "title": "6  Data Structures",
    "section": "6.4 Basic Plotting Examples",
    "text": "Now that you can read data in to R and python and define new variables, you can create plots! We’ll focus a bit more on this later in the class, but for now, I’d like to take a few minutes to explain how to make (basic) plots in R (with ggplot2) and in python (with plotnine, which is a ggplot2 clone).\nLet’s work with Historically Black College and University enrollment in this example:\n\nhbcu_all <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-02/hbcu_all.csv')\n\nlibrary(ggplot2)\n\n\nimport pandas as pd\nfrom plotnine import *\n\nhbcu_all = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-02/hbcu_all.csv')\n\nggplot2 and plotnine work with data frames. If you pass a data frame in as the data argument, you can refer to columns in that data with “bare” column names (you don’t have to reference the full data object using df$name or df.name; you can instead use name or \"name\"):\n\n\nggplot(hbcu_all, aes(x = Year, y = `4-year`)) + geom_line() +\n  ggtitle(\"4-year HBCU College Enrollment\")\n\n\n\n\n\n\nggplot(hbcu_all, aes(x = \"Year\", y = \"4-year\")) + geom_line() + \\\n  ggtitle(\"4-year HBCU College Enrollment\")\n## <ggplot: (8775799194874)>\n\n\n\n\nIf your data is in the right format, ggplot2 is very easy to use; if your data aren’t formatted neatly, it can be a real pain. If you want to plot multiple lines, you need to either list each variable you want to plot, one by one, or (more likely) you want to get your data into “long form”. You don’t need to know exactly how this works, but it is helpful to see the difference in the two datasets:\n\nlibrary(tidyr)\nhbcu_long <- pivot_longer(hbcu_all, -Year, names_to = \"type\", values_to = \"value\")\n\nhead(hbcu_all)\n## # A tibble: 6 × 12\n##    Year `Total enrollment`  Males Females `4-year` `2-year` `Total - Public`\n##   <dbl>              <dbl>  <dbl>   <dbl>    <dbl>    <dbl>            <dbl>\n## 1  1976             222613 104669  117944   206676    15937           156836\n## 2  1980             233557 106387  127170   218009    15548           168217\n## 3  1982             228371 104897  123474   212017    16354           165871\n## 4  1984             227519 102823  124696   212844    14675           164116\n## 5  1986             223275  97523  125752   207231    16044           162048\n## 6  1988             239755 100561  139194   223250    16505           173672\n## # … with 5 more variables: `4-year - Public` <dbl>, `2-year - Public` <dbl>,\n## #   `Total - Private` <dbl>, `4-year - Private` <dbl>, `2-year - Private` <dbl>\nhead(hbcu_long)\n## # A tibble: 6 × 3\n##    Year type              value\n##   <dbl> <chr>             <dbl>\n## 1  1976 Total enrollment 222613\n## 2  1976 Males            104669\n## 3  1976 Females          117944\n## 4  1976 4-year           206676\n## 5  1976 2-year            15937\n## 6  1976 Total - Public   156836\n\n\nhbcu_long = pd.melt(hbcu_all, id_vars = ['Year'], value_vars = hbcu_all.columns[1:11])\n\nIn the long form of the data, we have a row for each data point (year x measurement type), not for each year.\n\n\nggplot(hbcu_long, aes(x = Year, y = value, color = type)) + geom_line() +\n  ggtitle(\"HBCU College Enrollment\")\n\n\n\n\n\n\nggplot(hbcu_long, aes(x = \"Year\", y = \"value\", color = \"variable\")) + geom_line() + \\\n  ggtitle(\"HBCU College Enrollment\") + \\\n  theme(subplots_adjust={'right':0.75}) # This moves the key so it takes up 25% of the area\n## <ggplot: (8775799133931)>"
  },
  {
    "objectID": "07-data-transformations.html#identifying-the-problem-messy-data",
    "href": "07-data-transformations.html#identifying-the-problem-messy-data",
    "title": "7  Data Transformations",
    "section": "7.1 Identifying the problem: Messy data",
    "text": "library(dplyr) # Data wrangling\nlibrary(tidyr) # Data rearranging\nlibrary(tibble) # data table\n\nThese datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.\n\n\n\nTable 1\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nHere, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column).\n\n\n\nTable 2\n\n\ncountry\nyear\ntype\ncount\n\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nHere, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1.\nIn this form, we have two tables - one for population, and one for cases. Each year’s observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we’ll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year.\n\n\n\nTable 3\n\n\ncountry\nyear\nrate\n\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nThis form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can’t do anything with this format as it stands, because we can’t do math on data stored as characters. However, this form might be easier to read and record for a human being.\n\n\n\nTable 4a\n\n\ncountry\n1999\n2000\n\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\n\ncountry\n1999\n2000\n\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\n\n\n\nTable 5\n\n\ncountry\ncentury\nyear\nrate\n\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nTable 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don’t always handle dates the way you’d hope they would.\nThese variations highlight the principles which can be said to define a tidy dataset:\n\nEach variable must have its own column\nEach observation must have its own row\nEach value must have its own cell\n\n\nTry it out\nGo back through the 5 tables and determine whether each table is tidy, and if it is not, which rule or rules it violates. Figure out what you would have to do in order to compute a standardized TB infection rate per 100,000 people.\n\n\nSolution\n\n\ntable1 - this is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population*100,000.\ntable2 - each variable does not have its own column (so a single year’s observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1.\ntable3 - each value does not have its own cell (and each variable does not have its own column). In Table 3, you’d have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1.\ntable4a and table 4b - there are multiple observations in each row because there is not a column for year. To compute the rate, you’d need to “stack” the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1.\ntable 5 - each variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren’t actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you’d need to combine the two year columns together first).\n\n\n\nIt is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be:\n\nEach dataset goes into its own table\nEach variable gets its own column\n\nBy the end of this course, you should have the skills to “tidy” each of these tables."
  },
  {
    "objectID": "07-data-transformations.html#pivot-operations",
    "href": "07-data-transformations.html#pivot-operations",
    "title": "7  Data Transformations",
    "section": "7.2 Pivot operations",
    "text": "It’s fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren’t necessarily the most friendly for analysis.\n\n\n\nImage showing wide format and long format tables containing the same data. Image from https://github.com/kelseygonzalez/tidyexplain.\n\n\nThe two operations we’ll learn here are wide -> long and long -> wide.\n\n\n\nGIF showing the transition from wide format to long format and back using pivot_wider and pivot_longer commands. Image from https://github.com/kelseygonzalez/tidyexplain.\n\n\nThis animation uses the R functions pivot_wider() and pivot_longer() Animation source, but the concept is the same in both R and python.\n\n7.2.1 Longer\nIn many cases, the data come in what we might call “wide” form - some of the column names are not names of variables, but instead, are themselves values of another variable.\nTables 4a and 4b are good examples of data which is in “wide” form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively.\n\ntable4a\n## # A tibble: 3 × 3\n##   country     `1999` `2000`\n## * <chr>        <int>  <int>\n## 1 Afghanistan    745   2666\n## 2 Brazil       37737  80488\n## 3 China       212258 213766\ntable4b\n## # A tibble: 3 × 3\n##   country         `1999`     `2000`\n## * <chr>            <int>      <int>\n## 1 Afghanistan   19987071   20595360\n## 2 Brazil       172006362  174504898\n## 3 China       1272915272 1280428583\n\nThe solution to this is to rearrange the data into “long form”: to take the columns which contain values and “stack” them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn’t being stacked (e.g. country, in both the example above and the image below).\n\n\n\n\n\nA visual representation of what the pivot_longer operation looks like in practice.\n\n\n\n\nOnce our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we’ll have tidy(er) data.\n\n7.2.1.1 Manual Method\nWe can do the wide-to-long transition manually, and doing so is actually instructive.\nConsider the following table of average daily temperatures observed in Lincoln, NE in January 2022. This data is recorded in human-friendly form, in the approximate shape of a calendar. Each week has its own row, and each day has its own column.\n\n\n\nInitial data table\n\n\nOpen up the spreadsheet containing this table on your computer and let’s work through converting it to long format together.\n\n\nTo convert this data to long format, the first thing we need to do is create a new column: Day_of_Week\n\n\n\n\nAdding Day_of_Week column\n\n\n\n\n\nThen, we need to create a temperature column to hold the daily average temperature values.\n\n\n\n\nAdding Temperature column\n\n\n\n\n\nNow that we have the 3 columns our data will fit into set up, we can start moving data over.\n\nFirst, we will repeat Sunday for each of the first 5 rows in Column B, copying the values from column D into the Temperature column (Column C). Once that is done, we delete the Sunday column from our dataset to prevent duplication.\n\n\n\nCopying Sunday into Column B\n\n\n\n\n\nCopying Temperature into Column C\n\n\n\n\n\nDeleting data to prevent duplication\n\n\n\n\n\nDuplicating repeated data and moving Monday data over\n\nWe then duplicate the 5 week values, so that we can move another column of data over into our long format table.\n\n\n\nDuplicating Week 1-5 values\n\n\n\n\n\nCopying Monday into Column B\n\n\n\n\n\nCopying Temperature into Column C\n\n\n\n\n\nDeleting data to prevent duplication\n\n\n\n\n\nDuplicating repeated data and moving Tuesday data over\n\nWe then duplicate the 5 week values, so that we can move another column of data over into our long format table.\n\n\n\nDuplicating Week 1-5 values\n\n\n\n\n\nCopying Tuesday into Column B\n\n\n\n\n\nCopying Temperature into Column C\n\n\n\n\n\nDeleting data to prevent duplication\n\n\n\n\n\nRepeat these steps for each additional column\n\n\n\n\nWednesday data\n\n\n\n\n\nThursday data\n\n\nThis process is repeated for the additional days, resulting in a final data set that looks like this:\n\n\n\nFinal (long) data set\n\n\n\n\n\nArranging the data for plotting\n\nTo do something useful with this data, we might want to sort by Week number to get a chronological ordering of the temperature values:\n\n\n\nSorted by Week\n\n\nWe could even add a new variable, Day_of_Year, which would make it much easier to plot. We have data starting January 1, so in this case, the day component of the date is also the day of the year.\n\n\n\nAdding Day of Year\n\n\n\n\n\nPlotting the Data\n\n\n\nYou may at this point be wondering why we don’t just do this operation by hand… and it’s because copy-paste isn’t reproducible. I can’t guarantee that whomever did the copy-paste operation clicked on the correct cell in the spreadsheet, selected the correct values, and so on. But when I run the same code on the same file, I can be much more certain that I’ll get the same results.\n(Also, it takes forever to do the copy-paste operations manually, and I’m sure you have better ways to use your valuable time!)\n\n\n7.2.1.2 Computational Approach\nIn order to move from wide format to long format, we need to specify at least 2 of 3 possible quantities:\n\nThe values to keep to determine rows (the key)\n\nThe columns to merge into “long” form, where the column names are stored as a new variable\n\nThe values of interest\n\n\nFor most simple cases, if we have 2 of these 3 things, the pivot operation will go on as planned. Sadly, python and R require different defaults for which things are necessary, even if the fundamental operation is the same in each language.\n\n\n\nConversion using wide-to-long and merge/join statements in R\n\n\ntba <- table4a %>% \n  pivot_longer(-country, names_to = \"year\", values_to = \"cases\")\ntbb <- table4b %>% \n  pivot_longer(-country, names_to = \"year\", values_to = \"population\")\n\n# To get the tidy data, we join the two together\n# merging by country and year\nleft_join(tba, tbb, by = c(\"country\", \"year\")) %>%\n  # make year numeric b/c it's dumb not to\n  mutate(year = as.numeric(year))\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <dbl>  <int>      <int>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\nThe columns are moved to a variable with the name passed to the argument “names_to” (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument “values_to” (again, hopefully easy to remember).\nWe identify ID variables (variables which we don’t want to pivot) by not including them in the pivot statement. We can do this in one of two ways:\n\nselect only variables we want to pivot: pivot_longer(table4a, cols =1999:2000, names_to = \"year\", values_to = \"cases\")\nselect variables we don’t want to pivot, using - to remove them. (see above, where -country excludes country from the pivot operation)\n\nWhich option is easier depends how many things you’re pivoting (and how the columns are structured).\n\n\n\nSome notes on selecting variables in R\n\nNote: These details really only apply to the tidyverse - a series of R packages including dplyr and tidyr, among others.\nThere are many different ways you can select variables in tidyverse functions:\n\na character vector of column names passed to helper functions all_of and any_of. all_of will throw an error if something in the vector is missing, any_of will select anything that matches an entry in the vector but won’t error out if it doesn’t find some variable.\nUsing a pattern, such as starts_with, ends_with, contains, matches, and num_range. These functions take a single character string as an argument and select all columns that match the character string in the way indicated by the function name.\neverything matches all variables\nlast_col matches the last column\n\nYou can also use logical operators ! (taking the complement), & and | (and/or), c() to combine selections, and : to select a range of consecutive variables.\n\ndata(iris)\n\nlibrary(tidyselect) # this is the library with all of the functions described above\n# it is automatically loaded with dplyr, tidyr, etc.\n\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\niris %>% select(matches(\"Sepal\"))\n##     Sepal.Length Sepal.Width\n## 1            5.1         3.5\n## 2            4.9         3.0\n## 3            4.7         3.2\n## 4            4.6         3.1\n## 5            5.0         3.6\n## 6            5.4         3.9\n## 7            4.6         3.4\n## 8            5.0         3.4\n## 9            4.4         2.9\n## 10           4.9         3.1\n## 11           5.4         3.7\n## 12           4.8         3.4\n## 13           4.8         3.0\n## 14           4.3         3.0\n## 15           5.8         4.0\n## 16           5.7         4.4\n## 17           5.4         3.9\n## 18           5.1         3.5\n## 19           5.7         3.8\n## 20           5.1         3.8\n## 21           5.4         3.4\n## 22           5.1         3.7\n## 23           4.6         3.6\n## 24           5.1         3.3\n## 25           4.8         3.4\n## 26           5.0         3.0\n## 27           5.0         3.4\n## 28           5.2         3.5\n## 29           5.2         3.4\n## 30           4.7         3.2\n## 31           4.8         3.1\n## 32           5.4         3.4\n## 33           5.2         4.1\n## 34           5.5         4.2\n## 35           4.9         3.1\n## 36           5.0         3.2\n## 37           5.5         3.5\n## 38           4.9         3.6\n## 39           4.4         3.0\n## 40           5.1         3.4\n## 41           5.0         3.5\n## 42           4.5         2.3\n## 43           4.4         3.2\n## 44           5.0         3.5\n## 45           5.1         3.8\n## 46           4.8         3.0\n## 47           5.1         3.8\n## 48           4.6         3.2\n## 49           5.3         3.7\n## 50           5.0         3.3\n## 51           7.0         3.2\n## 52           6.4         3.2\n## 53           6.9         3.1\n## 54           5.5         2.3\n## 55           6.5         2.8\n## 56           5.7         2.8\n## 57           6.3         3.3\n## 58           4.9         2.4\n## 59           6.6         2.9\n## 60           5.2         2.7\n## 61           5.0         2.0\n## 62           5.9         3.0\n## 63           6.0         2.2\n## 64           6.1         2.9\n## 65           5.6         2.9\n## 66           6.7         3.1\n## 67           5.6         3.0\n## 68           5.8         2.7\n## 69           6.2         2.2\n## 70           5.6         2.5\n## 71           5.9         3.2\n## 72           6.1         2.8\n## 73           6.3         2.5\n## 74           6.1         2.8\n## 75           6.4         2.9\n## 76           6.6         3.0\n## 77           6.8         2.8\n## 78           6.7         3.0\n## 79           6.0         2.9\n## 80           5.7         2.6\n## 81           5.5         2.4\n## 82           5.5         2.4\n## 83           5.8         2.7\n## 84           6.0         2.7\n## 85           5.4         3.0\n## 86           6.0         3.4\n## 87           6.7         3.1\n## 88           6.3         2.3\n## 89           5.6         3.0\n## 90           5.5         2.5\n## 91           5.5         2.6\n## 92           6.1         3.0\n## 93           5.8         2.6\n## 94           5.0         2.3\n## 95           5.6         2.7\n## 96           5.7         3.0\n## 97           5.7         2.9\n## 98           6.2         2.9\n## 99           5.1         2.5\n## 100          5.7         2.8\n## 101          6.3         3.3\n## 102          5.8         2.7\n## 103          7.1         3.0\n## 104          6.3         2.9\n## 105          6.5         3.0\n## 106          7.6         3.0\n## 107          4.9         2.5\n## 108          7.3         2.9\n## 109          6.7         2.5\n## 110          7.2         3.6\n## 111          6.5         3.2\n## 112          6.4         2.7\n## 113          6.8         3.0\n## 114          5.7         2.5\n## 115          5.8         2.8\n## 116          6.4         3.2\n## 117          6.5         3.0\n## 118          7.7         3.8\n## 119          7.7         2.6\n## 120          6.0         2.2\n## 121          6.9         3.2\n## 122          5.6         2.8\n## 123          7.7         2.8\n## 124          6.3         2.7\n## 125          6.7         3.3\n## 126          7.2         3.2\n## 127          6.2         2.8\n## 128          6.1         3.0\n## 129          6.4         2.8\n## 130          7.2         3.0\n## 131          7.4         2.8\n## 132          7.9         3.8\n## 133          6.4         2.8\n## 134          6.3         2.8\n## 135          6.1         2.6\n## 136          7.7         3.0\n## 137          6.3         3.4\n## 138          6.4         3.1\n## 139          6.0         3.0\n## 140          6.9         3.1\n## 141          6.7         3.1\n## 142          6.9         3.1\n## 143          5.8         2.7\n## 144          6.8         3.2\n## 145          6.7         3.3\n## 146          6.7         3.0\n## 147          6.3         2.5\n## 148          6.5         3.0\n## 149          6.2         3.4\n## 150          5.9         3.0\niris %>% select(!matches(\"Sepal\"))\n##     Petal.Length Petal.Width    Species\n## 1            1.4         0.2     setosa\n## 2            1.4         0.2     setosa\n## 3            1.3         0.2     setosa\n## 4            1.5         0.2     setosa\n## 5            1.4         0.2     setosa\n## 6            1.7         0.4     setosa\n## 7            1.4         0.3     setosa\n## 8            1.5         0.2     setosa\n## 9            1.4         0.2     setosa\n## 10           1.5         0.1     setosa\n## 11           1.5         0.2     setosa\n## 12           1.6         0.2     setosa\n## 13           1.4         0.1     setosa\n## 14           1.1         0.1     setosa\n## 15           1.2         0.2     setosa\n## 16           1.5         0.4     setosa\n## 17           1.3         0.4     setosa\n## 18           1.4         0.3     setosa\n## 19           1.7         0.3     setosa\n## 20           1.5         0.3     setosa\n## 21           1.7         0.2     setosa\n## 22           1.5         0.4     setosa\n## 23           1.0         0.2     setosa\n## 24           1.7         0.5     setosa\n## 25           1.9         0.2     setosa\n## 26           1.6         0.2     setosa\n## 27           1.6         0.4     setosa\n## 28           1.5         0.2     setosa\n## 29           1.4         0.2     setosa\n## 30           1.6         0.2     setosa\n## 31           1.6         0.2     setosa\n## 32           1.5         0.4     setosa\n## 33           1.5         0.1     setosa\n## 34           1.4         0.2     setosa\n## 35           1.5         0.2     setosa\n## 36           1.2         0.2     setosa\n## 37           1.3         0.2     setosa\n## 38           1.4         0.1     setosa\n## 39           1.3         0.2     setosa\n## 40           1.5         0.2     setosa\n## 41           1.3         0.3     setosa\n## 42           1.3         0.3     setosa\n## 43           1.3         0.2     setosa\n## 44           1.6         0.6     setosa\n## 45           1.9         0.4     setosa\n## 46           1.4         0.3     setosa\n## 47           1.6         0.2     setosa\n## 48           1.4         0.2     setosa\n## 49           1.5         0.2     setosa\n## 50           1.4         0.2     setosa\n## 51           4.7         1.4 versicolor\n## 52           4.5         1.5 versicolor\n## 53           4.9         1.5 versicolor\n## 54           4.0         1.3 versicolor\n## 55           4.6         1.5 versicolor\n## 56           4.5         1.3 versicolor\n## 57           4.7         1.6 versicolor\n## 58           3.3         1.0 versicolor\n## 59           4.6         1.3 versicolor\n## 60           3.9         1.4 versicolor\n## 61           3.5         1.0 versicolor\n## 62           4.2         1.5 versicolor\n## 63           4.0         1.0 versicolor\n## 64           4.7         1.4 versicolor\n## 65           3.6         1.3 versicolor\n## 66           4.4         1.4 versicolor\n## 67           4.5         1.5 versicolor\n## 68           4.1         1.0 versicolor\n## 69           4.5         1.5 versicolor\n## 70           3.9         1.1 versicolor\n## 71           4.8         1.8 versicolor\n## 72           4.0         1.3 versicolor\n## 73           4.9         1.5 versicolor\n## 74           4.7         1.2 versicolor\n## 75           4.3         1.3 versicolor\n## 76           4.4         1.4 versicolor\n## 77           4.8         1.4 versicolor\n## 78           5.0         1.7 versicolor\n## 79           4.5         1.5 versicolor\n## 80           3.5         1.0 versicolor\n## 81           3.8         1.1 versicolor\n## 82           3.7         1.0 versicolor\n## 83           3.9         1.2 versicolor\n## 84           5.1         1.6 versicolor\n## 85           4.5         1.5 versicolor\n## 86           4.5         1.6 versicolor\n## 87           4.7         1.5 versicolor\n## 88           4.4         1.3 versicolor\n## 89           4.1         1.3 versicolor\n## 90           4.0         1.3 versicolor\n## 91           4.4         1.2 versicolor\n## 92           4.6         1.4 versicolor\n## 93           4.0         1.2 versicolor\n## 94           3.3         1.0 versicolor\n## 95           4.2         1.3 versicolor\n## 96           4.2         1.2 versicolor\n## 97           4.2         1.3 versicolor\n## 98           4.3         1.3 versicolor\n## 99           3.0         1.1 versicolor\n## 100          4.1         1.3 versicolor\n## 101          6.0         2.5  virginica\n## 102          5.1         1.9  virginica\n## 103          5.9         2.1  virginica\n## 104          5.6         1.8  virginica\n## 105          5.8         2.2  virginica\n## 106          6.6         2.1  virginica\n## 107          4.5         1.7  virginica\n## 108          6.3         1.8  virginica\n## 109          5.8         1.8  virginica\n## 110          6.1         2.5  virginica\n## 111          5.1         2.0  virginica\n## 112          5.3         1.9  virginica\n## 113          5.5         2.1  virginica\n## 114          5.0         2.0  virginica\n## 115          5.1         2.4  virginica\n## 116          5.3         2.3  virginica\n## 117          5.5         1.8  virginica\n## 118          6.7         2.2  virginica\n## 119          6.9         2.3  virginica\n## 120          5.0         1.5  virginica\n## 121          5.7         2.3  virginica\n## 122          4.9         2.0  virginica\n## 123          6.7         2.0  virginica\n## 124          4.9         1.8  virginica\n## 125          5.7         2.1  virginica\n## 126          6.0         1.8  virginica\n## 127          4.8         1.8  virginica\n## 128          4.9         1.8  virginica\n## 129          5.6         2.1  virginica\n## 130          5.8         1.6  virginica\n## 131          6.1         1.9  virginica\n## 132          6.4         2.0  virginica\n## 133          5.6         2.2  virginica\n## 134          5.1         1.5  virginica\n## 135          5.6         1.4  virginica\n## 136          6.1         2.3  virginica\n## 137          5.6         2.4  virginica\n## 138          5.5         1.8  virginica\n## 139          4.8         1.8  virginica\n## 140          5.4         2.1  virginica\n## 141          5.6         2.4  virginica\n## 142          5.1         2.3  virginica\n## 143          5.1         1.9  virginica\n## 144          5.9         2.3  virginica\n## 145          5.7         2.5  virginica\n## 146          5.2         2.3  virginica\n## 147          5.0         1.9  virginica\n## 148          5.2         2.0  virginica\n## 149          5.4         2.3  virginica\n## 150          5.1         1.8  virginica\niris %>% select(ends_with(\"Width\"))\n##     Sepal.Width Petal.Width\n## 1           3.5         0.2\n## 2           3.0         0.2\n## 3           3.2         0.2\n## 4           3.1         0.2\n## 5           3.6         0.2\n## 6           3.9         0.4\n## 7           3.4         0.3\n## 8           3.4         0.2\n## 9           2.9         0.2\n## 10          3.1         0.1\n## 11          3.7         0.2\n## 12          3.4         0.2\n## 13          3.0         0.1\n## 14          3.0         0.1\n## 15          4.0         0.2\n## 16          4.4         0.4\n## 17          3.9         0.4\n## 18          3.5         0.3\n## 19          3.8         0.3\n## 20          3.8         0.3\n## 21          3.4         0.2\n## 22          3.7         0.4\n## 23          3.6         0.2\n## 24          3.3         0.5\n## 25          3.4         0.2\n## 26          3.0         0.2\n## 27          3.4         0.4\n## 28          3.5         0.2\n## 29          3.4         0.2\n## 30          3.2         0.2\n## 31          3.1         0.2\n## 32          3.4         0.4\n## 33          4.1         0.1\n## 34          4.2         0.2\n## 35          3.1         0.2\n## 36          3.2         0.2\n## 37          3.5         0.2\n## 38          3.6         0.1\n## 39          3.0         0.2\n## 40          3.4         0.2\n## 41          3.5         0.3\n## 42          2.3         0.3\n## 43          3.2         0.2\n## 44          3.5         0.6\n## 45          3.8         0.4\n## 46          3.0         0.3\n## 47          3.8         0.2\n## 48          3.2         0.2\n## 49          3.7         0.2\n## 50          3.3         0.2\n## 51          3.2         1.4\n## 52          3.2         1.5\n## 53          3.1         1.5\n## 54          2.3         1.3\n## 55          2.8         1.5\n## 56          2.8         1.3\n## 57          3.3         1.6\n## 58          2.4         1.0\n## 59          2.9         1.3\n## 60          2.7         1.4\n## 61          2.0         1.0\n## 62          3.0         1.5\n## 63          2.2         1.0\n## 64          2.9         1.4\n## 65          2.9         1.3\n## 66          3.1         1.4\n## 67          3.0         1.5\n## 68          2.7         1.0\n## 69          2.2         1.5\n## 70          2.5         1.1\n## 71          3.2         1.8\n## 72          2.8         1.3\n## 73          2.5         1.5\n## 74          2.8         1.2\n## 75          2.9         1.3\n## 76          3.0         1.4\n## 77          2.8         1.4\n## 78          3.0         1.7\n## 79          2.9         1.5\n## 80          2.6         1.0\n## 81          2.4         1.1\n## 82          2.4         1.0\n## 83          2.7         1.2\n## 84          2.7         1.6\n## 85          3.0         1.5\n## 86          3.4         1.6\n## 87          3.1         1.5\n## 88          2.3         1.3\n## 89          3.0         1.3\n## 90          2.5         1.3\n## 91          2.6         1.2\n## 92          3.0         1.4\n## 93          2.6         1.2\n## 94          2.3         1.0\n## 95          2.7         1.3\n## 96          3.0         1.2\n## 97          2.9         1.3\n## 98          2.9         1.3\n## 99          2.5         1.1\n## 100         2.8         1.3\n## 101         3.3         2.5\n## 102         2.7         1.9\n## 103         3.0         2.1\n## 104         2.9         1.8\n## 105         3.0         2.2\n## 106         3.0         2.1\n## 107         2.5         1.7\n## 108         2.9         1.8\n## 109         2.5         1.8\n## 110         3.6         2.5\n## 111         3.2         2.0\n## 112         2.7         1.9\n## 113         3.0         2.1\n## 114         2.5         2.0\n## 115         2.8         2.4\n## 116         3.2         2.3\n## 117         3.0         1.8\n## 118         3.8         2.2\n## 119         2.6         2.3\n## 120         2.2         1.5\n## 121         3.2         2.3\n## 122         2.8         2.0\n## 123         2.8         2.0\n## 124         2.7         1.8\n## 125         3.3         2.1\n## 126         3.2         1.8\n## 127         2.8         1.8\n## 128         3.0         1.8\n## 129         2.8         2.1\n## 130         3.0         1.6\n## 131         2.8         1.9\n## 132         3.8         2.0\n## 133         2.8         2.2\n## 134         2.8         1.5\n## 135         2.6         1.4\n## 136         3.0         2.3\n## 137         3.4         2.4\n## 138         3.1         1.8\n## 139         3.0         1.8\n## 140         3.1         2.1\n## 141         3.1         2.4\n## 142         3.1         2.3\n## 143         2.7         1.9\n## 144         3.2         2.3\n## 145         3.3         2.5\n## 146         3.0         2.3\n## 147         2.5         1.9\n## 148         3.0         2.0\n## 149         3.4         2.3\n## 150         3.0         1.8\n\niris %>% select(Sepal.Length:Petal.Length)\n##     Sepal.Length Sepal.Width Petal.Length\n## 1            5.1         3.5          1.4\n## 2            4.9         3.0          1.4\n## 3            4.7         3.2          1.3\n## 4            4.6         3.1          1.5\n## 5            5.0         3.6          1.4\n## 6            5.4         3.9          1.7\n## 7            4.6         3.4          1.4\n## 8            5.0         3.4          1.5\n## 9            4.4         2.9          1.4\n## 10           4.9         3.1          1.5\n## 11           5.4         3.7          1.5\n## 12           4.8         3.4          1.6\n## 13           4.8         3.0          1.4\n## 14           4.3         3.0          1.1\n## 15           5.8         4.0          1.2\n## 16           5.7         4.4          1.5\n## 17           5.4         3.9          1.3\n## 18           5.1         3.5          1.4\n## 19           5.7         3.8          1.7\n## 20           5.1         3.8          1.5\n## 21           5.4         3.4          1.7\n## 22           5.1         3.7          1.5\n## 23           4.6         3.6          1.0\n## 24           5.1         3.3          1.7\n## 25           4.8         3.4          1.9\n## 26           5.0         3.0          1.6\n## 27           5.0         3.4          1.6\n## 28           5.2         3.5          1.5\n## 29           5.2         3.4          1.4\n## 30           4.7         3.2          1.6\n## 31           4.8         3.1          1.6\n## 32           5.4         3.4          1.5\n## 33           5.2         4.1          1.5\n## 34           5.5         4.2          1.4\n## 35           4.9         3.1          1.5\n## 36           5.0         3.2          1.2\n## 37           5.5         3.5          1.3\n## 38           4.9         3.6          1.4\n## 39           4.4         3.0          1.3\n## 40           5.1         3.4          1.5\n## 41           5.0         3.5          1.3\n## 42           4.5         2.3          1.3\n## 43           4.4         3.2          1.3\n## 44           5.0         3.5          1.6\n## 45           5.1         3.8          1.9\n## 46           4.8         3.0          1.4\n## 47           5.1         3.8          1.6\n## 48           4.6         3.2          1.4\n## 49           5.3         3.7          1.5\n## 50           5.0         3.3          1.4\n## 51           7.0         3.2          4.7\n## 52           6.4         3.2          4.5\n## 53           6.9         3.1          4.9\n## 54           5.5         2.3          4.0\n## 55           6.5         2.8          4.6\n## 56           5.7         2.8          4.5\n## 57           6.3         3.3          4.7\n## 58           4.9         2.4          3.3\n## 59           6.6         2.9          4.6\n## 60           5.2         2.7          3.9\n## 61           5.0         2.0          3.5\n## 62           5.9         3.0          4.2\n## 63           6.0         2.2          4.0\n## 64           6.1         2.9          4.7\n## 65           5.6         2.9          3.6\n## 66           6.7         3.1          4.4\n## 67           5.6         3.0          4.5\n## 68           5.8         2.7          4.1\n## 69           6.2         2.2          4.5\n## 70           5.6         2.5          3.9\n## 71           5.9         3.2          4.8\n## 72           6.1         2.8          4.0\n## 73           6.3         2.5          4.9\n## 74           6.1         2.8          4.7\n## 75           6.4         2.9          4.3\n## 76           6.6         3.0          4.4\n## 77           6.8         2.8          4.8\n## 78           6.7         3.0          5.0\n## 79           6.0         2.9          4.5\n## 80           5.7         2.6          3.5\n## 81           5.5         2.4          3.8\n## 82           5.5         2.4          3.7\n## 83           5.8         2.7          3.9\n## 84           6.0         2.7          5.1\n## 85           5.4         3.0          4.5\n## 86           6.0         3.4          4.5\n## 87           6.7         3.1          4.7\n## 88           6.3         2.3          4.4\n## 89           5.6         3.0          4.1\n## 90           5.5         2.5          4.0\n## 91           5.5         2.6          4.4\n## 92           6.1         3.0          4.6\n## 93           5.8         2.6          4.0\n## 94           5.0         2.3          3.3\n## 95           5.6         2.7          4.2\n## 96           5.7         3.0          4.2\n## 97           5.7         2.9          4.2\n## 98           6.2         2.9          4.3\n## 99           5.1         2.5          3.0\n## 100          5.7         2.8          4.1\n## 101          6.3         3.3          6.0\n## 102          5.8         2.7          5.1\n## 103          7.1         3.0          5.9\n## 104          6.3         2.9          5.6\n## 105          6.5         3.0          5.8\n## 106          7.6         3.0          6.6\n## 107          4.9         2.5          4.5\n## 108          7.3         2.9          6.3\n## 109          6.7         2.5          5.8\n## 110          7.2         3.6          6.1\n## 111          6.5         3.2          5.1\n## 112          6.4         2.7          5.3\n## 113          6.8         3.0          5.5\n## 114          5.7         2.5          5.0\n## 115          5.8         2.8          5.1\n## 116          6.4         3.2          5.3\n## 117          6.5         3.0          5.5\n## 118          7.7         3.8          6.7\n## 119          7.7         2.6          6.9\n## 120          6.0         2.2          5.0\n## 121          6.9         3.2          5.7\n## 122          5.6         2.8          4.9\n## 123          7.7         2.8          6.7\n## 124          6.3         2.7          4.9\n## 125          6.7         3.3          5.7\n## 126          7.2         3.2          6.0\n## 127          6.2         2.8          4.8\n## 128          6.1         3.0          4.9\n## 129          6.4         2.8          5.6\n## 130          7.2         3.0          5.8\n## 131          7.4         2.8          6.1\n## 132          7.9         3.8          6.4\n## 133          6.4         2.8          5.6\n## 134          6.3         2.8          5.1\n## 135          6.1         2.6          5.6\n## 136          7.7         3.0          6.1\n## 137          6.3         3.4          5.6\n## 138          6.4         3.1          5.5\n## 139          6.0         3.0          4.8\n## 140          6.9         3.1          5.4\n## 141          6.7         3.1          5.6\n## 142          6.9         3.1          5.1\n## 143          5.8         2.7          5.1\n## 144          6.8         3.2          5.9\n## 145          6.7         3.3          5.7\n## 146          6.7         3.0          5.2\n## 147          6.3         2.5          5.0\n## 148          6.5         3.0          5.2\n## 149          6.2         3.4          5.4\n## 150          5.9         3.0          5.1\n\nThese selection helpers can be extremely useful when choosing columns to pivot.\n\n\n\nConversion using wide-to-long and merge/join statements in Python\n\nA similar operation can be done in python using pd.melt. Think of “melting” your data down from a metal object to a liquid. (There actually used to be an R package that used cast/melt as the terms for pivot wider/longer, but it has been superseded by pivot_wider and pivot_longer syntax).\nAs in R, we identify the variables that we DON’T want to pivot, but in python syntax, these are called id_vars. Then, as in R, we need to specify the names of the columns we want to have at the end. In Python, we also have to specify value variables: variables that we will be pivoting (so we specify both those we are pivoting and those we aren’t). Remember, id_vars are variables that we would copy-paste over and over with each set of variables in the manual pivot operation, where value_vars are the set of columns we want to combine into variable:value pairs.\n\nimport pandas as pd\ntable4a = r.table4a # get variable from R within python\ntable4b = r.table4b\n\ntba = pd.melt(table4a, id_vars = ['country'], \n                       value_vars = ['1999', '2000'], \n                       var_name = 'year', \n                       value_name = 'cases')\ntbb = pd.melt(table4a, id_vars = ['country'], \n                       value_vars = ['1999', '2000'], \n                       var_name = 'year', \n                       value_name = 'population')\n\nfinal = pd.merge(tba, tbb, on= ['country', 'year'])\n# convert year to numeric from string\nfinal[\"year\"] = pd.to_numeric(final[\"year\"])\n\nfinal\n##        country  year   cases  population\n## 0  Afghanistan  1999     745         745\n## 1       Brazil  1999   37737       37737\n## 2        China  1999  212258      212258\n## 3  Afghanistan  2000    2666        2666\n## 4       Brazil  2000   80488       80488\n## 5        China  2000  213766      213766\n\n\n\n\nConversion using new variables, appending tables, and wide-to-long statements in R\n\nIf we wanted to avoid the table join, we could do this process another way: first, we would add a column to each tibble called id with values “cases” and “population” respectively. Then, we could bind the two tables together by row (so stack them on top of each other). We could then do a wide-to-long pivot, followed by a long-to-wide pivot to get our data into tidy form.\n\n# Create ID columns\ntable4a.x <- table4a %>% mutate(id = \"cases\")\ntable4b.x <- table4b %>% mutate(id = \"population\")\n# Create one table\ntable4 <- bind_rows(table4a.x, table4b.x)\n\ntable4_long <- table4 %>%\n  # rearrange columns\n  select(country, id, `1999`, `2000`) %>%\n  # Don't pivot country or id\n  pivot_longer(-c(country:id), names_to = \"year\", values_to = \"count\")\n\n# Intermediate fully-long form\ntable4_long\n## # A tibble: 12 × 4\n##    country     id         year       count\n##    <chr>       <chr>      <chr>      <int>\n##  1 Afghanistan cases      1999         745\n##  2 Afghanistan cases      2000        2666\n##  3 Brazil      cases      1999       37737\n##  4 Brazil      cases      2000       80488\n##  5 China       cases      1999      212258\n##  6 China       cases      2000      213766\n##  7 Afghanistan population 1999    19987071\n##  8 Afghanistan population 2000    20595360\n##  9 Brazil      population 1999   172006362\n## 10 Brazil      population 2000   174504898\n## 11 China       population 1999  1272915272\n## 12 China       population 2000  1280428583\n\n# make wider, with case and population columns\ntable4_tidy <- table4_long %>%\n  pivot_wider(names_from = id, values_from = count)\n\ntable4_tidy\n## # A tibble: 6 × 4\n##   country     year   cases population\n##   <chr>       <chr>  <int>      <int>\n## 1 Afghanistan 1999     745   19987071\n## 2 Afghanistan 2000    2666   20595360\n## 3 Brazil      1999   37737  172006362\n## 4 Brazil      2000   80488  174504898\n## 5 China       1999  212258 1272915272\n## 6 China       2000  213766 1280428583\n\n\n\n\nConversion using new variables, appending tables, and wide-to-long statements in Python\n\nIf we wanted to avoid the table join, we could do this process another way: first, we would add a column to each tibble called id with values “cases” and “population” respectively. Then, we could bind the two tables together by row (so stack them on top of each other). We could then do a wide-to-long pivot, followed by a long-to-wide pivot to get our data into tidy form.\n\n# Create ID columns\ntable4a_x = r.table4a.assign(id= \"cases\")\ntable4b_x = r.table4b.assign(id = \"population\")\n\n# Create one table\ntable4 = pd.concat([table4a_x, table4b_x])\ntable4\n##        country        1999        2000          id\n## 0  Afghanistan         745        2666       cases\n## 1       Brazil       37737       80488       cases\n## 2        China      212258      213766       cases\n## 0  Afghanistan    19987071    20595360  population\n## 1       Brazil   172006362   174504898  population\n## 2        China  1272915272  1280428583  population\ntable4_long = pd.melt(table4, \n                      id_vars = ['country', 'id'], \n                      value_vars = ['1999', '2000'], \n                      var_name = 'year')\n\n# Intermediate fully-long form\ntable4_long\n\n# make wider, with case and population columns\n# Index is the columns we want to keep that ID a unique combination of variables\n# Columns are the columns that become new labels\n# There should be a single value for each combination of index and the column variable(s)\n##         country          id  year       value\n## 0   Afghanistan       cases  1999         745\n## 1        Brazil       cases  1999       37737\n## 2         China       cases  1999      212258\n## 3   Afghanistan  population  1999    19987071\n## 4        Brazil  population  1999   172006362\n## 5         China  population  1999  1272915272\n## 6   Afghanistan       cases  2000        2666\n## 7        Brazil       cases  2000       80488\n## 8         China       cases  2000      213766\n## 9   Afghanistan  population  2000    20595360\n## 10       Brazil  population  2000   174504898\n## 11        China  population  2000  1280428583\ntable4_tidy = pd.pivot(table4_long, \n                       index = ['country', 'year'],  \n                       columns = ['id'], \n                       values = 'value')\n\ntable4_tidy\n## id                 cases  population\n## country     year                    \n## Afghanistan 1999     745    19987071\n##             2000    2666    20595360\n## Brazil      1999   37737   172006362\n##             2000   80488   174504898\n## China       1999  212258  1272915272\n##             2000  213766  1280428583\ntable4_tidy.reset_index() # to remove the index if you want the normal data table\n## id      country  year   cases  population\n## 0   Afghanistan  1999     745    19987071\n## 1   Afghanistan  2000    2666    20595360\n## 2        Brazil  1999   37737   172006362\n## 3        Brazil  2000   80488   174504898\n## 4         China  1999  212258  1272915272\n## 5         China  2000  213766  1280428583\n\nWe will talk more about long-to-wide pivot operations below. For now, it’s enough to know that the long-to-wide operation can be useful in getting your data into tidy form.\n\nTransitioning from wide data to long data isn’t too complicated in most situations – and it definitely beats doing that operation by hand, even for short, simple tables.\nIt takes some getting used to, but once you get a feel for how to do these operations, you’ll be able to handle messy data reproducibly - instead of describing how you did XYZ operations in Excel, you can provide a script that will take the original data as input and spit out clean, analysis-ready data as output.\n\nBecause wide-to-long transformations end up combining values from several columns into a single column, you can run into issues with type conversions that happen implicitly. If you try to pivot_longer() using a character column mixed in with numeric columns, your “value” column will be converted to a character automatically.\n\n\n\n\n7.2.2 Wider\nWhile it’s very common to need to transform data into a longer format, it’s not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again.\nTable 2 is an example of a table that is in long format but needs to be converted to a wider layout to be “tidy” - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows.\n\n\n\n\n\nA visual representation of what the pivot_wider operation looks like in practice.\n\n\n\n\n\n\nIn R, long-to-wide conversion operations are performed using pivot_wider()\n\n\ntable2 %>%\n  pivot_wider(names_from = type, values_from = count)\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <int>  <int>      <int>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\n\n\n\nIn python, long-to-wide conversion operations are performed using pd.pivot()\n\n\ntable2 = r.table2\n\npd.pivot(table2, index = ['country', 'year'], columns = 'type', values = 'count')\n\n# We can get rid of the index by adding a .reset_index() to the end of the command\n## type               cases  population\n## country     year                    \n## Afghanistan 1999     745    19987071\n##             2000    2666    20595360\n## Brazil      1999   37737   172006362\n##             2000   80488   174504898\n## China       1999  212258  1272915272\n##             2000  213766  1280428583\npd.pivot(table2, index = ['country', 'year'], columns = 'type', values = 'count').reset_index()\n## type      country  year   cases  population\n## 0     Afghanistan  1999     745    19987071\n## 1     Afghanistan  2000    2666    20595360\n## 2          Brazil  1999   37737   172006362\n## 3          Brazil  2000   80488   174504898\n## 4           China  1999  212258  1272915272\n## 5           China  2000  213766  1280428583\n\n\n\n\nTry it out!\nLet’s work with the dog breed traits data to see if we can get it into long format and then back into wide format based on some specific criteria.\n\nbreed_traits <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_traits.csv') %>%\n  # There's something funky with the spaces in this data frame... so make them ASCII\n  # (this fixes the encoding issues)\n  mutate(Breed = iconv(Breed, to = \"ASCII//translit\"))\n\n\nfrom unidecode import unidecode # Change unicode characters to ascii\nbreed_traits = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-01/breed_traits.csv')\nbreed_traits.Breed = breed_traits[\"Breed\"].transform(unidecode)\n\nYou may remember that the breed trait data includes a number of 1-5 numeric variables indicating score on various dimensions such as affection, shedding, openness, and playfulness.\nTask 1: Take all of the 1-5 numeric variables in the dataset and make a long-format dataset. Choose 5 breeds of your choice. Then, using your dataset, plot each breed’s score, with each dimension as a separate facet.\nYour code in ggplot2 would look something like this:\n\nggplot(long_data, aes(x = breed, y = value)) + \n  geom_col() + \n  facet_wrap(~variable) + \n  coord_flip()\n\nAnd in plotnine, it would look something like this:\n\nfrom plotnine import *\nggplot(long_data, aes(x = \"breed\", y = \"value\")) + \n  geom_col() + \n  facet_wrap('variable') +\n  coord_flip()\n\n\n\nR solution\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nmy_breeds <- breed_traits %>%\n  filter(Breed %in% c(\"Beagles\", \"Dachshunds\", \"Samoyeds\", \"Russell Terriers\", \"Lhasa Apsos\")) %>%\n  # Remove variables that aren't 1-5\n  pivot_longer(-c(Breed, Coat.Type, Coat.Length), names_to = \"Trait\", values_to = \"value\") \n\nmy_breeds %>%\n  ggplot(aes(x = Breed, y = value)) + geom_col() + facet_wrap(~Trait) + \n  coord_flip()\n\n\n\n\n\n\n\npython solution\n\n\nfrom plotnine import *\n\nbreed_list = [\"Beagles\", \"Dachshunds\", \"Samoyeds\", \"Russell Terriers\", \"Lhasa Apsos\"]\nmy_breeds = breed_traits.loc[breed_traits.Breed.isin(breed_list)].\\\n              melt(id_vars = ['Breed', 'Coat Type', 'Coat Length'], \n                   value_name = 'value',\n                   var_name = 'Trait')\n               \nggplot(my_breeds, aes(x = \"Breed\", y = \"value\")) + geom_col() + facet_wrap(\"Trait\") + coord_flip()\n## <ggplot: (8778678297465)>\n## \n## /__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/utils.py:371: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\nTask 2: Using your data from the previous task, determine which traits are 5 = good and which traits are 5 = bad (This may not be the same list for everyone!). Create a new variable in the data frame that encodes this information. There are several ways to do this task in R and in Python, so see if you can find a way that works for you.\n\n\nR solution\n\n\ngood_traits <- c(\"Affectionate.With.Family\", \"Good.With.Young.Children\", \"Good.With.Other.Dogs\", \"Openness.To.Strangers\", \"Playfulness.Level\", \"Adaptability.Level\", \"Trainability.Level\")\n\nmy_breeds <- my_breeds %>%\n  mutate(trait_type = ifelse(Trait %in% good_traits, \"Good\", \"Bad\"))\n\n\n\n\npython solution\n\n\nimport numpy as np\n\ngood_traits = [\"Affectionate With Family\", \"Good With Young Children\", \"Good With Other Dogs\", \"Openness To Strangers\", \"Playfulness Level\", \"Adaptability Level\", \"Trainability Level\"]\n\n# https://stackoverflow.com/questions/19913659/pandas-conditional-creation-of-a-series-dataframe-column\n\nmy_breeds[\"trait_type\"] = np.where(my_breeds[\"Trait\"].isin(good_traits), 'Good', 'Bad')\n\n\nTask 3: Using your data from the previous task, summarize your data so that you calculate the average score for good and bad traits for each dog breed. For each Breed, your solution should have two rows: one for good traits, one for bad traits. It may help to sketch out what a solution should look like for each Breed before you begin.\n\n\nR solution\n\n\nsummary_traits = data.frame()\n\nfor (i in unique(my_breeds$Breed)) {\n  sub_df <- filter(my_breeds, Breed == i)\n  sub_df_good <- filter(sub_df, trait_type == \"Good\")\n  sub_df_bad <- filter(sub_df, trait_type == \"Bad\")\n  summary_traits <- rbind(\n    summary_traits, \n    data.frame(Breed = i, \n               trait_type = c(\"Good\", \"Bad\"), \n               average = c(mean(sub_df_good$value, na.rm = T), mean(sub_df_bad$value)))\n  )\n  \n}\n\nsummary_traits\n##               Breed trait_type  average\n## 1           Beagles       Good 3.857143\n## 2           Beagles        Bad 2.857143\n## 3        Dachshunds       Good 4.000000\n## 4        Dachshunds        Bad 3.000000\n## 5          Samoyeds       Good 4.428571\n## 6          Samoyeds        Bad 3.428571\n## 7  Russell Terriers       Good 4.285714\n## 8  Russell Terriers        Bad 3.428571\n## 9       Lhasa Apsos       Good 3.571429\n## 10      Lhasa Apsos        Bad 2.714286\n\n\n\n\npython solution\n\n\nsummary_traits = pd.DataFrame()\n\nfor i in np.unique(my_breeds.Breed):\n  sub_df = my_breeds.loc[my_breeds.Breed == i]\n  sub_df_good = sub_df.loc[sub_df.trait_type == \"Good\"]\n  sub_df_bad = sub_df.loc[sub_df.trait_type == \"Bad\"]\n  \n  summary_traits = pd.concat([\n    summary_traits, \n    pd.DataFrame({'Breed': [i, i], 'trait_type': [\"Good\", \"Bad\"], \n    'average': [sub_df_good.value.mean(), sub_df_bad.value.mean()]})\n    ]\n  )\n\n# Reset index so it shows the row number\nsummary_traits = summary_traits.reset_index().drop(['index'], axis = 1)\nsummary_traits\n##               Breed trait_type   average\n## 0           Beagles       Good  3.857143\n## 1           Beagles        Bad  2.857143\n## 2        Dachshunds       Good  4.000000\n## 3        Dachshunds        Bad  3.000000\n## 4       Lhasa Apsos       Good  3.571429\n## 5       Lhasa Apsos        Bad  2.714286\n## 6  Russell Terriers       Good  4.285714\n## 7  Russell Terriers        Bad  3.428571\n## 8          Samoyeds       Good  4.428571\n## 9          Samoyeds        Bad  3.428571\n\n\nTask 4: Pivot your summary data frame from long to wide to get an overall summary of the good and bad for each of your 5 breeds. Plot the good score on the x axis, the bad score on the y axis, and use the breed name as the label. Use geom_label instead of geom_point.\n\n\nR solution\n\n\nsummary_traits %>%\n  pivot_wider(id_cols = 1, names_from = 2, values_from = 3) %>%\n  ggplot(aes(x = Good, y = Bad, label = Breed)) + geom_label()\n\n\n\n\n\n\n\npython solution\n\n\ntmp = summary_traits.pivot(index = \"Breed\", columns = 'trait_type', values = 'average')\ntmp[\"Breed\"] = tmp.index # Need to recreate the Breed column\n\nggplot(tmp, aes(x = \"Good\", y = \"Bad\", label = \"Breed\")) + geom_label()\n## <ggplot: (8778677982269)>"
  },
  {
    "objectID": "08-data-cleaning.html#merging-tables",
    "href": "08-data-cleaning.html#merging-tables",
    "title": "8  Data Summaries and Data Cleaning",
    "section": "8.1 Merging Tables",
    "text": "Sometimes, we have two tables that we want to join together by a certain variable.\nWe know how to work on one table at a time, creating new variables, editing old variables, and even reformatting the table using wide and long format, but data doesn’t always come organized in one table at a time. Instead, some data may be organized relationally - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between individuals that have to be documented.\n\n\n\nExample: Primary School Organization\n\nEach individual has certain characteristics:\n\nfull_name\ngender\nbirth date\nID number\n\nEach student has specific characteristics:\n\nID number\nparent name\nparent phone number\nmedical information\nClass ID\n\nTeachers may also have additional information:\n\nID number\nClass ID\nemployment start date\neducation level\ncompensation level\n\nThere are also fields like grades, which occur for each student in each class, but multiple times a year.\n\nID number\nStudent ID\nClass ID\nyear\nterm number\nsubject\ngrade\ncomment\n\nAnd for teachers, there are employment records on a yearly basis\n\nID number\nEmployee ID\nyear\nrating\ncomment\n\nBut each class also has characteristics that describe the whole class as a unit:\n\nlocation ID\nclass ID\nmeeting time\ngrade level\n\nEach location might also have some logistical information attached:\n\nlocation ID\nroom number\nbuilding\nnumber of seats\nAV equipment\n\n \nWe could go on, but you can see that this data is hierarchical, but also relational:\n\neach class has both a teacher and a set of students\neach class is held in a specific location that has certain equipment\n\nIt would be silly to store this information in a single table (though it probably can be done) because all of the teacher information would be duplicated for each student in each class; all of the student’s individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well.\nBut, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps:\n\nget the Class ID\nget any teachers that are assigned that Class ID - specifically, get their ID number\nget any students that are assigned that Class ID - specifically, get their ID number\nappend the results from teachers and students so that there is a list of all individuals in the class\nlook through the “individual data” table to find any individuals with matching ID numbers, and keep those individuals’ birth days.\n\n\n\nTable joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information.\nThere are 3 main types of table joins:\n\nFiltering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don’t change)\nEx: finding all teachers or students who have class ClassID\nSet operations, which treat observations as set elements (e.g. union, intersection, etc.)\nEx: taking the union of all student and teacher IDs to get a list of individual IDs\nMutating joins, which add columns from one table to matching rows in another table\nEx: adding birthday to the table of all individuals in a class\n\nkeys are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A primary key identifies an observation in its own table. A foreign key identifies an observation in another table.\nWe’re primarily going to focus on mutating joins, as filtering joins can be accomplished by … filtering … rather than by table joins. Feel free to read through the other types of joins here\n\n\nAnimating different types of joins\n\nNote: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.\nIf we start with two tables, x and y,\n\n\nlibrary(dplyr) # Must load this library to do these joins\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nx <- data.frame(c1 = 1:3, cx = c(\"x1\", \"x2\", \"x3\"))\ny <- data.frame(c1 = c(1, 2, 4), cy = c(\"y1\", \"y2\", \"y4\"))\n\n\nimport pandas as pd # must load pandas to do joins\nx = r.x\ny = r.y\n\nWe can do a filtering inner_join to keep only rows which are in both tables (but we keep all columns)\n\n\ninner_join(x, y)\n\nJoining, by = \"c1\"\n\n\n  c1 cx cy\n1  1 x1 y1\n2  2 x2 y2\n\n\n\npd.merge(x, y) # by default, merge uses inner join\n\n   c1  cx  cy\n0   1  x1  y1\n1   2  x2  y2\n\n\nBut what if we want to keep all of the rows in x? We would do a left_join\n\nIf there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one.\n ::: {.cell}\nleft_join(x, y)\n\nJoining, by = \"c1\"\n\n\n  c1 cx   cy\n1  1 x1   y1\n2  2 x2   y2\n3  3 x3 <NA>\n\n:::\n\npd.merge(x, y, how = \"left\")\n\n   c1  cx   cy\n0   1  x1   y1\n1   2  x2   y2\n2   3  x3  NaN\n\n\nIf we wanted to keep all of the rows in y, we would do a right_join:\n\n(or, we could do a left join with y and x, but… either way is fine).\n\nright_join(x, y)\n\nJoining, by = \"c1\"\n\n\n  c1   cx cy\n1  1   x1 y1\n2  2   x2 y2\n3  4 <NA> y4\n\n\n\npd.merge(x, y, how = \"right\")\n\n    c1   cx  cy\n0  1.0   x1  y1\n1  2.0   x2  y2\n2  4.0  NaN  y4\n\n\nAnd finally, if we want to keep all of the rows, we’d do a full_join (referred to as an “outer join” in pandas):\n\n\nfull_join(x, y)\n\nJoining, by = \"c1\"\n\n\n  c1   cx   cy\n1  1   x1   y1\n2  2   x2   y2\n3  3   x3 <NA>\n4  4 <NA>   y4\n\n\n\npd.merge(x, y, how = \"outer\")\n\n    c1   cx   cy\n0  1.0   x1   y1\n1  2.0   x2   y2\n2  3.0   x3  NaN\n3  4.0  NaN   y4\n\n\nYou can find other animations corresponding to filtering joins and set operations here\n\n\nTry it out\nRebrickable.com maintains a database of Lego sets, parts, and other data, available for download. You can download the data yourself, or you can use the tables I’ve downloaded and included: sets and themes\n\nsets <- read.csv(\"https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/sets.csv\")\nthemes <- read.csv(\"https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/themes.csv\")\n\n\nimport pandas as pd\nsets = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/sets.csv\")\nthemes = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/themes.csv\")\n\nLet’s start out by joining the two datasets together. Note that we’ll need to specify which columns to join by in both R and pandas. In R, we’ll need to use by = c(left_col = right_col) to specify the column names in the left and right data frames. In pandas, we’ll need to use arguments left_on = 'left_col' and right_on = 'right_col'.\nFirst, let’s try a full/outer join.\n\n\nR solution\n\n\nlego_fulljoin <- full_join(sets, themes, by = c(\"theme_id\" = \"id\"))\nhead(lego_fulljoin)\n\n  set_num                     name.x year theme_id num_parts       name.y\n1   001-1                      Gears 1965        1        43      Technic\n2  0011-2          Town Mini-Figures 1979       67        12 Classic Town\n3  0011-3 Castle 2 for 1 Bonus Offer 1987      199         0 Lion Knights\n4  0012-1         Space Mini-Figures 1979      143        12 Supplemental\n5  0013-1         Space Mini-Figures 1979      143        12 Supplemental\n6  0014-1         Space Mini-Figures 1979      143        12 Supplemental\n  parent_id\n1        NA\n2        50\n3       186\n4       126\n5       126\n6       126\n\n\n\n\n\nPython solution\n\n\nlego_fulljoin = pd.merge(sets, themes, left_on = \"theme_id\", right_on = \"id\", how = \"outer\")\nlego_fulljoin.head()\n\n  set_num                          name_x    year  ...  id   name_y  parent_id\n0   001-1                           Gears  1965.0  ...   1  Technic        NaN\n1   002-1  4.5V Samsonite Gears Motor Set  1965.0  ...   1  Technic        NaN\n2  1030-1  TECHNIC I: Simple Machines Set  1985.0  ...   1  Technic        NaN\n3  1038-1              ERBIE the Robo-Car  1985.0  ...   1  Technic        NaN\n4  1039-1            Manual Control Set 1  1986.0  ...   1  Technic        NaN\n\n[5 rows x 8 columns]\n\n\n\nSometimes, it’s easier to rename the columns before merging. Try that approach - if you have the same name for the columns that you intend to join on (and no other common names) then it’s easier to do the join and to understand what is happening. Try it out with the lego sets to see which approach you prefer.\n\n\nR solution\n\n\nsets_rn <- sets %>%\n  rename(set_name = name)\nthemes_rn <- themes %>%\n  rename(theme_name = name, theme_id = id, theme_parent_id = parent_id)\nlego_fulljoin <- full_join(sets_rn, themes_rn)\n\nJoining, by = \"theme_id\"\n\nhead(lego_fulljoin)\n\n  set_num                   set_name year theme_id num_parts   theme_name\n1   001-1                      Gears 1965        1        43      Technic\n2  0011-2          Town Mini-Figures 1979       67        12 Classic Town\n3  0011-3 Castle 2 for 1 Bonus Offer 1987      199         0 Lion Knights\n4  0012-1         Space Mini-Figures 1979      143        12 Supplemental\n5  0013-1         Space Mini-Figures 1979      143        12 Supplemental\n6  0014-1         Space Mini-Figures 1979      143        12 Supplemental\n  theme_parent_id\n1              NA\n2              50\n3             186\n4             126\n5             126\n6             126\n\n\n\n\n\nPython solution\n\nTo do this, I consulted stackoverflow\n\nsets_rn = sets # copy the dataset\nsets_rn = sets_rn.rename(columns = {'name':'set_name'})\nthemes_rn = themes.rename(columns = {'id': 'theme_id', 'parent_id':'theme_parent_id', 'name' :'theme_name'})\n\nlego_fulljoin = pd.merge(sets_rn, themes_rn, how = \"outer\")\nlego_fulljoin.head()\n\n  set_num                        set_name  ...  theme_name  theme_parent_id\n0   001-1                           Gears  ...     Technic              NaN\n1   002-1  4.5V Samsonite Gears Motor Set  ...     Technic              NaN\n2  1030-1  TECHNIC I: Simple Machines Set  ...     Technic              NaN\n3  1038-1              ERBIE the Robo-Car  ...     Technic              NaN\n4  1039-1            Manual Control Set 1  ...     Technic              NaN\n\n[5 rows x 7 columns]\n\n\n\nWhich type of join? In some cases, we might prefer to use a different type of join. If our goal is to add the context of theme information to the set data, we might not care about themes that don’t have corresponding sets in our data. Can you determine what type of join is appropriate here?\n\n\nR solution\n\n\nlego_data <- left_join(sets_rn, themes_rn)\n\nJoining, by = \"theme_id\"\n\nhead(lego_data)\n\n  set_num                   set_name year theme_id num_parts   theme_name\n1   001-1                      Gears 1965        1        43      Technic\n2  0011-2          Town Mini-Figures 1979       67        12 Classic Town\n3  0011-3 Castle 2 for 1 Bonus Offer 1987      199         0 Lion Knights\n4  0012-1         Space Mini-Figures 1979      143        12 Supplemental\n5  0013-1         Space Mini-Figures 1979      143        12 Supplemental\n6  0014-1         Space Mini-Figures 1979      143        12 Supplemental\n  theme_parent_id\n1              NA\n2              50\n3             186\n4             126\n5             126\n6             126\n\n\n\n\n\nPython solution\n\n\nlego_data = pd.merge(sets_rn, themes_rn, how = \"left\")\nlego_data.head()\n\n  set_num                    set_name  ...    theme_name  theme_parent_id\n0   001-1                       Gears  ...       Technic              NaN\n1  0011-2           Town Mini-Figures  ...  Classic Town             50.0\n2  0011-3  Castle 2 for 1 Bonus Offer  ...  Lion Knights            186.0\n3  0012-1          Space Mini-Figures  ...  Supplemental            126.0\n4  0013-1          Space Mini-Figures  ...  Supplemental            126.0\n\n[5 rows x 7 columns]\n\n\n\nUsing Your Data Pick a theme you’re interested in, and plot the number of pieces in the sets of that theme over time.\n\n\nR solution\n\nI want to look at Pirates sets. We can see that there are 3 generations of main “Pirates” theme sets, but there is a parent theme that contains all of them. So let’s filter the full dataset on that parent id.\n\nlibrary(ggplot2)\nthemes %>%\n  filter(grepl(\"Pirates\", name))\n\n   id                            name parent_id\n1 147                         Pirates        NA\n2 148                       Pirates I       147\n3 153                      Pirates II       147\n4 154                     Pirates III       147\n5 215                         Pirates       207\n6 263        Pirates of the Caribbean        NA\n7 638 Jake and the Never Land Pirates       504\n8 651                         Pirates       504\n\nlego_data %>%\n  filter(theme_parent_id == 147) %>%\n  ggplot(aes(x = year, y = num_parts)) + geom_jitter()\n\n\n\n\n\n\n\nPython solution\n\nIn this case, let’s look at any sets that have a theme name containing “Jurassic” (Park, World, etc.)\n\nfrom plotnine import *\ndinos = lego_data.loc[lego_data[\"theme_name\"].str.contains(\"Jurassic\")]\nggplot(dinos, aes(x = \"year\", y = \"num_parts\")) + geom_jitter()\n\n<ggplot: (8734863095263)>"
  },
  {
    "objectID": "08-data-cleaning.html#data-summaries",
    "href": "08-data-cleaning.html#data-summaries",
    "title": "8  Data Summaries and Data Cleaning",
    "section": "8.2 Data Summaries",
    "text": "We’ve talked before about using for loops to create summaries of your data, as in this example.\nIn may cases, however, it is easier to use a slightly different mechanism to work with groups of data. What do I mean by groups of data? When we used loops, the variable we “group by” is the variable controlling the loop.\n\n\nSummarizing data with Lego\n\n\n\n\nA lego “data frame” with two columns: green 1x2s and purple/pink 1x4s.\n\n\n\n\n\nWe can “group” the data frame by the different shades of 1x2 lego pieces\n\n\n\n\n\nWe can then compute a summary variable for the group, picking out e.g. the most “intense” shade in the purple/pink column.\n\n\n\n\n\nLooking only at the summary, we could then ungroup our data and return to a regular data frame… only this time, we have a summary, with one row for each different value of the green column\n\n\n\nIn R/tidyverse syntax, we would use the group_by function to group a dataframe by a variable, and then we would use mutate or summarize to create our new column(s). mutate would be used if we want to have the same number of rows in our output as we had in the input, while summarize would create one row per group.\nIn python syntax, we use groupby to group the DataFrame by a variable, and then we use .agg to aggregate. The function pd.NamedAgg(column, function) allows us to explicitly state that we want to use function function on column column, and assign that result to a new variable.\n\nSuppose we want to summarize the lego set data by year, computing the number of sets and the mean number of pieces per set. We’ll take the data set we generate and plot the number of pieces, with point size scaled to show the number of sets released that year.\n\nsets %>%\n  group_by(year) %>%\n  summarize(n = n(), mean_pieces = mean(num_parts)) %>%\n  ggplot(aes(x = year, y = mean_pieces, size = n)) + geom_point()\n\n\n\n\n\ntmp = sets.groupby(\"year\", as_index = False).agg(\n  mean_pieces = pd.NamedAgg('num_parts', 'mean'),\n  n = pd.NamedAgg('num_parts', 'count'))\nggplot(tmp, aes(x = \"year\", y = \"mean_pieces\", size = \"n\")) + geom_point()\n\n<ggplot: (8734847067167)>"
  },
  {
    "objectID": "08-data-cleaning.html#working-with-text",
    "href": "08-data-cleaning.html#working-with-text",
    "title": "8  Data Summaries and Data Cleaning",
    "section": "8.3 Working with Text",
    "text": "Nearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different “levels” of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated).\n\nSome people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski\n\n\n\n\nAlternately, the xkcd version of the above quote\n\n\n\n8.3.1 Basic String Operations\nThe tidyverse package to deal with strings is stringr. The functions in stringr take the form of str_XXX where XXX is a verb. So str_split(), str_replace(), str_remove(), str_to_lower() all should make some sense.\nThe corresponding python library is re, short for regular expression. Pandas also includes some functionality from this package in (partially) vectorized form.\n\nFor this example, we’ll use a subset of the US Department of Education College Scorecard data. Documentation, Data. I’ve selected a few columns from the institution-level data available on the College Scorecard site.\n\n\nLet’s take a look (Read in the data)\n\n\nlibrary(readr)\ncollege <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/College_Data_Abbrev.csv\", guess_max = 5000, na = '.')\n\n`curl` package not installed, falling back to using `url()`\nRows: 6806 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (14): INSTNM, CITY, STABBR, ZIP, ACCREDAGENCY, INSTURL, PREDDEG, MAIN, H...\ndbl  (3): UNITID, NUMBRANCH, ST_FIPS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(college)\n\nspec_tbl_df [6,806 × 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ UNITID      : num [1:6806] 100654 100663 100690 100706 100724 ...\n $ INSTNM      : chr [1:6806] \"Alabama A & M University\" \"University of Alabama at Birmingham\" \"Amridge University\" \"University of Alabama in Huntsville\" ...\n $ CITY        : chr [1:6806] \"Normal\" \"Birmingham\" \"Montgomery\" \"Huntsville\" ...\n $ STABBR      : chr [1:6806] \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ ZIP         : chr [1:6806] \"35762\" \"35294-0110\" \"36117-3553\" \"35899\" ...\n $ ACCREDAGENCY: chr [1:6806] \"Southern Association of Colleges and Schools Commission on Colleges\" \"Southern Association of Colleges and Schools Commission on Colleges\" \"Southern Association of Colleges and Schools Commission on Colleges\" \"Southern Association of Colleges and Schools Commission on Colleges\" ...\n $ INSTURL     : chr [1:6806] \"www.aamu.edu/\" \"https://www.uab.edu\" \"www.amridgeuniversity.edu\" \"www.uah.edu\" ...\n $ PREDDEG     : chr [1:6806] \"Predominantly bachelor's-degree granting\" \"Predominantly bachelor's-degree granting\" \"Predominantly bachelor's-degree granting\" \"Predominantly bachelor's-degree granting\" ...\n $ MAIN        : chr [1:6806] \"main campus\" \"main campus\" \"main campus\" \"main campus\" ...\n $ NUMBRANCH   : num [1:6806] 1 1 1 1 1 1 1 1 1 1 ...\n $ HIGHDEG     : chr [1:6806] \"Graduate\" \"Graduate\" \"Graduate\" \"Graduate\" ...\n $ CONTROL     : chr [1:6806] \"Public\" \"Public\" \"Private Non Profit\" \"Public\" ...\n $ ST_FIPS     : num [1:6806] 1 1 1 1 1 1 1 1 1 1 ...\n $ LOCALE      : chr [1:6806] \"12\" \"12\" \"12\" \"12\" ...\n $ LATITUDE    : chr [1:6806] \"34.783368\" \"33.505697\" \"32.362609\" \"34.724557\" ...\n $ LONGITUDE   : chr [1:6806] \"-86.568502\" \"-86.799345\" \"-86.17401\" \"-86.640449\" ...\n $ State       : chr [1:6806] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   UNITID = col_double(),\n  ..   INSTNM = col_character(),\n  ..   CITY = col_character(),\n  ..   STABBR = col_character(),\n  ..   ZIP = col_character(),\n  ..   ACCREDAGENCY = col_character(),\n  ..   INSTURL = col_character(),\n  ..   PREDDEG = col_character(),\n  ..   MAIN = col_character(),\n  ..   NUMBRANCH = col_double(),\n  ..   HIGHDEG = col_character(),\n  ..   CONTROL = col_character(),\n  ..   ST_FIPS = col_double(),\n  ..   LOCALE = col_character(),\n  ..   LATITUDE = col_character(),\n  ..   LONGITUDE = col_character(),\n  ..   State = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\n\nimport pandas as pd\ncollege = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/College_Data_Abbrev.csv\", na_values = '.')\n\n\n\n\nWhat proportion of the schools operating in each state have the state’s name in the school name?\n\nWe’ll use str_detect() to look for the state name in the college name.\n\nlibrary(stringr) # string processing\n\n# Outside the pipe\nstr_detect(college$INSTNM, pattern = college$State)[1:10]\n\n [1]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n# Using the pipe and mutate:\ncollege <- college %>%\n  mutate(uses_st_name = str_detect(INSTNM, State))\n\nlibrary(ggplot2) # graphs and charts\n# By state - percentage of institution names\ncollege %>%\n  group_by(State) %>%\n  summarize(pct_uses_st_name = mean(uses_st_name), n = n()) %>%\n  filter(n > 5) %>% # only states/territories with at least 5 schools\n  # Reorder state factor level by percentage that uses state name\n  mutate(State = reorder(State, -pct_uses_st_name)) %>%\n  ggplot(data = ., aes(x = State, y = pct_uses_st_name)) + \n  geom_col() + coord_flip() + \n  geom_text(aes(y = 1, label = paste(\"Total Schools:\", n)), hjust = 1)\n\n\n\n\nThis example turned out to be way more complicated in Python than I was anticipating, mostly because unlike R, python string operations aren’t vectorized over both the string and the pattern you’re searching for. So this example uses a few tricks (like apply + lambda functions) that we haven’t talked about yet.\n\nimport re # regular expressions\n\n# This doesn't work because str.contains doesn't take a vector of patterns\n# college[\"INSTNM\"].str.contains(college[\"State\"])[1:10]\n\n# This is a function that we create\n# We'll cover functions in the next chapter\n# but for now, I've used this so that the code is a little more readable...\n\ndef str_detect(x, y):\n  # Ensure x and y aren't null/NaN\n  null_vals = pd.isna(x) or pd.isna(y)\n  # If they aren't null, then search x for pattern y and return the result\n  if not null_vals:\n    return bool(re.search(y, x))\n  # If there are null/na values, return False\n  else:\n    return False\n\n# We then create a new variable by using our function on each row individually\ncollege = college.assign(uses_st_name = college.apply(lambda row: str_detect(row.INSTNM, row.State), axis = 1))\n\n# Then we aggregate\ncollege_names = college.groupby(\"State\", as_index = False).agg(\n  pct_uses_st_name = pd.NamedAgg('uses_st_name', 'mean'),\n  n = pd.NamedAgg('uses_st_name', 'count')\n)\n\n# Sorting by percent using state name\ncollege_names = college_names.loc[college_names.n > 5].\\\n  sort_values('pct_uses_st_name', axis = 0)\n\n# Creating a label variable\ncollege_names['label'] = \"Total Schools: \" + college_names['n'].astype(str)\n\n# Sorting states and enforcing that order - like making a factor in R\nstate_list = college_names.State.unique().tolist()\ncollege_names.State = pd.Categorical(college_names.State, categories = state_list)\n\nfrom plotnine import * # graphs and charts\n# By state - percentage of institution names\nggplot(data = college_names) + \\\ngeom_text(aes(x = \"State\", y = 1, label = 'label'), ha='right') + \\\ngeom_col(aes(x = \"State\", y = \"pct_uses_st_name\")) + coord_flip()\n\n<ggplot: (8734846979404)>\n\n\n\n\n\n\n\nI’m not going to get into regular expressions in this class, but if you do want more power to understand how to work with strings, that’s an excellent skill to pick up.\n\n\n8.3.2 Joining and Splitting Variables\nThere’s another string-related task that is fairly commonly encountered: separating variables into two different columns (as in Table 3 in the previous chapter).\n\n\n\n\n\nA visual representation of what separating variables means for data set operations.\n\n\n\n\n\n\nSeparating Variables in R\n\nWe can use str_extract() if we want, but it’s actually faster to use separate(), which is part of the tidyr package. There is also extract(), which is another tidyr function that uses regular expressions and capture groups to split variables up.\n\nlibrary(dplyr)\nlibrary(tidyr)\n\ntable3 %>%\n  separate(col = rate, into = c(\"cases\", \"population\"), sep = \"/\", remove = F)\n\n# A tibble: 6 × 5\n  country      year rate              cases  population\n  <chr>       <int> <chr>             <chr>  <chr>     \n1 Afghanistan  1999 745/19987071      745    19987071  \n2 Afghanistan  2000 2666/20595360     2666   20595360  \n3 Brazil       1999 37737/172006362   37737  172006362 \n4 Brazil       2000 80488/174504898   80488  174504898 \n5 China        1999 212258/1272915272 212258 1272915272\n6 China        2000 213766/1280428583 213766 1280428583\n\n\nI’ve left the rate column in the original data frame just to make it easy to compare and verify that yes, it worked.\nseparate() will also take a full on regular expression if you want to capture only parts of a string to put into new columns.\n\n\n\nSeparating Variables in python\n\nIn python, we can do a similar observation, but one convention of python that is very useful here is that we can do a multiple-assign on the left hand side.\n\ntable3 = r.table3\n\ntable3[[\"cases\", \"population\"]] = table3[\"rate\"].str.split(\"/\", expand = True)\ntable3\n\n       country  year               rate   cases  population\n0  Afghanistan  1999       745/19987071     745    19987071\n1  Afghanistan  2000      2666/20595360    2666    20595360\n2       Brazil  1999    37737/172006362   37737   172006362\n3       Brazil  2000    80488/174504898   80488   174504898\n4        China  1999  212258/1272915272  212258  1272915272\n5        China  2000  213766/1280428583  213766  1280428583\n\n\nWe use col.str.split() to split the column, expand = True indicates that we want separate columns, and then by including two things on the left hand side, we can store each column into its own new value.\n\nAnd, of course, there is a complementary operation, which is when it’s necessary to join two columns to get a useable data value.\n\n\n\n\n\nA visual representation of what uniting variables means for data set operations.\n\n\n\n\n\n\nJoining Variables in R\n\nseparate() has a complement, unite(), which is useful for handling situations like in table5:\n\ntable5 %>%\n  unite(col = \"year\", century:year, sep = '') %>%\n  separate(col = rate, into = c(\"cases\", \"population\"), sep = \"/\")\n\n# A tibble: 6 × 4\n  country     year  cases  population\n  <chr>       <chr> <chr>  <chr>     \n1 Afghanistan 1999  745    19987071  \n2 Afghanistan 2000  2666   20595360  \n3 Brazil      1999  37737  172006362 \n4 Brazil      2000  80488  174504898 \n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\nNote that separate and unite both work with character variables - it’s not necessarily true that you’ll always be working with character formats when you need to do these operations. For instance, it’s relatively common to need to separate dates into year, month, and day as separate columns (or to join them together).\nOf course, it’s much easier just to do a similar two-step operation (we have to convert to numeric variables to do math)\n\ntable5 %>%\n  mutate(year = as.numeric(century)*100 + as.numeric(year)) %>% \n  select(-century)\n\n# A tibble: 6 × 3\n  country      year rate             \n  <chr>       <dbl> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n(Handy shortcut functions in dplyr don’t completely remove the need to think).\n\n\n\nJoining Variables in python\n\n\nr.table5.assign(year = r.table5.century + r.table5.year) # String concatenation\n\n       country century  year               rate\n0  Afghanistan      19  1999       745/19987071\n1  Afghanistan      20  2000      2666/20595360\n2       Brazil      19  1999    37737/172006362\n3       Brazil      20  2000    80488/174504898\n4        China      19  1999  212258/1272915272\n5        China      20  2000  213766/1280428583\n\n\nIn python, we can join character values using +, so it’s an even simpler process. Of course, as in R, it may be better to do a two-step operation to convert to numeric variables. Unlike in R, you can do the string concatenation process first and then convert to numeric variables without having to think too much.\n\ntmp = r.table5.assign(year = r.table5.century + r.table5.year)\n\ntmp.\\\nassign(year = tmp.year.astype(int)).\\\ndrop(\"century\", axis = 1)\n\n       country  year               rate\n0  Afghanistan  1999       745/19987071\n1  Afghanistan  2000      2666/20595360\n2       Brazil  1999    37737/172006362\n3       Brazil  2000    80488/174504898\n4        China  1999  212258/1272915272\n5        China  2000  213766/1280428583\n\n\n\n\n\nAdditional String Manipulation Information\nString manipulation\n\nR4DS chapter - strings\nPCRE tester\nR regex tester - has a short timeout period and will disconnect you if you’re idle too long. But you can also clone the repo here and run it locally."
  },
  {
    "objectID": "09-functions.html#when-to-write-a-function",
    "href": "09-functions.html#when-to-write-a-function",
    "title": "9  Functions",
    "section": "9.1 When to write a function?",
    "text": "If you’ve written the same code (with a few minor changes, like variable names) more than twice, you should probably write a function instead. There are a few benefits to this rule:\n\nYour code stays neater (and shorter), so it is easier to read, understand, and maintain.\nIf you need to fix the code because of errors, you only have to do it in one place.\nYou can re-use code in other files by keeping functions you need regularly in a file (or if you’re really awesome, in your own package!)\nIf you name your functions well, your code becomes easier to understand thanks to grouping a set of actions under a descriptive function name.\n\n\nThere is some extensive material on this subject in R for Data Science on functions. If you want to really understand how functions work in R, that is a good place to go.\n\n\n\nThis example is modified from R for Data Science (Wickham and Grolemund 2016, chap. 19).\nWhat does this code do? Does it work as intended?\n\nRPython\n\n\n\ndf <- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b <- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c <- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d <- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n  'a': np.random.randn(10), \n  'b': np.random.randn(10), \n  'c': np.random.randn(10), \n  'd': np.random.randn(10)})\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\ndf.b = (df.b - min(df.b))/(max(df.b) - min(df.a))\ndf.c = (df.c - min(df.c))/(max(df.c) - min(df.c))\ndf.d = (df.d - min(df.d))/(max(df.d) - min(df.d))\n\n\n\n\nThe code rescales a set of variables to have a range from 0 to 1. But, because of the copy-pasting, the code’s author made a mistake and forgot to change an a to b.\nWriting a function to rescale a variable would prevent this type of copy-paste error.\nTo write a function, we first analyze the code to determine how many inputs it has\n\nRPython\n\n\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\nThis code has only one input: df$a.\n\n\n\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\n\nThis code has only one input: df.a\n\n\n\nTo convert the code into a function, we first rewrite it using general names\n\nRPython\n\n\nIn this case, it might help to replace df$a with x.\n\nx <- df$a \n\n(x - min(x, na.rm = TRUE)) / \n  (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n\n [1] 0.0000000 0.5887631 1.0000000 0.4426543 0.7201530 0.3618722 0.1179800\n [8] 0.3547786 0.7780236 0.5820836\n\n\n\n\nIn this case, it might help to replace df.a with x.\n\nx = df.a\n\n(x - min(x))/(max(x) - min(x))\n\n0    0.123945\n1    0.250606\n2    0.525586\n3    0.287812\n4    0.000000\n5    0.106069\n6    0.336325\n7    1.000000\n8    0.491033\n9    0.146617\nName: a, dtype: float64\n\n\n\n\n\nThen, we make it a bit easier to read, removing duplicate computations if possible (for instance, computing min two times).\n\nRPython\n\n\nIn R, we can use the range function, which computes the maximum and minimum at the same time and returns the result as c(min, max)\n\nrng <- range(x, na.rm = T)\n\n(x - rng[1])/(rng[2] - rng[1])\n\n [1] 0.0000000 0.5887631 1.0000000 0.4426543 0.7201530 0.3618722 0.1179800\n [8] 0.3547786 0.7780236 0.5820836\n\n\n\n\nIn python, range is the equivalent of seq() in R, so we are better off just using min and max.\n\nx = df.a\n\n\nxmin, xmax = [x.min(), x.max()]\n(x - xmin)/(xmax - xmin)\n\n0    0.123945\n1    0.250606\n2    0.525586\n3    0.287812\n4    0.000000\n5    0.106069\n6    0.336325\n7    1.000000\n8    0.491033\n9    0.146617\nName: a, dtype: float64\n\n\n\n\n\nFinally, we turn this code into a function\n\nRPython\n\n\n\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])\n}\n\nrescale01(df$a)\n\n [1] 0.0000000 0.5887631 1.0000000 0.4426543 0.7201530 0.3618722 0.1179800\n [8] 0.3547786 0.7780236 0.5820836\n\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df$a, df$b, df$c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, between { and } (this is true in R, in python, there are different conventions, but the same principle applies)\nThe function returns the last value computed: in this case, (x - rng[1])/(rng[2]-rng[1]). You can make this explicit by adding a return() statement around that calculation.\n\n\n\n\ndef rescale01(x):\n  xmin, xmax = [x.min(), x.max()]\n  return (x - xmin)/(xmax - xmin)\n\nrescale01(df.a)\n\n0    0.123945\n1    0.250606\n2    0.525586\n3    0.287812\n4    0.000000\n5    0.106069\n6    0.336325\n7    1.000000\n8    0.491033\n9    0.146617\nName: a, dtype: float64\n\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df.a, df.b, df.c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, indented relative to the line with def: function_name():. At the end of the function, you should have a blank line with no spaces or tabs.\nThe function returns the value it is told to return: in this case, (x - xmin)/(xmax - xmin). In Python, you must return a value if you want the function to perform a computation. 1\n\n\n\n\nThe process for creating a function is important: first, you figure out how to do the thing you want to do. Then, you simplify the code as much as possible. Only at the end of that process do you create an actual function."
  },
  {
    "objectID": "09-functions.html#syntax",
    "href": "09-functions.html#syntax",
    "title": "9  Functions",
    "section": "9.2 Syntax",
    "text": "R and python syntax for defining functions. Portions of the command that indicate the function name, function scope, and return statement are highlighted in each block.\n\n\nIn R, functions are defined as other variables, using <-, but we specify the arguments a function takes by using the function() statement. The contents of the function are contained within { and }. If the function returns a value, a return() statement can be used; alternately, if there is no return statement, the last computation in the function will be returned.\nIn python, functions are defined using the def command, with the function name, parentheses, and the function arguments to follow. The first line of the function definition ends with a :, and all subsequent lines of the function are indented (this is how python knows where the end of the function is). A python function return statement is return <value>, with no parentheses needed.\nNote that in python, the return statement is not optional. It is not uncommon to have python functions that don’t return anything; in R, this is a bit less common, for reasons we won’t get into here."
  },
  {
    "objectID": "09-functions.html#arguments-and-parameters",
    "href": "09-functions.html#arguments-and-parameters",
    "title": "9  Functions",
    "section": "9.3 Arguments and Parameters",
    "text": "An argument is the name for the object you pass into a function.\nA parameter is the name for the object once it is inside the function (or the name of the thing as defined in the function).\n\nLet’s examine the difference between arguments and parameters by writing a function that takes a puppy’s name and returns “ is a good pup!”.\n\nRPython\n\n\n\ndog <- \"Eddie\"\n\ngoodpup <- function(name) {\n  paste(name, \"is a good pup!\")\n}\n\ngoodpup(dog)\n\n[1] \"Eddie is a good pup!\"\n\n\nIn this example R function, when we call goodpup(dog), dog is the argument. name is the parameter.\n\n\n\ndog = \"Eddie\"\n\ndef goodpup(name):\n  return name + \" is a good pup!\"\n\ngoodpup(dog)\n\n'Eddie is a good pup!'\n\n\nIn this example python function, when we call goodpup(dog), dog is the argument. name is the parameter.\n\n\n\nWhat is happening inside the computer’s memory as goodpup runs?\n\n\n\nA sketch of the execution of the program goodpup, showing that name is only defined within the local environment that is created while goodpup is running. We can never access name in our global environment.\n\n\n\nThis is why the distinction between arguments and parameters matters. Parameters are only accessible while inside of the function - and in that local environment, we need to call the object by the parameter name, not the name we use outside the function (the argument name).\nWe can even call a function with an argument that isn’t defined outside of the function call: goodpup(\"Tesla\") produces “Tesla is a good pup!”. Here, I do not have a variable storing the string “Tesla”, but I can make the function run anyways. So “Tesla” here is an argument to goodpup but it is not a variable in my environment.\nThis is a confusing set of concepts and it’s ok if you only just sort of get what I’m trying to explain here. Hopefully it will become more clear as you write more code.\n\nTry it out\nFor each of the following blocks of code, identify the function name, function arguments, parameter names, and return statements. When the function is called, see if you can predict what the output will be.\n\nFunction 1Answer\n\n\n\n\ndef hello_world():\n  print(\"Hello World\")\n\n\nhello_world()\n\n\n\n\nFunction name: hello_world\nFunction parameters: none\nFunction arguments: none\nFunction output:\n\n\nhello_world()\n\nHello World\n\n\n\n\n\n\nFunction 2Answer\n\n\n\nmy_mean <- function(x) {\n  censor_x <- sample(x, size = length(x) - 2, replace = F)\n  mean(censor_x)\n}\n\n\nset.seed(3420523)\nmy_mean(1:10)\n\n\n\n\nFunction name: my_mean\nFunction parameters: x\nFunction arguments: 1:10\nFunction output: (varies each time the function is run unless you set the seed)\n\n\nset.seed(3420523)\nmy_mean(1:10)\n\n[1] 6\n\n\n\n\n\n\n\n9.3.1 Named Arguments and Parameter Order\nIn the examples above, you didn’t have to worry about what order parameters were passed into the function, because there were 0 and 1 parameters, respectively. But what happens when we have a function with multiple parameters?\n\nRPython\n\n\n\ndivide <- function(x, y) {\n  x / y\n}\n\n\n\n\n\ndef divide(x, y):\n  return x / y\n\n\n\n\nIn this function, the order of the parameters matters! divide(3, 6) does not produce the same result as divide(6, 3). As you might imagine, this can quickly get confusing as the number of parameters in the function increases.\nIn this case, it can be simpler to use the parameter names when you pass in arguments.\n\nRPython\n\n\n\ndivide(3, 6)\n\n[1] 0.5\n\ndivide(x = 3, y = 6)\n\n[1] 0.5\n\ndivide(y = 6, x = 3)\n\n[1] 0.5\n\ndivide(6, 3)\n\n[1] 2\n\ndivide(x = 6, y = 3)\n\n[1] 2\n\ndivide(y = 3, x = 6)\n\n[1] 2\n\n\n\n\n\ndivide(3, 6)\n\n0.5\n\ndivide(x = 3, y = 6)\n\n0.5\n\ndivide(y = 6, x = 3)\n\n0.5\n\ndivide(6, 3)\n\n2.0\n\ndivide(x = 6, y = 3)\n\n2.0\n\ndivide(y = 3, x = 6)\n\n2.0\n\n\n\n\n\nAs you can see, the order of the arguments doesn’t much matter, as long as you use named arguments, but if you don’t name your arguments, the order very much matters."
  },
  {
    "objectID": "09-functions.html#input-validation",
    "href": "09-functions.html#input-validation",
    "title": "9  Functions",
    "section": "9.4 Input Validation",
    "text": "When you write a function, you often assume that your parameters will be of a certain type. But you can’t guarantee that the person using your function knows that they need a certain type of input. In these cases, it’s best to validate your function input.\n\n\nRPython\n\n\nIn R, you can use stopifnot() to check for certain essential conditions. If you want to provide a more illuminating error message, you can check your conditions using if() and then use stop(\"better error message\") in the body of the if statement.\n\nadd <- function(x, y) {\n  x + y\n}\n\nadd(\"tmp\", 3)\n\nError in x + y: non-numeric argument to binary operator\n\nadd <- function(x, y) {\n  stopifnot(is.numeric(x))\n  stopifnot(is.numeric(y))\n  x + y\n}\n\nadd(\"tmp\", 3)\n\nError in add(\"tmp\", 3): is.numeric(x) is not TRUE\n\nadd(3, 4)\n\n[1] 7\n\n\n\n\nIn Python, the easiest way to handle errors is to use a try statement, which operates rather like an if statement: if the statement executes, then we’re good to go; if not, we can use except to handle different types of errors. The else clause is there to handle anything that needs to happen if the statement in the try clause executes without any errors.\n\n\ndef add(x, y):\n  x + y\n\nadd(\"tmp\", 3)\n\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: can only concatenate str (not \"int\") to str\n\nDetailed traceback:\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 2, in add\n\ndef add(x, y):\n  try:\n    return x + y\n  except TypeError:\n    print(\"x and y must be add-able\")\n  else:\n    # We should never get here, because the try clause has a return statement\n    print(\"Else clause?\")\n  return\n\nadd(\"tmp\", 3)\n\nx and y must be add-able\n\nadd(3, 4)\n\n7\n\n\nYou can read more about error handling in Python here\n\n\n\n\nInput validation is one aspect of defensive programming - programming in such a way that you try to ensure that your programs don’t error out due to unexpected bugs by anticipating ways your programs might be misunderstood or misused. If you’re interested, Wikipedia has more about defensive programming."
  },
  {
    "objectID": "09-functions.html#scope",
    "href": "09-functions.html#scope",
    "title": "9  Functions",
    "section": "9.5 Scope",
    "text": "When talking about functions, for the first time we start to confront a critical concept in programming, which is scope. Scope is the part of the program where the name you’ve given a variable is valid - that is, where you can use a variable.\n\nA variable is only available from inside the region it is created.\n\nWhat do I mean by the part of a program? The lexical scope is the portion of the code (the set of lines of code) where the name is valid.\nThe concept of scope is best demonstrated through a series of examples, so in the rest of this section, I’ll show you some examples of how scope works and the concepts that help you figure out what “scope” actually means in practice.\n\n9.5.1 Name Masking\nScope is most clearly demonstrated when we use the same variable name inside and outside a function. Note that this is 1) bad programming practice, and 2) fairly easily avoided if you can make your names even slightly more creative than a, b, and so on. But, for the purposes of demonstration, I hope you’ll forgive my lack of creativity in this area so that you can see how name masking works.\n\nWhat does this function return, 10 or 20?\n\nPseudocodeSketchRPython\n\n\na = 10\n\nmyfun = function() {\n  a = 20\n  return a\n}\n\nmyfun()\n\n\n\n\n\nA sketch of the global environment as well as the environment within myfun(). Because a=20 inside myfun(), when we call myfun(), we get the value of a within that environment, instead of within the global environment.\n\n\n\n\n\na <- 10\n\nmyfun <- function() {\n  a <- 20\n  a\n}\n\nmyfun()\n\n[1] 20\n\n\n\n\n\n\na = 10\n\ndef myfun():\n  a = 20\n  return a\n\nmyfun()\n\n20\n\n\n\n\n\n\nThe lexical scope of the function is the area that is between the braces (in R) or the indented region (in python). Outside the function, a has the value of 10, but inside the function, a has the value of 20. So when we call myfun(), we get 20, because the scope of myfun is the local context where a is evaluated, and the value of a in that environment dominates.\nThis is an example of name masking, where names defined inside of a function mask names defined outside of a function.\n\n\n9.5.2 Environments and Scope\nAnother principle of scoping is that if you call a function and then call the same function again, the function’s environment is re-created each time. Each function call is unrelated to the next function call when the function is defined using local variables.\n\n\nPseudocodeSketchRPython\n\n\nmyfun = function() {\n  if a is not defined\n    a = 1\n  else\n    a = a + 1\n}\n\nmyfun()\nmyfun()\n\nWhat does this output?\n\n\n\n\n\nWhen we define myfun, we create a template for an environment with variables and code to excecute. Each time myfun() is called, that template is used to create a new environment. This prevents successive calls to myfun() from affecting each other – which means a = 1 every time.\n\n\n\n\n\nmyfun <- function() {\n  if (!exists(\"aa\")) {\n    aa <- 1\n  } else {\n    aa <- aa + 1\n  }\n  return(aa)\n}\n\nmyfun()\n\n[1] 1\n\nmyfun()\n\n[1] 1\n\n\n\n\n\ndef myfun():\n  try: aa\n  except NameError: aa = 1\n  else: aa = aa + 1\n  return aa\n\nmyfun()\n\n1\n\nmyfun()\n\n1\n\n\nNote that the try command here is used to handle the case where a doesn’t exist. If there is a NameError (which will happen if aa is not defined) then we define aa = 1, if there is not a NameError, then aa = aa + 1.\nThis is necessary because Python does not have a built-in way to test if a variable exists before it is used.\n\n\n\n\n\n\n9.5.3 Dynamic Lookup\nScoping determines where to look for values – when, however, is determined by the sequence of steps in the code. When a function is called, the calling environment (the global environment or set of environments at the time the function is called) determines what values are used.\nIf an object doesn’t exist in the function’s environment, the global environment will be searched next; if there is no object in the global environment, the program will error out. This behavior, combined with changes in the calling environment over time, can mean that the output of a function can change based on objects outside of the function.\n\n\nPseudocodeSketchRPython\n\n\nmyfun = function() x + 1\n\nx = 14\n\nmyfun()\n\nx = 20\n\nmyfun()\n\nWhat will the output be of this code?\n\n\n\n\n\nThe state of the global environment at the time the function is called (that is, the state of the calling environment) can change the results of the function\n\n\n\n\n\nmyfun <- function() {\n  x + 1\n}\n\nx <- 14\n\nmyfun()\n\n[1] 15\n\nx <- 20\n\nmyfun()\n\n[1] 21\n\n\n\n\n\n\ndef myfun():\n  return x + 1\n\n\nx = 14\n\nmyfun()\n\n15\n\nx = 20\n\nmyfun()\n\n21\n\n\n\n\n\n\n\n\n9.5.4 Try It Out\nWhat does the following function return? Make a prediction, then run the code yourself. (Taken from (Wickham 2015, chap. 6))\n\nR codeR solutionPython codePython solution\n\n\n\nf <- function(x) {\n  f <- function(x) {\n    f <- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n\n\n\nf <- function(x) {\n  f <- function(x) {\n    f <- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n[1] 202\n\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ^ 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ** 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n\n202"
  },
  {
    "objectID": "09-functions.html#debugging",
    "href": "09-functions.html#debugging",
    "title": "9  Functions",
    "section": "9.6 Debugging",
    "text": "Now that you’re writing functions, it’s time to talk a bit about debugging techniques. This is a lifelong topic - as you become a more advanced programmer, you will need to develop more advanced debugging skills as well (because you’ll become more adept at screwing things up).\n\n\n\nThe faces of debugging (by Allison Horst)\n\n\nLet’s start with the basics: print debugging.\n\n9.6.1 Print Debugging\nThis technique is basically exactly what it sounds like. You insert a ton of print statements to give you an idea of what is happening at each step of the function.\nLet’s try it out on the previous try it out example (see what I did there?)\n\nNote that I’ve modified the code slightly so that we store the value into retval and then return it later - this allows us to see the code execution without calling functions twice (which would make the print output a bit more confusing).\n\nRPython\n\n\n\nf <- function(x) {\n  print (\"Entering Outer Function\")\n  print (sprintf(\"x = %0.2f\", x))\n  f <- function(x) {\n    print (\"Entering Middle Function\")\n    print (sprintf(\"x = %0.2f\", x))\n    f <- function() {\n      print (\"Entering Inner Function\")\n      print (sprintf(\"x = %0.2f\", x))\n      print (sprintf(\"Inner Function: Returning %0.2f\", x^2))\n      x ^ 2\n    }\n    retval <- f() + 1\n    print (sprintf(\"Middle Function: Returning %0.2f\", retval))\n    retval\n  }\n  retval <- f(x) * 2\n  print (sprintf(\"Outer Function: Returning %0.2f\", retval))\n  retval\n}\nf(10)\n\n[1] \"Entering Outer Function\"\n[1] \"x = 10.00\"\n[1] \"Entering Middle Function\"\n[1] \"x = 10.00\"\n[1] \"Entering Inner Function\"\n[1] \"x = 10.00\"\n[1] \"Inner Function: Returning 100.00\"\n[1] \"Middle Function: Returning 101.00\"\n[1] \"Outer Function: Returning 202.00\"\n\n\n[1] 202\n\n\n\n\n\ndef f(x):\n  print(\"Entering Outer function, x = \" + str(x))\n  def f(x):\n    print(\"Entering Middle function, x = \" + str(x))\n    def f():\n      print(\"Entering Inner function, x = \" + str(x))\n      print(\"Inner Function: Returning \" + str(x ** 2))\n      return x ** 2\n    \n    retval = f() + 1\n    print(\"Middle Function: Returning \" + str(retval))\n    return retval\n  \n  retval = f(x) * 2\n  print(\"Outer Function: Returning \" + str(retval))\n  return retval\n\nf(10)\n\nEntering Outer function, x = 10\nEntering Middle function, x = 10\nEntering Inner function, x = 10\nInner Function: Returning 100\nMiddle Function: Returning 101\nOuter Function: Returning 202\n202\n\n\nNote that Python has a logging module that may make it easier to print out things (and to filter the debug print statements from other reasonable program output.) You can see an example of this on Rosetta Code.\n\n\n\n\n\n\n9.6.2 General Debugging Strategies\n\nDebugging: Being the detective in a crime movie where you are also the murderer. - some t-shirt I saw once\n\nThe overall process is well described in Advanced R by H. Wickham2; I’ve copied it here because it’s such a succinct distillation of the process, but I’ve adapted some of the explanations to this class rather than the original context of package development.\n\nRealize that you have a bug\nGoogle! In R you can automate this with the errorist and searcher packages, but general Googling the error + the programming language + any packages you think are causing the issue is a good strategy.\nMake the error repeatable: This makes it easier to figure out what the error is, faster to iterate, and easier to ask for help.\n\nUse binary search (remove 1/2 of the code, see if the error occurs, if not go to the other 1/2 of the code. Repeat until you’ve isolated the error.)\nGenerate the error faster - use a minimal test dataset, if possible, so that you can ask for help easily and run code faster. This is worth the investment if you’ve been debugging the same error for a while.\nNote which inputs don’t generate the bug – this negative “data” is helpful when asking for help.\n\nFigure out where it is. Debuggers may help with this, but you can also use the scientific method to explore the code, or the tried-and-true method of using lots of print() statements.\nFix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation.\n\nThere are several other general strategies for debugging:\n\nRetype (from scratch) your code\nThis works well if it’s a short function or a couple of lines of code, but it’s less useful if you have a big script full of code to debug. However, it does sometimes fix really silly typos that are hard to spot, like having typed <-- instead of <- in R and then wondering why your answers are negative.\nVisualize your data as it moves through the program. This may be done using print() statements, or the debugger, or some other strategy depending on your application.\nTracing statements. Again, this is part of print() debugging, but these messages indicate progress - “got into function x”, “returning from function y”, and so on.\nRubber ducking. Have you ever tried to explain a problem you’re having to someone else, only to have a moment of insight and “oh, never mind”? Rubber ducking outsources the problem to a nonjudgmental entity, such as a rubber duck3. You simply explain, in terms simple enough for your rubber duck to understand, exactly what your code does, line by line, until you’ve found the problem. A more thorough explanation can be found at gitduck.com.\n\n\n\n\nYou may find it helpful to procure a rubber duck expert for each language you work in. I use color-your-own rubber ducks to endow my ducks with certain language expertise. Other people use plain rubber ducks and give them capes.\n\n\nDo not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s well-known enough that it has its own t-shirt, in addition to an xkcd comic. At some point, getting up and going for a walk may help. Redesigning your code to be more modular and more organized is also a good idea.\n\nThese next two sections are included as FYI, but you don’t have to read them just now. They’re important, but not urgent, if that makes sense.\n\n\n\n9.6.3 Dividing Problems into Smaller Parts\n\n“Divide each difficulty into as many parts as is feasible and necessary to resolve it.” -René Descartes, Discourse on Method\n\nIn programming, as in life, big, general problems are very hard to solve effectively. Instead, the goal is to break a problem down into smaller pieces that may actually be solveable.\nWe’ll start with a non-programming example:\n\n\nGeneral problem statement : “I’m exhausted all the time”\nOk, so this is a problem that many of us have from time to time (or all the time). If we get a little bit more specific at outlining the problem, though, we can sometimes get a bit more insight into how to solve it.\nSpecific problem statement: “I wake up in the morning and I don’t have any energy to do anything. I want to go back to sleep, but I have too much to do to actually give in and sleep. I spend my days worrying about how I’m going to get all of the things on my to-do list done, and then I lie awake at night thinking about how many things there are to do tomorrow. I don’t have time for hobbies or exercise, so I drink a lot of coffee instead to make it through the day.”\nThis is a much more specific list of issues, and some of these issues are actually things we can approach separately.\nSeparating things into solvable problems:\nMoving through the list above, we can isolate a few issues. Some of these issues are undoubtedly related to each other, but we can approach them separately (for the most part).\n\nPoor quality sleep (tired in the morning, lying awake at night)\nToo many things to do (to-do list)\nChemical solutions to low energy (coffee during the day)\nAnxiety about completing tasks (worrying, insomnia)\nLack of personal time for hobbies or exercise\n\nBrainstorm Solutions:\n\nGet a check-up to rule out any other issues that could cause sleep quality degradation - depression, anxiety, sleep apnea, thyroid conditions, etc.\n\nAsk the doctor about taking melatonin supplements for a short time to ensure that sleep starts off well (note, don’t take medical advice from a stats textbook!)\n\nReformat your to-do list:\n\nSet time limits for things on the to-do list\nBreak the to-do list into smaller, manageable tasks that can be accomplished within a relatively short interval - such as an hour\nSort the to-do list by priority and level of “fun” so that each day has a few hard tasks and a couple of easy/fun tasks. Do the hard tasks first, and use the easy/fun tasks as a reward.\n\nSet a time limit for caffeine (e.g. no coffee after noon) so that caffeine doesn’t contribute to poor quality sleep\nAddress anxiety with medication (from 1), scheduled time for mindfulness meditation, and/or self-care activities\nScheduling time for exercise/hobbies\n\nscheduling exercise in the morning to take advantage of the endorphins generated by working out\nscheduling hobbies in the evening to reward yourself for a day’s work and wind down work well before bedtime\n\n\nApproach each sub-problem separately\nWhen the sub-problem has a viable solution, move on to the next sub-problem. Don’t try to tackle everything at once. Here, that might look like this list, where each step is taken separately and you give each thing a few days to see how it affects your sleep quality. In programming, of course, this list would perhaps be a bit more sequential, but real life is messy and the results take a while to populate.\n\n\n[1] Make the doctor’s appointment.\n[5] While waiting for the appointment, schedule exercise early in the day and hobbies later in the day to create a “no-work” period before bedtime.\n[1] Go to the doctor’s appointment, follow up with any concerns.\n\n[1] If doctor approves, start taking melatonin according to directions\n\n[2] Work on reformatting the to-do list into manageable chunks. Schedule time to complete chunks using your favorite planning method.\n[4] If anxiety is still an issue after following up with the doctor, add some mindfulness meditation or self-care to the schedule in the mornings or evenings.\n[3] If sleep quality is still an issue, set a time limit for caffeine\n[2] Revise your to-do list and try a different tactic if what you were trying didn’t work.\n\n\nHere’s another example of how to break down a real-world personal problem in programming/debugging style.\n\n\n9.6.4 Minimal Working (or Reproducible) Examples\nIf all else has failed, and you can’t figure out what is causing your error, it’s probably time to ask for help. If you have a friend or buddy that knows the language you’re working in, by all means ask for help sooner - use them as a rubber duck if you have to. But when you ask for help online, often you’re asking people who are much more knowledgeable about the topic - members of R core and the people who developed NumPy and Pandas browse stackoverflow and may drop in and help you out. Under those circumstances, it’s better to make the task of helping you as easy as possible because it shows respect for their time. The same thing goes for your supervisors and professors.\n\n\n\nThe reprex R package will help you make a reproducible example (drawing by Allison Horst)\n\n\nThere is also a reprexpy package in python, but it doesn’t have nearly as nice of an illustration to go with it.\nSo, with that said, there are numerous resources for writing what’s called a “minimal working example”, “reproducible example” (commonly abbreviated reprex), or MCVE (minimal complete verifiable example). Much of this is lifted directly from the StackOverflow post describing a minimal reproducible example.\nThe goal is to reproduce the error message with information that is\n\nminimal - as little code as possible to still reproduce the problem\ncomplete - everything necessary to reproduce the issue is contained in the description/question\nreproducible - test the code you provide to reproduce the problem.\n\nYou should format your question to make it as easy as possible to help you. Make it so that code can be copied from your post directly and pasted into a terminal. Describe what you see and what you’d hope to see if the code were working.\nOther resources: - reprex package: Do’s and Don’ts - How to use the reprex package - vignette with videos from Jenny Bryan - reprex magic - Vignette adapted from a blog post by Nick Tierney - reprexpy package\n\n\n\n\nWickham, H. 2015. Advanced R. Chapman & Hall/CRC The R Series. CRC Press. https://books.google.com/books?id=FfsYCwAAQBAJ.\n\n\nWickham, H., and G. Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. https://books.google.com/books?id=vfi3DQAAQBAJ."
  },
  {
    "objectID": "10-graphics.html#why-do-we-create-graphics",
    "href": "10-graphics.html#why-do-we-create-graphics",
    "title": "10  Data Visualization",
    "section": "10.1 Why do we create graphics?",
    "text": "The greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (“Why, that scatter diagram has a hole in the middle!”) – John Tukey, Data Based Graphics: Visual Display in the Decades to Come\n\nFundamentally, charts are easier to understand than raw data.\nWhen you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the “real world” into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is “tangible” in some way – if not physical and literally touch-able, at least something we can view and “wrap our heads around”.\nConsider this thought experiment: You have a simple data set - 2 variables, 500 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options:\n\nPrint out the data set\nCreate some summary statistics of each variable and perhaps the covariance between the two variables\nDraw a scatter plot of the two variables\n\nWhich one would you rather use? Why?\nOur brains are very good at processing large amounts of visual information quickly. Evolutionarily, it’s important to be able to e.g. survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we’re not nearly as equipped to process quickly (and in parallel).\nIt’s a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out.\n\nYou’re going to learn how to make graphics by finding sample code, changing that code to match your data set, and tweaking things as you go. That’s the best way to learn this, and while ggplot and plotnine do have a structure and some syntax to learn, once you’re familiar with the principles, you’ll still want to learn graphics by doing it.\n\n\nIn this chapter, we’re going to use the ggplot2 package to create graphics in R, and the plotnine package to create graphics in python. plotnine is a direct port of ggplot2 to Python using the Python graphics engine. For the most part, the syntax is extremely similar, with only minimal changes to account for the fact that some R syntax doesn’t work in Python, and a few differences with the python rendering engine for graphics."
  },
  {
    "objectID": "10-graphics.html#the-grammar-of-graphics",
    "href": "10-graphics.html#the-grammar-of-graphics",
    "title": "10  Data Visualization",
    "section": "10.2 The Grammar of Graphics",
    "text": "The grammar of graphics is an approach first introduced in Leland Wilkinson’s book (Wilkinson 2005). Unlike other graphics classification schemes, the grammar of graphics makes an attempt to describe how the dataset itself relates to the components of the chart.\n\n\n\nBuilding a masterpiece, by Allison Horst\n\n\nThis has a few advantages:\n\nIt’s relatively easy to represent the same dataset with different types of plots (and to find their strengths and weaknesses)\nGrammar leads to a concise description of the plot and its contents\nWe can add layers to modify the graphics, each with their own basic grammar (just like we combine sentences and clauses to build a rich, descriptive paragraph)\n\n\n\n\nA pyramid view of the major components of the grammar of graphics, with data as the base, aesthetics building on data, scales building on aesthetics, geometric objects, statistics, facets, and the coordinate system at the top of the pyramid. Source: (Sarkar 2018)\n\n\n\nI have turned off warnings for all of the code chunks in this chapter. When you run the code you may get warnings about e.g. missing points - this is normal, I just didn’t want to have to see them over and over again - I want you to focus on the changes in the code.\n\n\n10.2.1 Exploratory Data Analysis with the grammar of graphics\n\nSketchRPython\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\n\nlibrary(ggplot2)\ndata(txhousing)\n\nggplot(data = txhousing, aes(x = date, y = median)) + \n  geom_point()\n\n\n\n\n\n\n\nfrom plotnine import *\nfrom plotnine.data import txhousing\n\nggplot(aes(x = \"date\", y = \"median\"), data = txhousing) + geom_point()\n\n<ggplot: (8755715767149)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\n\n\n\n\nWhen creating a grammar of graphics chart, we start with the data (this is consistent with the data-first tidyverse philosophy).\n\nIdentify the dimensions of your dataset you want to visualize.\nDecide what aesthetics you want to map to different variables. For instance, it may be natural to put time on the \\(x\\) axis, or the experimental response variable on the \\(y\\) axis. You may want to think about other aesthetics, such as color, size, shape, etc. at this step as well.\n\nIt may be that your preferred representation requires some summary statistics in order to work. At this stage, you would want to determine what variables you feed in to those statistics, and then how the statistics relate to the geoms that you’re envisioning. You may want to think in terms of layers - showing the raw data AND a summary geom.\n\nIn most cases, ggplot will determine the scale for you, but sometimes you want finer control over the scale - for instance, there may be specific, meaningful bounds for a variable that you want to directly set.\nCoordinate system: Are you going to use a polar coordinate system? (Please say no, for reasons we’ll get into later!)\nFacets: Do you want to show subplots based on specific categorical variable values?\n\n(this list modified from Sarkar (2018)).\n\nLet’s explore the txhousing data a bit more thoroughly by adding some complexity to our chart. This example will give me an opportunity to show you how an exploratory data analysis might work in practice, while also demonstrating some of ggplot2’s features.\nBefore we start exploring, let’s add a title and label our axes, so that we’re creating good, informative charts.\n\nRPython\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) + \n  geom_point() + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nggplot(aes(x = \"date\", y = \"median\"), data = txhousing) +\\\ngeom_point() +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n\n<ggplot: (8755715763667)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\n\n\n\n\nFirst, we may want to show some sort of overall trend line. We can start with a linear regression, but it may be better to use a loess smooth (loess regression is a fancy weighted average and can create curves without too much additional effort on your part).\n\nSketchRPython\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(data = txhousing, aes(x = date, y = median)) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nggplot(aes(x = \"date\", y = \"median\"), data = txhousing) + geom_point() +\\\ngeom_smooth(method = \"lm\", color = \"blue\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n# By default, geom_smooth in plotnine has a black line you can't see well\n\n<ggplot: (8755715531114)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(aes(x = \"date\", y = \"median\"), data = txhousing) + geom_point() +\\\ngeom_smooth(method = \"loess\", color = \"blue\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n\n<ggplot: (8755712288371)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\n\n\n\n\nLooking at the plots here, it’s clear that there are small sub-groupings (see, for instance, the almost continuous line of points at the very top of the group between 2000 and 2005). Let’s see if we can figure out what those additional variables are…\nAs it happens, the best viable option is City.\n\nSketchRPython\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nThat’s a really crowded graph! It’s slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) + \n  # geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nggplot(aes(x = \"date\", y = \"median\", color = \"city\"), data = txhousing) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n\n<ggplot: (8755715566535)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\n\nThat’s a really crowded graph! It’s slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(aes(x = \"date\", y = \"median\", color = \"city\"), data = txhousing) +\\\ngeom_smooth(method = \"loess\") +\\\ntheme(subplots_adjust={'right': 0.5}) +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n\nThis is one of the first places we see differences in Python and R’s graphs - python doesn’t allocate sufficient space for the legend by default. In Python, you have to manually adjust the theme to show the legend (or plot the legend separately).\n\n\n\nIn reality, though, you should not ever map color to something with more than about 7 categories if your goal is to allow people to trace the category back to the label. It just doesn’t work well perceptually.\nSo let’s work with a smaller set of data: Houston, Dallas, Fort worth, Austin, and San Antonio (the major cities).\nAnother way to show this data is to plot each city as its own subplot. In ggplot2 lingo, these subplots are called “facets”. In visualization terms, we call this type of plot “small multiples” - we have many small charts, each showing the trend for a subset of the data.\n\nRPython\n\n\n\ncitylist <- c(\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\")\nhousingsub <- dplyr::filter(txhousing, city %in% citylist)\n\nggplot(data = housingsub, aes(x = date, y = median, color = city)) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nHere’s the facetted version of the chart:\n\nggplot(data = housingsub, aes(x = date, y = median)) + \n  geom_point() + \n  geom_smooth(method = \"loess\") + \n  facet_wrap(~city) + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\nggplot(aes(x = \"date\", y = \"median\", color = \"city\"), data = housingsub) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\") +\\\ntheme(subplots_adjust={'right': 0.75}) +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n\n<ggplot: (8755715575787)>\n\n\n\n\n\nHere’s the facetted version of the chart:\n\nggplot(aes(x = \"date\", y = \"median\"), data = housingsub) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\", color = \"blue\") +\\\nfacet_wrap(\"city\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n\n<ggplot: (8755712203858)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/utils.py:371: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/utils.py:371: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\nNow that we’ve simplified our charts a bit, we can explore a couple of the other quantitative variables by mapping them to additional aesthetics:\n\nRPython\n\n\n\nggplot(data = housingsub, aes(x = date, y = median, size = sales)) + \n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") + \n  facet_wrap(~city) +\n  # Remove extra information from the legend - \n  # line and error bands aren't what we want to show\n  # Also add a title\n  guides(size = guide_legend(title = 'Number of Sales', \n                             override.aes = list(linetype = NA, \n                                                 fill = 'transparent'))) + \n  # Move legend to bottom right of plot\n  theme(legend.position = c(1, 0), legend.justification = c(1, 0)) + \n  xlab(\"Date\") + ylab(\"Median Home Price\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\n( # This is used to group lines together in python\nggplot(aes(x = \"date\", y = \"median\", size = \"sales\"), data = housingsub)\n+ geom_point(alpha = .15) # Make points transparent\n+ geom_smooth(method = \"loess\") \n+ facet_wrap(\"city\")\n+ guides(size = guide_legend(title = 'Number of Sales')) \n+ xlab(\"Date\") + ylab(\"Median Home Price\")\n+ ggtitle(\"Texas Housing Prices\")\n)\n\n<ggplot: (8755709592998)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/utils.py:371: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/utils.py:371: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\nNot all of the features we used in R are available in plotnine in Python (in part because of limitations of the underlying graphics interface that plotnine uses). This does somewhat limit the customization we can do with python, but for the most part we can still get the same basic information back out.\n\n\n\nUp to this point, we’ve used the same position information - date for the y axis, median sale price for the y axis. Let’s switch that up a bit so that we can play with some transformations on the x and y axis and add variable mappings to a continuous variable.\n\nRPython\n\n\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) + \n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nThe points for Fort Worth are compressed pretty tightly relative to the points for Houston and Dallas. When we get this type of difference, it is sometimes common to use a log transformation1. Here, I have transformed both the x and y axis, since the number of sales seems to be proportional to the number of listings.\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) + \n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  scale_x_log10() + \n  scale_y_log10() + \n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") + \n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n( # This is used to group lines together in python\nggplot(aes(x = \"listings\", y = \"sales\", color = \"city\"), data = housingsub)\n+ geom_point(alpha = .15) # Make points transparent\n+ geom_smooth(method = \"loess\") \n+ scale_x_log10()\n+ scale_y_log10()\n+ xlab(\"Date\") + ylab(\"Median Home Price\")\n+ ggtitle(\"Texas Housing Prices\")\n)\n\n<ggplot: (8755709518626)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 6 rows containing missing values.\n\n\n\n\n\nNotice that the gridlines included in python by default are different than those in ggplot2 by default (personally, I vastly prefer the python version - it makes it obvious that we’re using a log scale).\n\n\n\nFor the next demonstration, let’s look at just Houston’s data. We can examine the inventory’s relationship to the number of sales by looking at the inventory-date relationship in x and y, and mapping the size or color of the point to number of sales.\n\nRPython\n\n\n\nhouston <- dplyr::filter(txhousing, city == \"Houston\")\n\nggplot(data = houston, aes(x = date, y = inventory, size = sales)) + \n  geom_point(shape = 1) + \n  xlab(\"Date\") + ylab(\"Months of Inventory\") + \n  guides(size = guide_legend(title = \"Number of Sales\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\nggplot(data = houston, aes(x = date, y = inventory, color = sales)) + \n  geom_point() + \n  xlab(\"Date\") + ylab(\"Months of Inventory\") + \n  guides(size = guide_colorbar(title = \"Number of Sales\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\nWhich is easier to read?\nWhat happens if we move the variables around and map date to the point color?\n\nggplot(data = houston, aes(x = sales, y = inventory, color = date)) + \n  geom_point() + \n  xlab(\"Number of Sales\") + ylab(\"Months of Inventory\") + \n  guides(size = guide_colorbar(title = \"Date\")) + \n  ggtitle(\"Houston Housing Data\")\n\n\n\n\nIs that easier or harder to read?\n\n\n\nhouston = txhousing[txhousing.city==\"Houston\"]\n\n(\n  ggplot(aes(x = \"date\", y = \"inventory\", size = \"sales\"), data = houston)\n  + geom_point(shape = 'o', fill = 'none') \n  + xlab(\"Date\") + ylab(\"Median Home Price\")\n  + guides(size = guide_legend(title = \"Number of Sales\"))\n  + ggtitle(\"Houston Housing Data\")\n)\n\n<ggplot: (8755709813125)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\n\nIn plotnine, we have to use matplotlib marker syntax.\n\n\n(\n  ggplot(aes(x = \"date\", y = \"inventory\", color = \"sales\"), data = houston)\n  + geom_point() \n  + xlab(\"Date\") + ylab(\"Median Home Price\")\n  + guides(size = guide_legend(title = \"Number of Sales\"))\n  + ggtitle(\"Houston Housing Data\")\n)\n\n<ggplot: (8755709425993)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\n\nPlotnine also defaults to different color schemes than ggplot2 – just something to know if you want the plot to be exactly the same. Personally, I prefer the viridis color scheme (what plotnine uses) to the ggplot2 defaults.\nWhat happens if we move the variables around and map date to the point color?\n\n(\nggplot(aes(x = \"sales\", y = \"inventory\", color = \"date\"), data = houston) \n  + geom_point() \n  + xlab(\"Number of Sales\") + ylab(\"Months of Inventory\") \n  + guides(size = guide_colorbar(title = \"Date\")) \n  + ggtitle(\"Houston Housing Data\")\n  + theme(subplots_adjust={'right': 0.75})\n)\n\n<ggplot: (8755709425156)>\n\n/__w/Stat151/Stat151/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\n\nIs that easier or harder to read?\n\n\n\n\n\n\n10.2.2 What type of chart to use?\nIt can be hard to know what type of chart to use for a particular type of data. I recommend figuring out what you want to show first, and then thinking about how to show that data with an appropriate plot type. Consider the following factors:\n\nWhat type of variable is x? Categorical? Continuous? Discrete?\nWhat type of variable is y?\nHow many observations do I have for each x/y variable?\nAre there any important moderating variables?\nDo I have data that might be best shown in small multiples? E.g. a categorical moderating variable and a lot of data, where the categorical variable might be important for showing different features of the data?\n\nOnce you’ve thought through this, take a look through catalogues like the R Graph Gallery to see what visualizations match your data and use-case."
  },
  {
    "objectID": "10-graphics.html#creating-good-charts",
    "href": "10-graphics.html#creating-good-charts",
    "title": "10  Data Visualization",
    "section": "10.3 Creating Good Charts",
    "text": "A chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g. the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate.\nUnfortunately, while our visual system is amazing, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms.\n\n10.3.1 Perceptual and Cognitive Factors\n\n10.3.1.1 Color\nOur eyes are optimized for perceiving the yellow/green region of the color spectrum. Why? Well, our sun produces yellow light, and plants tend to be green. It’s pretty important to be able to distinguish different shades of green (evolutionarily speaking) because it impacts your ability to feed yourself. There aren’t that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum.\n\n\n\nSensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons)\n\n\nNot everyone perceives color in the same way. Some individuals are colorblind or color deficient. We have 3 cones used for color detection, as well as cells called rods which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, <1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances.\n\n\nYou can take a test designed to screen for colorblindness here\n\nYour monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. A different test is available here.\n \n In reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown.\n\n\n\nIt is possible to simulate the effect of color blindness and color deficiency on an image.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context.\n\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\nOur brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly “remove” the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown above - because we’re correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade.\nImplications and Guidelines\n\nDo not use rainbow color gradient schemes - because of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance.\nAvoid any scheme that uses green-yellow-red signaling if you have a target audience that may include colorblind people.\nTo “colorblind-proof” a graphic, you can use a couple of strategies:\n\ndouble encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out\nIf you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it!\nIf you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -> dark by colorblind people, so it will be correctly perceived no matter what color you use.\nIf you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is purple - white - orange. In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly.\n\nBe conscious of what certain colors “mean”\n\nLeveraging common associations can make it easier to read a color scale and remember what it stands for (e.g. blue for cold, orange/red for hot is a natural scale, red = Republican and blue = Democrat in the US, white -> blue gradients for showing rainfall totals)\nSome colors can can provoke emotional responses that may not be desirable.2\nIt is also important to be conscious of the social baggage that certain color schemes may have - the pink/blue color scheme often used to denote gender can be unnecessarily polarizing, and it may be easier to use a colder color (blue or purple) for men and a warmer color (yellow, orange, lighter green) for women3.\n\nThere are packages such as RColorBrewer and dichromat that have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer). You can also take a look at other ways to find nice color palettes.\n\n\n\n10.3.1.2 Short Term Memory\nWe have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time.\n\nTry it out!\n\n\nClick here, read the information, and then click to hide it.\n\n1 4 2 2 3 9 8 0 7 8\n\n\n\nWait a few seconds, then expand this section\n\nWhat was the third number?\n\n\nWithout rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 “bits” of information.\nIn charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart.\nWhere possible, try to keep your legends to 6 or 7 characteristics.\nImplications and Guidelines\n\nLimit the number of categories in your legends to minimize the short term memory demands on your reader.\n\nWhen using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude.\n\nUse colors and symbols which have implicit meaning to minimize the need to refer to the legend.\nAdd annotations on the plot, where possible, to reduce the need to re-read captions.\n\n\n\n10.3.1.3 Grouping and Sense-making\nImposing order on visual chaos.\n\nAmbiguous ImagesIllusory ContoursFigure/Ground\n\n\nWhat does the figure below look like to you?\n\n\n\nIs it a rabbit, or a duck?\n\n\nWhen faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options.\n\n\n\n\n\nConsider this image - what do you see?\n\n\nDid you see something like “3 circles, a triangle with a black outline, and a white triangle on top of that”? In reality, there are 3 angles and 3 pac-man shapes. But, it’s much more likely that we’re seeing layers of information, where some of the information is obscured (like the “mouth” of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience.\n\n\nNow, look at the logo for the Pittsburgh Zoo.\n\nDo you see the gorilla and lionness? Or do you see a tree? Here, we’re not entirely sure which part of the image is the figure and which is the background.\n\n\n\nThe ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs.\nThe catchphrase of Gestalt psychology is\n\nThe whole is greater than the sum of the parts\n\nThat is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene.\n\n\n\nThe Gestalt Heuristics help us to impose order on ambiguous visual stimuli\n\n\nYou can read about the gestalt rules here, but they are also demonstrated in the figure above.\nIn graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving “with” the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders.\n\nSuppose I want to emphasize the change in the murder rate between 1980 and 2010.\nI could use a bar chart (showing only the first 4 states alphabetically for space)\n\nRPython\n\n\n\nfbiwide <- read.csv(\"https://github.com/srvanderplas/Stat151/raw/main/data/fbiwide.csv\")\nlibrary(dplyr)\n\nfbiwide %>% \n  filter(Year %in% c(1980, 2010)) %>%\n  filter(State %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\")) %>% \n  ggplot(aes(x = State, y = Murder/Population*100000, fill = factor(Year))) + \n  geom_col(position = \"dodge\") +\n  coord_flip() + \n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\n\n\n\nimport pandas as pd\nfbiwide = r.fbiwide\nfbiwide = fbiwide.assign(YearFactor = pd.Categorical(fbiwide.Year))\nfbiwide = fbiwide.assign(Murder100k = fbiwide.Murder/fbiwide.Population * 100000)\n\nyr1980_2010 = fbiwide[fbiwide.Year.isin([1980,2010])]\nsubdata = yr1980_2010[yr1980_2010.State.isin([\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\"])]\n\n(\nggplot(aes(x = \"State\", y = \"Murder100k\", fill = \"YearFactor\"), data = subdata) + \n  geom_col(stat='identity', position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Murders per 100,000 residents\")\n)\n\n<ggplot: (8755707330292)>\n\n\n\n\n\n\n\n\nOr, I could use a line chart\n\nRPython\n\n\n\nfbiwide %>% \n  filter(Year %in% c(1980, 2010)) %>%\n  ggplot(aes(x = Year, y = Murder/Population*100000, group = State)) + \n  geom_line() + \n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\n\n\n\n(\nggplot(aes(x = \"Year\", y = \"Murder100k\", group = \"State\"), data = yr1980_2010) + \n  geom_line() + \n  ylab(\"Murders per 100,000 residents\")\n)\n\n<ggplot: (8755707272217)>\n\n\n\n\n\n\n\n\nOr, I could use a box plot\n\nRPython\n\n\n\nfbiwide %>% \n  filter(Year %in% c(1980, 2010)) %>%\n  ggplot(aes(x = factor(Year), y = Murder/Population*100000)) + \n  geom_boxplot() + \n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\n\n\n\n\n(\nggplot(aes(x = \"YearFactor\", y = \"Murder100k\"), data = yr1980_2010) + \n  geom_boxplot() + \n  ylab(\"Murders per 100,000 residents\")\n)\n\n<ggplot: (8755709423144)>\n\n\n\n\n\n\n\n\nWhich one best demonstrates that in every state and region, the murder rate decreased?\nThe line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g. almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn’t allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can’t see the individual relationships.\n\nThe aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) can easily make when examining those plots.4\n\n\n\n10.3.2 General guidelines for accuracy\nThere are certain tasks which are easier for us relative to other, similar tasks.\n\n\n\n\n\nWhich of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. In fact, the line lengths are the same in both panels.\n\n\n\n\nWhen making judgments corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level.5\n\nPosition (common scale)\nPosition (non-aligned scale)\nLength, Direction, Angle, Slope\nArea\nVolume, Density, Curvature\nShading, Color Saturation, Color Hue\n\nIf we compare a pie chart and a stacked bar chart, the bar chart asks readers to make judgements of position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not preferable – they make it harder on the reader, and as a result we are less accurate when reading information from pie charts.\nWhen creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc.\nThere is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area.\n.\nTed ED: How to spot a misleading graph - Lea Gaslowitz\nBusiness Insider: The Worst Graphs Ever\nExtra dimensions and other annotations are sometimes called “chartjunk” and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration)."
  },
  {
    "objectID": "10-graphics.html#references",
    "href": "10-graphics.html#references",
    "title": "10  Data Visualization",
    "section": "References",
    "text": "R graphics\n\nggplot2 cheat sheet\nggplot2 aesthetics cheat sheet - aesthetic mapping one page cheatsheet\nggplot2 reference guide\nR graph cookbook\nData Visualization in R (@ramnathv)\n\n\n\nPython graphics\n\nPlotnine documentation\nMatplotlib documentation - Matplotlib is the base that plotnine uses to replicate ggplot2 functionality\n\nVisualization with Matplotlib chapter of Python Data Science\n\nScientific Visualization with Python\n\n\n\n\n\nSarkar, Dipanjan (DJ). 2018. “A Comprehensive Guide to the Grammar of Graphics for Effective Visualization of Multi-Dimensional….” Medium. https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Statistics and Computing. New York: Springer Science."
  },
  {
    "objectID": "git-github.html",
    "href": "git-github.html",
    "title": "Introduction to Statistical Computing",
    "section": "",
    "text": ""
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Sarkar, Dipanjan (DJ). 2018. “A Comprehensive Guide to the Grammar of Graphics for Effective Visualization of Multi-Dimensional….” Medium. https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149.\n\n\nSeverance, Dr Charles Russell. 2016. Python for Everybody: Exploring Data in Python 3. Edited by Sue Blumenberg and Elliott Hauser. Ann Arbor, MI: CreateSpace Independent Publishing Platform. https://www.py4e.com/html3/.\n\n\nWickham, H. 2015. Advanced R. Chapman & Hall/CRC The R Series. CRC Press. https://books.google.com/books?id=FfsYCwAAQBAJ.\n\n\nWickham, H., and G. Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. https://books.google.com/books?id=vfi3DQAAQBAJ.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Statistics and Computing. New York: Springer Science."
  }
]