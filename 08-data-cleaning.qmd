---
engine: knitr
execute:
  freeze: auto  # re-render only when source changes
---

# Data Summaries and Data Cleaning

In this chapter, we'll talk a bit about different operations you may need to do in order to clean your data up and get it into the form you want.

## Merging Tables

Sometimes, we have two tables that we want to join together by a certain variable.


We know how to work on one table at a time, creating new variables, editing old variables, and even reformatting the table using wide and long format, but data doesn't always come organized in one table at a time. Instead, some data may be organized relationally - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between individuals that have to be documented. 

::: ex

<details><summary>Example: Primary School Organization</summary>

Each individual has certain characteristics: 

- full_name
- gender
- birth date
- ID number

Each student has specific characteristics:

- ID number 
- parent name
- parent phone number
- medical information
- Class ID

Teachers may also have additional information:

- ID number
- Class ID
- employment start date
- education level
- compensation level

There are also fields like grades, which occur for each student in each class, but multiple times  a year. 

- ID number
- Student ID
- Class ID
- year
- term number
- subject
- grade
- comment

And for teachers, there are employment records on a yearly basis

- ID number
- Employee ID
- year
- rating
- comment

But each class also has characteristics that describe the whole class as a unit: 

- location ID
- class ID
- meeting time
- grade level

Each location might also have some logistical information attached:

- location ID
- room number
- building
- number of seats
- AV equipment


![Primary School Database Schema](images/PrimarySchoolExample.png)
<!-- <a href="https://dbdiagram.io/embed/5ef387179ea313663b3b048e">Link to diagram of the database</a> -->

We could go on, but you can see that this data is hierarchical, but also relational:

- each class has both a teacher and a set of students
- each class is held in a specific location that has certain equipment

It would be silly to store this information in a single table (though it probably can be done) because all of the teacher information would be duplicated for each student in each class; all of the student's individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well. 

But, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps: 

- get the Class ID
- get any teachers that are assigned that Class ID - specifically, get their ID number
- get any students that are assigned that Class ID - specifically, get their ID number
- append the results from teachers and students so that there is a list of all individuals in the class
- look through the "individual data" table to find any individuals with matching ID numbers, and keep those individuals' birth days. 

</details>

:::

Table joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information. 

There are 3 main types of table joins:

- **Filtering joins**, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don't change)    
Ex: finding all teachers or students who have class ClassID

- **Set operations**, which treat observations as set elements (e.g. union, intersection, etc.)    
Ex: taking the union of all student and teacher IDs to get a list of individual IDs

- **Mutating joins**, which add columns from one table to matching rows in another table    
Ex: adding birthday to the table of all individuals in a class

**keys** are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A **primary key** identifies an observation in its own table. A **foreign key** identifies an observation in another table.

We're primarily going to focus on mutating joins, as filtering joins can be accomplished by ... filtering ... rather than by table joins. Feel free to read through the other types of joins [here](https://r4ds.had.co.nz/relational-data.html#filtering-joins)

<details><summary>Animating different types of joins</summary>
Note: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.

If we start with two tables, x and y, 

![](images/dplyr-original-dfs.png)

```{r}
library(dplyr) # Must load this library to do these joins
x <- data.frame(c1 = 1:3, cx = c("x1", "x2", "x3"))
y <- data.frame(c1 = c(1, 2, 4), cy = c("y1", "y2", "y4"))
```

```{python}
import pandas as pd # must load pandas to do joins
x = r.x
y = r.y
```


We can do a filtering `inner_join` to keep only rows which are in both tables (but we keep all columns)

![](images/dplyr-inner-join.gif)

```{r}
inner_join(x, y)
```

```{python}
pd.merge(x, y) # by default, merge uses inner join
```

But what if we want to keep all of the rows in x? We would do a `left_join`

![](images/dplyr-left-join.gif)

If there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one. 

![](images/dplyr-left-join-extra.gif)
```{r}
left_join(x, y)
```

```{python}
pd.merge(x, y, how = "left")
```


If we wanted to keep all of the rows in y, we would do a `right_join`: 

![](images/dplyr-right-join.gif)

(or, we could do a left join with y and x, but... either way is fine).

```{r}
right_join(x, y)
```

```{python}
pd.merge(x, y, how = "right")
```

And finally, if we want to keep all of the rows, we'd do a `full_join` (referred to as an "outer join" in pandas):

![](images/dplyr-full-join.gif)

```{r}
full_join(x, y)
```

```{python}
pd.merge(x, y, how = "outer")
```

You can find other animations corresponding to filtering joins and set operations  [here](https://github.com/gadenbuie/tidyexplain)

</details>

### Try it out {.tryitout -}

[Rebrickable.com](https://rebrickable.com/downloads/) maintains a database of Lego sets, parts, and other data, available for download. You can download the data yourself, or you can use the tables I've downloaded and included: [sets](data/sets.csv) and [themes](data/themes.csv)

```{r}
sets <- read.csv("https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/sets.csv")
themes <- read.csv("https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/themes.csv")
```

```{python}
import pandas as pd
sets = pd.read_csv("https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/sets.csv")
themes = pd.read_csv("https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/themes.csv")
```

Let's start out by joining the two datasets together. Note that we'll need to specify which columns to join by in both R and pandas. In R, we'll need to use `by = c(left_col = right_col)` to specify the column names in the left and right data frames. In pandas, we'll need to use arguments `left_on = 'left_col'` and `right_on = 'right_col'`. 

**First, let's try a full/outer join.**

<details><summary>R solution</summary>
```{r}
lego_fulljoin <- full_join(sets, themes, by = c("theme_id" = "id"))
head(lego_fulljoin)
```
</details>

<details><summary>Python solution</summary>
```{python}
lego_fulljoin = pd.merge(sets, themes, left_on = "theme_id", right_on = "id", how = "outer")
lego_fulljoin.head()
```
</details>

**Sometimes, it's easier to rename the columns before merging.**
Try that approach - if you have the same name for the columns that you intend to join on (and no other common names) then it's easier to do the join and to understand what is happening. Try it out with the lego sets to see which approach you prefer.

<details><summary>R solution</summary>

```{r}
sets_rn <- sets %>%
  rename(set_name = name)
themes_rn <- themes %>%
  rename(theme_name = name, theme_id = id, theme_parent_id = parent_id)
lego_fulljoin <- full_join(sets_rn, themes_rn)
head(lego_fulljoin)
```

</details>

<details><summary>Python solution</summary>
To do this, I consulted [stackoverflow](https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas)

```{python}
sets_rn = sets # copy the dataset
sets_rn = sets_rn.rename(columns = {'name':'set_name'})
themes_rn = themes.rename(columns = {'id': 'theme_id', 'parent_id':'theme_parent_id', 'name' :'theme_name'})

lego_fulljoin = pd.merge(sets_rn, themes_rn, how = "outer")
lego_fulljoin.head()
```

</details>

**Which type of join?**
In some cases, we might prefer to use a different type of join. If our goal is to add the context of theme information to the set data, we might not care about themes that don't have corresponding sets in our data. Can you determine what type of join is appropriate here?


<details><summary>R solution</summary>

```{r}
lego_data <- left_join(sets_rn, themes_rn)
head(lego_data)
```

</details>

<details><summary>Python solution</summary>

```{python}
lego_data = pd.merge(sets_rn, themes_rn, how = "left")
lego_data.head()
```

</details>

**Using Your Data**
Pick a theme you're interested in, and plot the number of pieces in the sets of that theme over time.


<details><summary>R solution</summary>
I want to look at Pirates sets. We can see that there are 3 generations of main "Pirates" theme sets, but there is a parent theme that contains all of them. So let's filter the full dataset on that parent id.

```{r}
library(ggplot2)
themes %>%
  filter(grepl("Pirates", name))

lego_data %>%
  filter(theme_parent_id == 147) %>%
  ggplot(aes(x = year, y = num_parts)) + geom_jitter()
```

</details>

<details><summary>Python solution</summary>
In this case, let's look at any sets that have a theme name containing "Jurassic" (Park, World, etc.)

```{python}
from plotnine import *
dinos = lego_data.loc[lego_data["theme_name"].str.contains("Jurassic")]
ggplot(dinos, aes(x = "year", y = "num_parts")) + geom_jitter()
```

</details>

## Data Summaries

We've talked before about using for loops to create summaries of your data, as in [this example](#summaries-part-2). 

In may cases, however, it is easier to use a slightly different mechanism to work with groups of data. What do I mean by groups of data? When we used loops, the variable we "group by" is the variable controlling the loop. 

<details><summary>Summarizing data with Lego</summary>

![A lego "data frame" with two columns: green 1x2s and purple/pink 1x4s.](images/lego-ungrouped-data-frame.png)

![We can "group" the data frame by the different shades of 1x2 lego pieces](images/lego-grouped-data-frame.png)

![We can then compute a summary variable for the group, picking out e.g. the most "intense" shade in the purple/pink column.](images/lego-grouped-data-frame-summary.png)

![Looking only at the summary, we could then ungroup our data and return to a regular data frame... only this time, we have a summary, with one row for each different value of the green column](images/lego-grouped-summary.png)

</details>

In R/tidyverse syntax, we would use the `group_by` function to group a dataframe by a variable, and then we would use `mutate` or `summarize` to create our new column(s). `mutate` would be used if we want to have the same number of rows in our output as we had in the input, while `summarize` would create one row per group.

In python syntax, we use `groupby` to group the DataFrame by a variable, and then we use `.agg` to aggregate. The function `pd.NamedAgg(column, function)` allows us to explicitly state that we want to use function `function` on column `column`, and assign that result to a new variable.

::: ex

Suppose we want to summarize the lego set data by year, computing the number of sets and the mean number of pieces per set. We'll take the data set we generate and plot the number of pieces, with point size scaled to show the number of sets released that year.

```{r}
sets %>%
  group_by(year) %>%
  summarize(n = n(), mean_pieces = mean(num_parts)) %>%
  ggplot(aes(x = year, y = mean_pieces, size = n)) + geom_point()
```

```{python}
tmp = sets.groupby("year", as_index = False).agg(
  mean_pieces = pd.NamedAgg('num_parts', 'mean'),
  n = pd.NamedAgg('num_parts', 'count'))
ggplot(tmp, aes(x = "year", y = "mean_pieces", size = "n")) + geom_point()

```

:::

## Working with Text

Nearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different "levels" of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated). 

> Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski

![Alternately, the xkcd version of the above quote](https://imgs.xkcd.com/comics/perl_problems.png)


### Basic String Operations

The tidyverse package to deal with strings is [`stringr`](https://stringr.tidyverse.org/). The functions in stringr take the form of `str_XXX` where XXX is a verb. So `str_split()`, `str_replace()`, `str_remove()`, `str_to_lower()` all should make some sense.

The corresponding python library is `re`, short for regular expression. Pandas also includes some functionality from this package in (partially) vectorized form.

::: ex

```{r readr-college-data, eval = F, include = F}
library(readr)
library(magrittr)
library(dplyr)

college_data <- read_csv("https://ed-public-download.app.cloud.gov/downloads/Most-Recent-Cohorts-All-Data-Elements.csv") 
college_data2 <- college_data %>%
  select(UNITID, INSTNM, CITY, STABBR, ZIP, ACCREDAGENCY, INSTURL, PREDDEG, MAIN, NUMBRANCH, HIGHDEG, CONTROL, ST_FIPS, LOCALE, LATITUDE, LONGITUDE) %>%
  mutate(PREDDEG = factor(PREDDEG, levels = 0:4, labels = c("Not classified", "Predominantely certificate-degree granting", "Predominantely associate's-degree granting", "Predominantly bachelor's-degree granting", "Entirely graduate-degree granting")),
         MAIN = factor(MAIN, levels = 0:1, labels = c("Not main campus", "main campus")),
         HIGHDEG = factor(HIGHDEG, levels = 0:4, labels = c("Non-degree granting", "Certificate", "Associate", "Bachelors", "Graduate")),
         CONTROL = factor(CONTROL, levels = 1:3, labels = c("Public", "Private Non Profit", "Private For Profit"))) %>%
  mutate(ST_FIPS = as.character(ST_FIPS)) 
write_csv(college_data2, "data/College_Data_Abbrev.csv")


college_fips <- read_csv("data/CollegeFips.csv", col_names = F) %>%
  set_names(c("ST_FIPS", "State")) %>%
  mutate_all(as.character)

college_data2 %>% select(STABBR, ST_FIPS) %>%
  unique() %>%
  left_join(college_fips) %>%
  write_csv("data/College_FIPS_Abbr.csv")

college_data2 <- left_join(college_data2, college_fips)
write_csv(college_data2, "data/College_Data_Abbrev.csv", na = '.')
```

For this example, we'll use a subset of the US Department of Education College Scorecard data. [Documentation](https://collegescorecard.ed.gov/data/documentation/), [Data](https://collegescorecard.ed.gov/data/). I've selected a few columns from the institution-level data available on the College Scorecard site. 

<details><summary>Let's take a look (Read in the data)</summary>

```{r summary-college-data}
library(readr)
college <- read_csv("https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/College_Data_Abbrev.csv", guess_max = 5000, na = '.')
str(college)
```

```{python summary-college-data-py}
import pandas as pd
college = pd.read_csv("https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/College_Data_Abbrev.csv", na_values = '.')
```

</details>


<details><summary>What proportion of the schools operating in each state have the state's name in the school name?</summary>

We'll use `str_detect()` to look for the state name in the college name. 

```{r string-ops, fig.width = 4, fig.height = 8, out.width = 4}
library(stringr) # string processing

# Outside the pipe
str_detect(college$INSTNM, pattern = college$State)[1:10]

# Using the pipe and mutate:
college <- college %>%
  mutate(uses_st_name = str_detect(INSTNM, State))

library(ggplot2) # graphs and charts
# By state - percentage of institution names
college %>%
  group_by(State) %>%
  summarize(pct_uses_st_name = mean(uses_st_name), n = n()) %>%
  filter(n > 5) %>% # only states/territories with at least 5 schools
  # Reorder state factor level by percentage that uses state name
  mutate(State = reorder(State, -pct_uses_st_name)) %>%
  ggplot(data = ., aes(x = State, y = pct_uses_st_name)) + 
  geom_col() + coord_flip() + 
  geom_text(aes(y = 1, label = paste("Total Schools:", n)), hjust = 1)
```

This example turned out to be way more complicated in Python than I was anticipating, mostly because unlike R, python string operations aren't vectorized over both the string and the pattern you're searching for. So this example uses a few tricks (like apply + lambda functions) that we haven't talked about yet.

```{python string-ops-py, fig.width = 4, fig.height = 8, out.width = 4, error = T}
import re # regular expressions

# This doesn't work because str.contains doesn't take a vector of patterns
# college["INSTNM"].str.contains(college["State"])[1:10]

# This is a function that we create
# We'll cover functions in the next chapter
# but for now, I've used this so that the code is a little more readable...

def str_detect(x, y):
  # Ensure x and y aren't null/NaN
  null_vals = pd.isna(x) or pd.isna(y)
  # If they aren't null, then search x for pattern y and return the result
  if not null_vals:
    return bool(re.search(y, x))
  # If there are null/na values, return False
  else:
    return False

# We then create a new variable by using our function on each row individually
college = college.assign(uses_st_name = college.apply(lambda row: str_detect(row.INSTNM, row.State), axis = 1))

# Then we aggregate
college_names = college.groupby("State", as_index = False).agg(
  pct_uses_st_name = pd.NamedAgg('uses_st_name', 'mean'),
  n = pd.NamedAgg('uses_st_name', 'count')
)

# Sorting by percent using state name
college_names = college_names.loc[college_names.n > 5].\
  sort_values('pct_uses_st_name', axis = 0)

# Creating a label variable
college_names['label'] = "Total Schools: " + college_names['n'].astype(str)

# Sorting states and enforcing that order - like making a factor in R
state_list = college_names.State.unique().tolist()
college_names.State = pd.Categorical(college_names.State, categories = state_list)

from plotnine import * # graphs and charts
# By state - percentage of institution names
ggplot(data = college_names) + \
geom_text(aes(x = "State", y = 1, label = 'label'), ha='right') + \
geom_col(aes(x = "State", y = "pct_uses_st_name")) + coord_flip()
```

</details>

:::

I'm not going to get into regular expressions in this class, but if you do want more power to understand how to work with strings, that's an excellent skill to pick up. 


### Joining and Splitting Variables

There's another string-related task that is fairly commonly encountered: separating variables into two different columns (as in Table 3 in the previous chapter).

```{r tidy-separate-pic, echo = F, out.width = "50%", fig.cap = "A visual representation of what separating variables means for data set operations."}
knitr::include_graphics("images/tidyr_separate.png")
```

<details><summary>Separating Variables in R</summary>

We can use `str_extract()` if we want, but it's actually faster to use `separate()`, which is part of the `tidyr` package. There is also `extract()`, which is another `tidyr` function that uses regular expressions and capture groups to split variables up. 

```{r separate-demo}
library(dplyr)
library(tidyr)

table3 %>%
  separate(col = rate, into = c("cases", "population"), sep = "/", remove = F)
```



I've left the rate column in the original data frame just to make it easy to compare and verify that yes, it worked. 

`separate()` will also take a full on regular expression if you want to capture only parts of a string to put into new columns. 
</details>

<details><summary>Separating Variables in python</summary>

In python, we can do a similar observation, but one convention of python that is very useful here is that we can do a multiple-assign on the left hand side.


```{python separate-demo-py}
table3 = r.table3

table3[["cases", "population"]] = table3["rate"].str.split("/", expand = True)
table3
```

We use `col.str.split()` to split the column, `expand = True` indicates that we want separate columns, and then by including two things on the left hand side, we can store each column into its own new value.

</details>

And, of course, there is a complementary operation, which is when it's necessary to join two columns to get a useable data value. 

```{r tidyr-unite-pic, echo = F, out.width = "50%", fig.cap = "A visual representation of what uniting variables means for data set operations."}
knitr::include_graphics("images/tidyr_unite.png")
```

<details><summary>Joining Variables in R</summary>
`separate()` has a complement, `unite()`, which is useful for handling situations like in table5:

```{r tidyr-unite-demo}
table5 %>%
  unite(col = "year", century:year, sep = '') %>%
  separate(col = rate, into = c("cases", "population"), sep = "/")
```

Note that separate and unite both work with character variables - it's not necessarily true that you'll always be working with character formats when you need to do these operations. For instance, it's relatively common to need to separate dates into year, month, and day as separate columns (or to join them together). 

Of course, it's much easier just to do a similar two-step operation (we have to convert to numeric variables to do math)

```{r tidyr-unite-mutate}
table5 %>%
  mutate(year = as.numeric(century)*100 + as.numeric(year)) %>% 
  select(-century)
```

(Handy shortcut functions in `dplyr` don't completely remove the need to think).

</details>


<details><summary>Joining Variables in python</summary>


```{python unite-demo-python}
r.table5.assign(year = r.table5.century + r.table5.year) # String concatenation
```

In python, we can join character values using `+`, so it's an even simpler process. Of course, as in R, it may be better to do a two-step operation to convert to numeric variables. Unlike in R, you can do the string concatenation process first and then convert to numeric variables without having to think too much.


```{python unite-mutate-py}
tmp = r.table5.assign(year = r.table5.century + r.table5.year)

tmp.\
assign(year = tmp.year.astype(int)).\
drop("century", axis = 1)
```

</details>



### Additional String Manipulation Information {- .learn-more}

String manipulation

- [R4DS chapter - strings](https://r4ds.had.co.nz/strings.html)
- [PCRE tester](https://regex101.com/)
- [R regex tester](https://spannbaueradam.shinyapps.io/r_regex_tester/) - has a short timeout period and will disconnect you if you're idle too long. But you can also clone the repo  [here](https://github.com/AdamSpannbauer/r_regex_tester_app) and run it locally. 
